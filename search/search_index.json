{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udd0d CRISP-T (Sense-making from Text and Numbers!)","text":"<p>TL;DR \ud83d\ude80 CRISP-T is a qualitative research method and a toolkit to perform textual (e.g. topic modelling) and numeric (e.g. decision trees) analysis of mixed datasets for computational triangulation and sense-making using large language models. \ud83d\udc49 See Demo.</p> <p> </p> <p>\u2705 CRISP is written in Python, but you don\u2019t need to know Python to use it!</p> <p>\u2705 CRISP is not a data science tool; it\u2019s a sense-making tool!</p> <p>\u2705 CRISP does not replace your analysis; it just augments it!</p> <p>\u2705 CRISP employs an interpretivist approach, and the same lens is required to comprehend its results!</p> <p>\u2705 CRISP does not need LLMs but can augment them with tools!</p> <p>\u2705 CRISP is designed to simplify your life as a qualitative researcher!</p> <p>\ud83d\udcaf CRISP is open-source! licensed under the GPL-3.0 License.</p> <p>Qualitative research focuses on collecting and analyzing textual data\u2014such as interview transcripts, open-ended survey responses, and field notes\u2014to explore complex phenomena and human experiences. Researchers may also incorporate quantitative or external sources (e.g., demographics, census data, social media) to provide context and triangulate findings. Characterized by an inductive approach, qualitative research emphasizes generating theories from data rather than testing hypotheses. While qualitative and quantitative data are often used together, there is no standard method for combining them.</p> <p>CRISP-T is a method and toolset to integrate textual data (as a list of documents) and numeric data (as Pandas DataFrame) into structured classes that retain metadata from various analytical processes, such as topic modeling and decision trees. Researchers, with or without GenAI assistance, can define relationships between textual and numerical datasets based on their chosen theoretical lens.  An optional final analytical phase ensures that proposed relationships actually hold true. Further, if the numeric and textual datasets share same id, or if the textual metadata contains keywords that match numeric column names; both datasets are filtered simultaneously, ensuring alignment and facilitating triangulation. \ud83d\udc49 See Demo.</p> <p>CRISP-T implements semantic search using ChromaDB to find relevant documents or document chunks based on similarity to a query or reference documents. This is useful for literature reviews to find documents likely to fit inclusion criteria within your corpus/search results. It can also be used for coding/annotating documents by finding relevant chunks within a specific document.</p> <p>An MCP server exposes all functionality as tools, resources, and prompts, enabling integration with AI agent platforms such as Claude desktop, VSCODE and other MCP-compatible clients. CRISP-T cannot directly code the documents, but it provides semantic chunk search that may be used in association with other tools to acheive automated coding. For example, VSCODE provides built in tools for editing text and markdown files, which can be used to code documents based on semantic search.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install crisp-t\n</code></pre> <p>Include machine learning features for numeric data analysis (Recommended):</p> <pre><code>pip install crisp-t[ml]\n</code></pre> <p>Include XGBoost for gradient boosting features (Optional):</p> <pre><code>pip install crisp-t[xg]\n</code></pre> <ul> <li>Mac users need to install libomp: <code>brew install libomp</code> for XGBoost to work. (Needed only if you want to use XGBoost)</li> </ul>"},{"location":"#command-line-scripts","title":"Command Line Scripts","text":"<p>CRISP-T now provides four main command-line scripts:</p> <ul> <li><code>crisp</code> \u2014 Main CLI for qualitative triangulation and analysis (see below)</li> <li><code>crispviz</code> \u2014 Visualization CLI for corpus data (word frequencies, topic charts, wordclouds, etc.)</li> <li><code>crispt</code> \u2014 Corpus manipulation CLI (create, edit, query, and manage corpus objects)</li> <li><code>crisp-mcp</code> \u2014 Starts the MCP server for AI integration (see MCP section below)</li> </ul> <p>All scripts are installed as entry points and can be run directly from the command line after installation.</p>"},{"location":"#crisp-analytical-cli","title":"crisp (Analytical CLI)","text":"<pre><code>crisp [OPTIONS]\n</code></pre>"},{"location":"#first-step-is-to-create-a-corpus-from-sources-data-import","title":"\u26a0\ufe0f First step is to create a corpus from sources (Data Import).","text":"<p>Source data is read from a directory containing text files (.txt, .pdf) and a single .csv file (for numeric data). The corpus is saved to <code>--out</code> folder and this folder can be used as input for all subsequent analyses.</p> <p>\u26a0\ufe0f This import step only needs to be done once.</p> <pre><code>crisp --source PATH --out PATH\n</code></pre> <p>e.g., <code>crisp --source crisp_source --out crisp_input</code></p> <p>\u2139\ufe0f <code>crisp_input</code> is recommended for <code>--out</code> option above. The folder is created in the current directory. \ud83d\udc49 See Demo.</p> <p>\u2139\ufe0f From here onwards, you can load the corpus from that folder using <code>--inp</code> option for all subsequent analyses. You can omit <code>--inp</code> if you are using the <code>crisp_input</code> folder, as it is the default for <code>--inp</code>. \ud83d\udc49 See Demo.</p> <p>\u26a1\ufe0f Advanced users may also load corpus from URL <code>--source</code> or multiple URLs using <code>--sources</code> option. \u26a1\ufe0f</p> <p>\u26a0\ufe0f While analysing multiple datasets, use <code>crisp --clear</code> option to clear cache before switching datasets.</p>"},{"location":"#inputoutput-options","title":"Input/Output Options","text":"<ul> <li><code>--source, -s PATH|URL</code>: Read source data from a directory (reads .txt, .pdf and a single .csv) or from a URL</li> <li><code>--sources PATH|URL</code>: Provide multiple sources; can be used multiple times</li> <li><code>--inp, -i PATH</code>: Load an existing corpus from a folder containing <code>corpus.json</code> (and optional <code>corpus_df.csv</code>)</li> <li><code>--out, -o PATH</code>: When saving the corpus, provide a folder path; the CLI writes <code>corpus.json</code> (and <code>corpus_df.csv</code> if available) into that folder. When saving analysis results (topics, sentiment, etc.), this acts as a base path: files are written with suffixes, e.g., <code>results_topics.json</code>.</li> <li><code>--unstructured, -t TEXT</code>: Text CSV column(s) to analyze/compare (can be used multiple times). This is useful when you have free-form text data in a DataFrame. If this is provided, those columns are used as documents.</li> <li><code>--ignore TEXT</code>: Comma-separated words to ignore during ingestion (applies to <code>--source/--sources</code>)</li> </ul>"},{"location":"#analysis-options","title":"Analysis Options","text":"<ul> <li><code>--codedict</code>: Generate qualitative coding dictionary</li> <li><code>--topics</code>: Generate topic model using LDA</li> <li><code>--assign</code>: Assign documents to topics</li> <li><code>--cat</code>: List categories of entire corpus or individual documents</li> <li><code>--summary</code>: Generate extractive text summary</li> <li><code>--sentiment</code>: Generate sentiment scores using VADER</li> <li><code>--sentence</code>: Generate sentence-level scores when applicable</li> <li><code>--nlp</code>: Generate all NLP reports (combines above text analyses)</li> <li><code>--nnet</code>, <code>--cls</code>, <code>--knn</code>, <code>--kmeans</code>, <code>--cart</code>, <code>--pca</code>, <code>--regression</code>, <code>--lstm</code>, <code>--ml</code>: Machine learning and clustering options (requires <code>crisp-t[ml]</code>)</li> <li><code>--regression</code>: Perform linear or logistic regression (automatically detects binary outcomes for logistic regression)</li> <li><code>--lstm</code>: Train LSTM model on text data to predict outcome variable (requires binary outcome and 'id' column for alignment)</li> <li><code>--visualize</code>: Generate visualizations (word clouds, topic charts, etc.)</li> <li><code>--num, -n INTEGER</code>: Number parameter (clusters, topics, epochs, etc.) - default: 3</li> <li><code>--rec, -r INTEGER</code>: Record parameter (top N results, recommendations) - default: 3</li> <li><code>--filters, -f TEXT</code>: Filters to apply as <code>key=value</code> (can be used multiple times); keeps only documents where <code>document.metadata[key] == value</code>. Invalid formats raise an error.</li> <li><code>--verbose, -v</code>: Print verbose messages for debugging</li> </ul>"},{"location":"#data-sources","title":"Data Sources","text":"<ul> <li><code>--source, -s PATH|URL</code>: Read source data from a directory (reads .txt and .pdf) or from a URL</li> <li><code>--sources PATH|URL</code>: Provide multiple sources; can be used multiple times</li> </ul>"},{"location":"#display-options","title":"Display Options","text":"<p>The <code>--print, -p</code> option provides flexible ways to display corpus information with color-coded output. You can use either quoted or unquoted syntax:</p> <p>Syntax: - Quoted: <code>--print \"command subcommand\"</code> - Unquoted: <code>--print command --print subcommand</code></p> <p>Basic Options: - <code>--print all</code>: Display all corpus information (documents, dataframe, metadata) - <code>--print documents</code>: Show first 5 documents with IDs, names, and text snippets - <code>--print documents --print N</code>: Show first N documents (e.g., <code>--print documents --print 10</code> shows 10 documents) - <code>--print documents --print metadata</code>: Display metadata for all documents (categories, scores, etc.)</p> <p>DataFrame Options: - <code>--print dataframe</code>: Show DataFrame head with shape and column information - <code>--print dataframe --print metadata</code>: Display DataFrame columns starting with <code>metadata_</code> prefix - <code>--print dataframe --print stats</code>: Show descriptive statistics and value distributions</p> <p>Metadata Options: - <code>--print metadata</code>: Display all corpus metadata keys and values - <code>--print metadata --print KEY</code>: Show specific metadata (e.g., <code>--print metadata --print pca</code>)   - Available keys include: pca, numeric_clusters, kmeans, nnet_predictions, svm_confusion_matrix, decision_tree_accuracy, and more</p> <p>Legacy Option: - <code>--print stats</code>: (Deprecated) Use <code>--print dataframe --print stats</code> instead</p> <p>Examples:</p> <pre><code># Show first 10 documents (unquoted syntax)\ncrisp --print documents --print 10\n\n# Show first 10 documents (quoted syntax - backward compatible)\ncrisp --print \"documents 10\"\n\n# View document metadata (unquoted)\ncrisp --print documents --print metadata\n\n# View document metadata (quoted)\ncrisp --print \"documents metadata\"\n\n# Check PCA results (unquoted)\ncrisp --print metadata --print pca\n\n# View DataFrame statistics (unquoted)\ncrisp --print dataframe --print stats\n</code></pre>"},{"location":"#crispviz-visualization-cli","title":"crispviz (Visualization CLI)","text":"<pre><code>crispviz [OPTIONS]\n</code></pre> <ul> <li><code>--inp, --source, --sources</code>: Input corpus or sources</li> <li><code>--out</code>: Output directory for PNG images</li> <li>Visualization flags: <code>--freq</code>, <code>--by-topic</code>, <code>--wordcloud</code>, <code>--ldavis</code>, <code>--top-terms</code>, <code>--corr-heatmap</code>, <code>--tdabm</code></li> <li>Optional params: <code>--bins</code>, <code>--top-n</code>, <code>--columns</code>, <code>--topics-num</code></li> </ul> <p>Visualization Options: - <code>--freq</code>: Export word frequency distribution - <code>--by-topic</code>: Export distribution by dominant topic (requires LDA) - <code>--wordcloud</code>: Export topic wordcloud (requires LDA) - <code>--ldavis</code>: Export interactive LDA visualization as HTML (requires LDA and pyLDAvis) - <code>--top-terms</code>: Export top terms bar chart - <code>--corr-heatmap</code>: Export correlation heatmap from CSV numeric columns - <code>--tdabm</code>: Export TDABM visualization (requires TDABM analysis in corpus metadata). Use <code>crispt --tdabm</code> to perform the analysis first. - <code>--topics-num N</code>: Number of topics for LDA (default: 8, based on Mettler et al., 2025)</p>"},{"location":"#crispt-corpus-manipulation-cli","title":"crispt (Corpus Manipulation CLI)","text":"<pre><code>crispt [OPTIONS]\n</code></pre> <ul> <li><code>--id</code>, <code>--name</code>, <code>--description</code>: Corpus metadata</li> <li><code>--doc</code>: Add document as <code>id|name|text</code> or <code>id|text</code> (repeatable)</li> <li><code>--remove-doc</code>: Remove document by ID (repeatable)</li> <li><code>--meta</code>: Add/update corpus metadata as <code>key=value</code> (repeatable)</li> <li><code>--add-rel</code>: Add relationship as <code>first|second|relation</code> (repeatable)</li> <li><code>--clear-rel</code>: Clear all relationships</li> <li><code>--out</code>: Save corpus to folder/file as <code>corpus.json</code></li> <li><code>--inp</code>: Load corpus from folder/file containing <code>corpus.json</code></li> <li>Query options:<ul> <li><code>--df-cols</code>: Print DataFrame column names</li> <li><code>--df-row-count</code>: Print DataFrame row count</li> <li><code>--df-row INDEX</code>: Print DataFrame row by index</li> <li><code>--doc-ids</code>: Print all document IDs</li> <li><code>--doc-id ID</code>: Print document by ID</li> <li><code>--relationships</code>: Print all relationships</li> <li><code>--relationships-for-keyword KEYWORD</code>: Print relationships involving a keyword</li> </ul> </li> <li>Semantic search (requires <code>chromadb</code>):<ul> <li><code>--semantic QUERY</code>: Perform semantic search with query string</li> <li><code>--similar-docs DOC_IDS</code>: Find documents similar to comma-separated list of document IDs (useful for literature reviews)</li> <li><code>--num N</code>: Number of results to return (default: 5). Used for <code>--semantic</code> and <code>--similar-docs</code></li> <li><code>--semantic-chunks QUERY</code>: Perform semantic search on document chunks. Returns matching chunks for a specific document (use with <code>--doc-id</code> and <code>--rec</code> for similarity threshold between 0 and 10 with a default of 8.5)</li> <li><code>--rec THRESHOLD</code>: Threshold for semantic operations. For <code>--semantic-chunks</code>, use 0-10 (default: 8.5). For <code>--similar-docs</code>, use 0-1 (default: 0.7). Only results with similarity above this value are returned</li> <li><code>--metadata-df</code>: Export collection metadata as DataFrame+</li> <li><code>--metadata-keys KEYS</code>: Comma-separated metadata keys to include+</li> </ul> </li> <li>TDABM analysis:<ul> <li><code>--tdabm Y_VAR:X_VARS:RADIUS</code>: Perform Topological Data Analysis Ball Mapper (TDABM) analysis. Format: <code>y_variable:x_variables:radius</code> (e.g., <code>satisfaction:age,income:0.3</code>). Radius defaults to 0.3 if omitted.</li> </ul> </li> </ul> <p>\u2139\ufe0f <code>--metadata-df</code> and <code>--metadata-keys</code> options can be used  to export or add metadata from NLP to the DataFrame. For example, you can extract sentiment scores or topic assignments as additional columns for numerical analysis. This is useful if dataframe and documents are aligned as in a survey response.</p>"},{"location":"#example-usage","title":"Example Usage","text":"<p>When saving the corpus via <code>--out</code>, the CLI writes <code>corpus.json</code> (and <code>corpus_df.csv</code> if present) into the specified folder. If you pass a file path, only its parent directory is used for writing <code>corpus.json</code>.</p>"},{"location":"#mcp-server","title":"MCP Server","text":"<p>CRISP-T provides a Model Context Protocol (MCP) server that exposes all functionality as tools, resources, and prompts. This enables integration with AI assistants and other MCP-compatible clients.</p>"},{"location":"#using-the-mcp-server","title":"Using the MCP Server","text":""},{"location":"#configuring-mcp-clients","title":"Configuring MCP Clients","text":""},{"location":"#claude-desktop","title":"Claude Desktop","text":"<p>Add to your Claude Desktop configuration file:</p> <p>MacOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></p> <pre><code>{\n  \"mcpServers\": {\n    \"crisp-t\": {\n      \"command\": \"&lt;python-path&gt;crisp-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"#using-with-other-mcp-clients","title":"Using with Other MCP Clients","text":"<p>The server can be used with any MCP-compatible client. Configure your client to run the <code>crisp-mcp</code> command via stdio.</p>"},{"location":"#available-tools","title":"Available Tools","text":"<p>The MCP server provides tools for:</p> <p>Corpus Management - <code>load_corpus</code> - Load corpus from folder or source - <code>save_corpus</code> - Save corpus to folder - <code>add_document</code> - Add new document - <code>remove_document</code> - Remove document by ID - <code>get_document</code> - Get document details - <code>list_documents</code> - List all document IDs - <code>add_relationship</code> - Link text keywords with numeric columns - <code>get_relationships</code> - Get all relationships - <code>get_relationships_for_keyword</code> - Query relationships by keyword</p> <p>NLP/Text Analysis - <code>assign_topics</code> - Assign documents to topics (creates keyword labels) - <code>extract_categories</code> - Extract common concepts - <code>generate_summary</code> - Generate extractive summary - <code>sentiment_analysis</code> - VADER sentiment analysis</p> <p>Semantic Search (requires <code>chromadb</code>) - <code>semantic_search</code> - Find documents similar to a query using semantic similarity - <code>find_similar_documents</code> - Find documents similar to a set of reference documents (useful for literature reviews and qualitative research) - <code>semantic_chunk_search</code> - Find relevant chunks within a specific document (useful for coding/annotating documents) - <code>export_metadata_df</code> - Export ChromaDB metadata as DataFrame</p> <p>DataFrame/CSV Operations - <code>get_df_columns</code> - Get DataFrame column names - <code>get_df_row_count</code> - Get number of rows - <code>get_df_row</code> - Get specific row by index</p> <p>Machine Learning (requires <code>crisp-t[ml]</code>) - <code>kmeans_clustering</code> - K-Means clustering - <code>decision_tree_classification</code> - Decision tree with feature importance - <code>svm_classification</code> - SVM classification - <code>neural_network_classification</code> - Neural network classification - <code>regression_analysis</code> - Linear/logistic regression with coefficients - <code>pca_analysis</code> - Principal Component Analysis - <code>association_rules</code> - Apriori association rules - <code>knn_search</code> - K-nearest neighbors search - <code>lstm_text_classification</code> - LSTM model for text-based outcome prediction</p>"},{"location":"#resources","title":"Resources","text":"<p>The server exposes corpus documents as resources: - <code>corpus://document/{id}</code> - Access document text by ID</p>"},{"location":"#prompts","title":"Prompts","text":"<ul> <li><code>analysis_workflow</code> - Complete step-by-step analysis guide based on INSTRUCTIONS.md</li> <li><code>triangulation_guide</code> - Guide for triangulating qualitative and quantitative findings</li> </ul>"},{"location":"#example-mcp-commands","title":"Example MCP commands","text":""},{"location":"#role-of-crisp-t-in-research-and-practice","title":"Role of CRISP-T in research and practice","text":"<p>The workflow enables AI assistants to help conduct comprehensive analyses by combining text analytics, machine learning, and triangulation of qualitative-quantitative findings.</p> <p>For example, in market research, a company collects: - Textual feedback from customer support interactions. - Numerical data on customer retention and sales performance. Using this framework, business analysts can investigate how recurring concerns in feedback correspond to measurable business outcomes.</p>"},{"location":"#framework-documentation","title":"Framework Documentation","text":"<p>For detailed information about available functions, metadata handling, and theoretical frameworks, see the comprehensive user instructions. For semantic search examples and best practices, see the Semantic Search Guide. Documentation (WIP) is also available here.</p>"},{"location":"#data-model","title":"Data model","text":""},{"location":"#references","title":"References","text":"<ul> <li>Mettler et al. (2025) Computational Text Analysis for Qualitative IS Research: A Methodological Reflection</li> <li>TDABM (Topological Data Analysis Ball Mapper) Rudkin and Dlotko (2024)</li> </ul>"},{"location":"#citation","title":"Citation","text":"<ul> <li>Released on 10/11/2025 for presentation at ICIS 2025 conference.</li> <li>Paper coming soon. Cite this repository in the meantime:</li> </ul>"},{"location":"#give-us-a-star","title":"Give us a star \u2b50\ufe0f","text":"<p>If you find this project useful, give us a star. It helps others discover the project.</p>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Bell Eapen (UIS) |  Contact | </li> </ul>"},{"location":"MCP_SERVER/","title":"CRISP-T MCP Server Guide","text":""},{"location":"MCP_SERVER/#overview","title":"Overview","text":"<p>The CRISP-T MCP (Model Context Protocol) server exposes all CRISP-T functionality through a standardized interface that can be used by AI assistants and other MCP-compatible clients.</p>"},{"location":"MCP_SERVER/#installation","title":"Installation","text":"<p>Install CRISP-T with MCP support:</p> <pre><code>pip install crisp-t\n</code></pre> <p>For machine learning features:</p> <pre><code>pip install crisp-t[ml]\n</code></pre>"},{"location":"MCP_SERVER/#starting-the-server","title":"Starting the Server","text":"<p>The MCP server runs via stdio and can be started with:</p> <pre><code>crisp-mcp\n</code></pre> <p>Or using Python directly:</p> <pre><code>python -m crisp_t.mcp\n</code></pre>"},{"location":"MCP_SERVER/#client-configuration","title":"Client Configuration","text":""},{"location":"MCP_SERVER/#claude-desktop","title":"Claude Desktop","text":"<p>Add the following to your Claude Desktop configuration file:</p> <p>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></p> <p>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></p> <pre><code>{\n  \"mcpServers\": {\n    \"crisp-t\": {\n      \"command\": \"crisp-mcp\"\n    }\n  }\n}\n</code></pre> <p>After adding the configuration, restart Claude Desktop. The CRISP-T tools will be available in the MCP menu.</p>"},{"location":"MCP_SERVER/#other-mcp-clients","title":"Other MCP Clients","text":"<p>Configure your MCP client to run the <code>crisp-mcp</code> command via stdio. Consult your client's documentation for specific configuration instructions.</p>"},{"location":"MCP_SERVER/#available-tools","title":"Available Tools","text":""},{"location":"MCP_SERVER/#corpus-management","title":"Corpus Management","text":""},{"location":"MCP_SERVER/#load_corpus","title":"<code>load_corpus</code>","text":"<p>Load a corpus from a folder or source directory.</p> <p>Arguments: - <code>inp</code> (optional): Path to folder containing corpus.json - <code>source</code> (optional): Source directory or URL to read data from - <code>text_columns</code> (optional): Comma-separated text column names for CSV data - <code>ignore_words</code> (optional): Comma-separated words to ignore during analysis</p> <p>Example:</p> <pre><code>{\n  \"inp\": \"/path/to/corpus_folder\"\n}\n</code></pre>"},{"location":"MCP_SERVER/#save_corpus","title":"<code>save_corpus</code>","text":"<p>Save the current corpus to a folder.</p> <p>Arguments: - <code>out</code> (required): Output folder path</p>"},{"location":"MCP_SERVER/#add_document","title":"<code>add_document</code>","text":"<p>Add a new document to the corpus.</p> <p>Arguments: - <code>doc_id</code> (required): Unique document ID - <code>text</code> (required): Document text content - <code>name</code> (optional): Document name</p>"},{"location":"MCP_SERVER/#remove_document","title":"<code>remove_document</code>","text":"<p>Remove a document by ID.</p> <p>Arguments: - <code>doc_id</code> (required): Document ID to remove</p>"},{"location":"MCP_SERVER/#get_document","title":"<code>get_document</code>","text":"<p>Get document details by ID.</p> <p>Arguments: - <code>doc_id</code> (required): Document ID</p>"},{"location":"MCP_SERVER/#list_documents","title":"<code>list_documents</code>","text":"<p>List all document IDs in the corpus.</p>"},{"location":"MCP_SERVER/#add_relationship","title":"<code>add_relationship</code>","text":"<p>Add a relationship between text keywords and numeric columns.</p> <p>Arguments: - <code>first</code> (required): First entity (e.g., \"text:healthcare\") - <code>second</code> (required): Second entity (e.g., \"num:age_group\") - <code>relation</code> (required): Relationship type (e.g., \"correlates\")</p> <p>Example:</p> <pre><code>{\n  \"first\": \"text:satisfaction\",\n  \"second\": \"num:rating_score\",\n  \"relation\": \"predicts\"\n}\n</code></pre>"},{"location":"MCP_SERVER/#get_relationships","title":"<code>get_relationships</code>","text":"<p>Get all relationships in the corpus.</p>"},{"location":"MCP_SERVER/#get_relationships_for_keyword","title":"<code>get_relationships_for_keyword</code>","text":"<p>Get relationships involving a specific keyword.</p> <p>Arguments: - <code>keyword</code> (required): Keyword to search for</p>"},{"location":"MCP_SERVER/#nlptext-analysis","title":"NLP/Text Analysis","text":""},{"location":"MCP_SERVER/#generate_coding_dictionary","title":"<code>generate_coding_dictionary</code>","text":"<p>Generate a qualitative coding dictionary with categories, properties, and dimensions.</p> <p>Arguments: - <code>num</code> (optional): Number of categories to extract (default: 3) - <code>top_n</code> (optional): Top N items per category (default: 3)</p> <p>Returns: - Categories (verbs representing main actions/themes) - Properties (nouns associated with categories) - Dimensions (adjectives/adverbs describing properties)</p>"},{"location":"MCP_SERVER/#topic_modeling","title":"<code>topic_modeling</code>","text":"<p>Perform LDA topic modeling to discover latent topics.</p> <p>Arguments: - <code>num_topics</code> (optional): Number of topics to generate (default: 3) - <code>num_words</code> (optional): Number of words per topic (default: 5)</p> <p>Returns: Topics with keywords and weights showing word importance within each topic.</p>"},{"location":"MCP_SERVER/#assign_topics","title":"<code>assign_topics</code>","text":"<p>Assign documents to their dominant topics.</p> <p>Arguments: - <code>num_topics</code> (optional): Number of topics (should match topic_modeling, default: 3)</p> <p>Returns: Document-topic assignments with contribution percentages. These assignments create keyword labels that can be used to filter or categorize documents.</p>"},{"location":"MCP_SERVER/#extract_categories","title":"<code>extract_categories</code>","text":"<p>Extract common categories/concepts from the corpus.</p> <p>Arguments: - <code>num</code> (optional): Number of categories (default: 10)</p>"},{"location":"MCP_SERVER/#generate_summary","title":"<code>generate_summary</code>","text":"<p>Generate an extractive text summary of the corpus.</p> <p>Arguments: - <code>weight</code> (optional): Summary length parameter (default: 10)</p>"},{"location":"MCP_SERVER/#sentiment_analysis","title":"<code>sentiment_analysis</code>","text":"<p>Perform VADER sentiment analysis.</p> <p>Arguments: - <code>documents</code> (optional): Analyze at document level (default: false) - <code>verbose</code> (optional): Verbose output (default: true)</p> <p>Returns: Sentiment scores (negative, neutral, positive, compound).</p>"},{"location":"MCP_SERVER/#semantic-search-requires-chromadb","title":"Semantic Search (requires chromadb)","text":""},{"location":"MCP_SERVER/#semantic_search","title":"<code>semantic_search</code>","text":"<p>Find documents similar to a query using semantic similarity.</p> <p>Arguments: - <code>query</code> (required): Search query text - <code>n_results</code> (optional): Number of similar documents to return (default: 5)</p> <p>Returns: List of similar documents with their IDs and names, ranked by semantic similarity.</p> <p>Example:</p> <pre><code>{\n  \"query\": \"machine learning and AI\",\n  \"n_results\": 5\n}\n</code></pre>"},{"location":"MCP_SERVER/#find_similar_documents","title":"<code>find_similar_documents</code>","text":"<p>Find documents similar to a given set of reference documents based on semantic similarity. This tool is particularly useful for literature reviews and qualitative research where you want to find additional documents that are similar to a set of known relevant documents. It can also be used to identify documents with similar themes, topics, or content for grouping and analysis purposes.</p> <p>Arguments: - <code>document_ids</code> (required): A single document ID or comma-separated list of document IDs to use as reference - <code>n_results</code> (optional): Number of similar documents to return (default: 5) - <code>threshold</code> (optional): Minimum similarity threshold 0-1 (default: 0.7). Only documents above this threshold are returned</p> <p>Returns: List of document IDs similar to the reference documents, excluding the reference documents themselves.</p> <p>Example:</p> <pre><code>{\n  \"document_ids\": \"doc1,doc5,doc12\",\n  \"n_results\": 10,\n  \"threshold\": 0.7\n}\n</code></pre> <p>Use Cases: - Literature reviews: Find papers similar to known relevant papers - Qualitative research: Identify documents with similar themes - Content grouping: Group similar documents for analysis - Document recommendation: Suggest related documents to researchers</p>"},{"location":"MCP_SERVER/#semantic_chunk_search","title":"<code>semantic_chunk_search</code>","text":"<p>Perform semantic search on chunks of a specific document. This tool is useful for coding and annotating documents by identifying relevant sections that match specific concepts or themes.</p> <p>Arguments: - <code>query</code> (required): Search query text (concept or set of concepts) - <code>doc_id</code> (required): Document ID to search within - <code>threshold</code> (optional): Minimum similarity threshold 0-1 (default: 0.5). Only chunks above this threshold are returned - <code>n_results</code> (optional): Maximum number of chunks to retrieve before filtering (default: 10)</p> <p>Returns: List of matching text chunks from the specified document that can be used for qualitative analysis or document annotation.</p> <p>Example:</p> <pre><code>{\n  \"query\": \"patient satisfaction\",\n  \"doc_id\": \"interview_01\",\n  \"threshold\": 0.6,\n  \"n_results\": 10\n}\n</code></pre> <p>Use Cases: - Coding qualitative interview transcripts for specific themes - Identifying sections of documents relevant to research questions - Annotating documents with concept labels - Finding evidence for theoretical constructs within texts</p>"},{"location":"MCP_SERVER/#export_metadata_df","title":"<code>export_metadata_df</code>","text":"<p>Export ChromaDB collection metadata as a pandas DataFrame.</p> <p>Arguments: - <code>metadata_keys</code> (optional): Comma-separated list of metadata keys to include</p> <p>Returns: DataFrame with document metadata that can be merged with numeric data for mixed-methods analysis.</p>"},{"location":"MCP_SERVER/#tdabm-topological-data-analysis-ball-mapper","title":"TDABM (Topological Data Analysis Ball Mapper)","text":""},{"location":"MCP_SERVER/#tdabm_analysis","title":"<code>tdabm_analysis</code>","text":"<p>Perform Topological Data Analysis Ball Mapper (TDABM) analysis to uncover hidden, global patterns in complex, noisy, or high-dimensional data.</p> <p>Based on the algorithm by Rudkin and Dlotko (2024), TDABM creates a point cloud from multidimensional data and covers it with overlapping balls, revealing topological structure and relationships between variables.</p> <p>Arguments: - <code>y_variable</code> (required): Name of the continuous Y variable to analyze - <code>x_variables</code> (required): Comma-separated list of ordinal/numeric X variable names - <code>radius</code> (optional): Radius for ball coverage (default: 0.3). Smaller values create more detailed mappings.</p> <p>Example:</p> <pre><code>{\n  \"y_variable\": \"satisfaction\",\n  \"x_variables\": \"age,income,education\",\n  \"radius\": 0.3\n}\n</code></pre> <p>Returns: Analysis results in JSON format including landmark points, their locations, connections, and mean Y values. Results are stored in corpus metadata['tdabm'].</p> <p>Use Cases: - Discovering hidden patterns in multidimensional data - Visualizing relationships between multiple variables - Identifying clusters and connections in complex datasets - Performing model-free exploratory data analysis - Understanding global structure in high-dimensional data</p> <p>Note: After running TDABM analysis, use <code>save_corpus</code> to persist results, then visualize with <code>crispviz --tdabm</code>.</p> <p>Reference: Rudkin, S., &amp; Dlotko, P. (2024). Topological Data Analysis Ball Mapper for multidimensional data visualization. Paper reference to be added - algorithm implementation based on the TDABM methodology described by the authors.</p>"},{"location":"MCP_SERVER/#dataframecsv-operations","title":"DataFrame/CSV Operations","text":""},{"location":"MCP_SERVER/#get_df_columns","title":"<code>get_df_columns</code>","text":"<p>Get all column names from the DataFrame.</p>"},{"location":"MCP_SERVER/#get_df_row_count","title":"<code>get_df_row_count</code>","text":"<p>Get the number of rows in the DataFrame.</p>"},{"location":"MCP_SERVER/#get_df_row","title":"<code>get_df_row</code>","text":"<p>Get a specific row by index.</p> <p>Arguments: - <code>index</code> (required): Row index</p>"},{"location":"MCP_SERVER/#machine-learning-requires-crisp-tml","title":"Machine Learning (requires crisp-t[ml])","text":""},{"location":"MCP_SERVER/#kmeans_clustering","title":"<code>kmeans_clustering</code>","text":"<p>Perform K-Means clustering on numeric data.</p> <p>Arguments: - <code>num_clusters</code> (optional): Number of clusters (default: 3) - <code>outcome</code> (optional): Outcome variable to exclude</p> <p>Returns: Cluster assignments and membership information.</p>"},{"location":"MCP_SERVER/#decision_tree_classification","title":"<code>decision_tree_classification</code>","text":"<p>Train a decision tree classifier and return variable importance.</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable - <code>top_n</code> (optional): Top N important features (default: 10)</p> <p>Returns: - Confusion matrix - Feature importance rankings (shows which variables are most predictive)</p>"},{"location":"MCP_SERVER/#svm_classification","title":"<code>svm_classification</code>","text":"<p>Perform SVM classification.</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable</p> <p>Returns: Confusion matrix showing classification performance.</p>"},{"location":"MCP_SERVER/#neural_network_classification","title":"<code>neural_network_classification</code>","text":"<p>Train a neural network classifier.</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable</p> <p>Returns: Predictions and accuracy metrics.</p>"},{"location":"MCP_SERVER/#regression_analysis","title":"<code>regression_analysis</code>","text":"<p>Perform linear or logistic regression (auto-detects based on outcome type).</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable</p> <p>Returns: - Model type (linear or logistic) - Coefficients for each factor (showing strength and direction of relationships) - Intercept - Performance metrics (R\u00b2, accuracy, etc.)</p>"},{"location":"MCP_SERVER/#pca_analysis","title":"<code>pca_analysis</code>","text":"<p>Perform Principal Component Analysis.</p> <p>Arguments: - <code>outcome</code> (required): Variable to exclude from PCA - <code>n_components</code> (optional): Number of components (default: 3)</p>"},{"location":"MCP_SERVER/#association_rules","title":"<code>association_rules</code>","text":"<p>Generate association rules using Apriori algorithm.</p> <p>Arguments: - <code>outcome</code> (required): Variable to exclude - <code>min_support</code> (optional): Minimum support 1-99 (default: 50) - <code>min_threshold</code> (optional): Minimum threshold 1-99 (default: 50)</p>"},{"location":"MCP_SERVER/#knn_search","title":"<code>knn_search</code>","text":"<p>Find K-nearest neighbors for a specific record.</p> <p>Arguments: - <code>outcome</code> (required): Target variable - <code>n</code> (optional): Number of neighbors (default: 3) - <code>record</code> (optional): Record index, 1-based (default: 1)</p>"},{"location":"MCP_SERVER/#resources","title":"Resources","text":"<p>The server exposes corpus documents as resources that can be read:</p> <ul> <li><code>corpus://document/{id}</code> - Access document text content by ID</li> </ul>"},{"location":"MCP_SERVER/#prompts","title":"Prompts","text":""},{"location":"MCP_SERVER/#analysis_workflow","title":"<code>analysis_workflow</code>","text":"<p>Complete step-by-step guide for conducting a CRISP-T analysis based on INSTRUCTIONS.md.</p> <p>This prompt provides: - Data preparation steps - Descriptive analysis workflow - Advanced pattern discovery techniques - Predictive modeling approaches - Validation and triangulation strategies</p>"},{"location":"MCP_SERVER/#triangulation_guide","title":"<code>triangulation_guide</code>","text":"<p>Guide for triangulating qualitative and quantitative findings.</p> <p>This prompt explains: - How to link topic keywords with numeric variables - Strategies for comparing patterns across data types - Using relationships to document connections - Best practices for validation</p>"},{"location":"MCP_SERVER/#example-workflow","title":"Example Workflow","text":"<p>Here's a typical analysis workflow using the MCP server:</p> <ol> <li> <p>Load data <code>load_corpus(inp=\"/path/to/corpus\")</code></p> </li> <li> <p>Explore the data <code>list_documents()    get_df_columns()    get_df_row_count()</code></p> </li> <li> <p>Perform text analysis <code>generate_coding_dictionary(num=10, top_n=5)    topic_modeling(num_topics=5, num_words=10)    assign_topics(num_topics=5)    sentiment_analysis(documents=true)</code></p> </li> <li> <p>Perform numeric analysis <code>regression_analysis(outcome=\"satisfaction_score\")    decision_tree_classification(outcome=\"readmission\", top_n=10)</code></p> </li> <li> <p>Link findings <code>add_relationship(      first=\"text:healthcare_access\",      second=\"num:insurance_status\",      relation=\"correlates\"    )</code></p> </li> <li> <p>Save results <code>save_corpus(out=\"/path/to/output\")</code></p> </li> </ol>"},{"location":"MCP_SERVER/#key-features","title":"Key Features","text":""},{"location":"MCP_SERVER/#topic-modeling-creates-keywords","title":"Topic Modeling Creates Keywords","text":"<p>When you use <code>topic_modeling</code> and <code>assign_topics</code>, the tool assigns topic keywords to documents. These keywords can then be used to: - Filter documents by theme - Create relationships with numeric columns - Categorize documents for further analysis</p>"},{"location":"MCP_SERVER/#regression-shows-coefficients","title":"Regression Shows Coefficients","text":"<p>The <code>regression_analysis</code> tool returns coefficients for each predictor variable, showing: - Strength of relationship (larger absolute value = stronger) - Direction of relationship (positive or negative) - Statistical significance</p>"},{"location":"MCP_SERVER/#decision-trees-show-importance","title":"Decision Trees Show Importance","text":"<p>The <code>decision_tree_classification</code> tool returns variable importance rankings, indicating which features are most predictive of the outcome.</p>"},{"location":"MCP_SERVER/#relationships-link-data-types","title":"Relationships Link Data Types","text":"<p>Use <code>add_relationship</code> to document connections between: - Text findings (keywords from topics) - Numeric variables (DataFrame columns) - Theoretical constructs</p> <p>Example: Link \"healthcare_quality\" keyword to \"patient_satisfaction\" column with relation \"predicts\".</p>"},{"location":"MCP_SERVER/#tips-for-effective-use","title":"Tips for Effective Use","text":"<ol> <li> <p>Always load corpus first: Most tools require a loaded corpus to function.</p> </li> <li> <p>Use prompts for guidance: Request the <code>analysis_workflow</code> or <code>triangulation_guide</code> prompts for step-by-step instructions.</p> </li> <li> <p>Save frequently: Use <code>save_corpus</code> to preserve analysis metadata and relationships.</p> </li> <li> <p>Document relationships: Use <code>add_relationship</code> to link textual findings with numeric variables based on your theoretical framework.</p> </li> <li> <p>Iterate and refine: Analysis is iterative. Load your saved corpus and continue refining based on new insights.</p> </li> </ol>"},{"location":"MCP_SERVER/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MCP_SERVER/#no-corpus-loaded-error","title":"\"No corpus loaded\" error","text":"<p>Make sure to call <code>load_corpus</code> before using other tools.</p>"},{"location":"MCP_SERVER/#ml-dependencies-not-available-error","title":"\"ML dependencies not available\" error","text":"<p>Install the ML extras:</p> <pre><code>pip install crisp-t[ml]\n</code></pre>"},{"location":"MCP_SERVER/#server-not-appearing-in-claude-desktop","title":"Server not appearing in Claude Desktop","text":"<ol> <li>Check that the configuration file is in the correct location</li> <li>Verify the JSON syntax is valid</li> <li>Restart Claude Desktop after adding the configuration</li> <li>Check that <code>crisp-mcp</code> command is in your PATH</li> </ol>"},{"location":"MCP_SERVER/#additional-resources","title":"Additional Resources","text":"<ul> <li>CRISP-T Documentation</li> <li>INSTRUCTION.md - Detailed function reference</li> <li>Example Workflow</li> <li>GitHub Repository</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Feature/fix filter 1 #48 (dermatologist)</li> <li>Feature/parallel 1 #47 (dermatologist)</li> <li>Add subcommand support, unquoted syntax, and color formatting to --print option #46 (Copilot)</li> <li>Update README and add TDABM documentation for multidimensional data v\u2026 #44 (dermatologist)</li> <li>Implement TDABM \\(Topological Data Analysis Ball Mapper\\) for multidimensional data visualization #43 (Copilot)</li> <li>Update deps #41 (dermatologist)</li> <li>Replace sample CSV with dynamically created temporary CSV for data pr\u2026 #40 (dermatologist)</li> <li>Add LDA Visualization with pyLDAvis, Coherence and Perplexity Metrics #39 (Copilot)</li> <li>Add tqdm progress bars for long-running functions #37 (Copilot)</li> <li>Add LSTM text classification method to test convergence between text and outcome variables #35 (Copilot)</li> <li>Add semantic search based on document list for literature reviews #33 (Copilot)</li> <li>Add semantic chunk search for document coding and annotation #31 (Copilot)</li> <li>Bump astral-sh/setup-uv from 6 to 7 #29 (dependabot[bot])</li> <li>Add semantic search with ChromaDB and metadata export functionality #28 (Copilot)</li> <li>Add AGENTS.md file with comprehensive instructions for LLM agents #26 (Copilot)</li> <li>Update pytest.yml to support multiple OS and extend timeout; revise D\u2026 #25 (dermatologist)</li> <li>Feature/fix ml output 1 #24 (dermatologist)</li> <li>Feature/demo 1 #23 (dermatologist)</li> <li>Feature/copilot mcp fix 1 #22 (dermatologist)</li> <li>Add Linear and Logistic Regression Support to ML Module #19 (Copilot)</li> <li>Feature/vibe 1 #17 (dermatologist)</li> <li>Feature/utils 1 #16 (dermatologist)</li> <li>Feature/document meta 1 #15 (dermatologist)</li> <li>Feature/cli 1 #14 (dermatologist)</li> <li>Implement comprehensive CLI interface and documentation for CRISP-T framework #13 (Copilot)</li> <li>Feature/visualize 1 #11 (dermatologist)</li> <li>Feature/model 2 #10 (dermatologist)</li> <li>Update GitHub Actions to use actions/checkout@v5 across all workflows #9 (dermatologist)</li> <li>Feature/uv 1 #8 (dermatologist)</li> <li>Feature/readme 1 #4 (dermatologist)</li> <li>Bump astral-sh/setup-uv from 5 to 6 #2 (dependabot[bot])</li> </ul>"},{"location":"changelog/#v110-2025-10-28","title":"v1.1.0 (2025-10-28)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v100-2025-10-26","title":"v1.0.0 (2025-10-26)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Improve the --print command with more options and better display. #45</li> </ul>"},{"location":"changelog/#v090-2025-10-25","title":"v0.9.0 (2025-10-25)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Implement Topological Data Analysis Ball Mapper \\(TDABM\\) #42</li> </ul>"},{"location":"changelog/#v080-2025-10-22","title":"v0.8.0 (2025-10-22)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: LDA output Visualization for text mining #38</li> </ul>"},{"location":"changelog/#v070-2025-10-21","title":"v0.7.0 (2025-10-21)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Add a progress bar for long running functions. #36</li> </ul>"},{"location":"changelog/#v060-2025-10-19","title":"v0.6.0 (2025-10-19)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Add LSTM method to ml.py. #34</li> </ul>"},{"location":"changelog/#v050-2025-10-18","title":"v0.5.0 (2025-10-18)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Feature request: Semantic Search based on a list of documents #32</li> </ul>"},{"location":"changelog/#v040-2025-10-15","title":"v0.4.0 (2025-10-15)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Feature request: Semantic Search for a concept within a document #30</li> </ul>"},{"location":"changelog/#v030-2025-10-12","title":"v0.3.0 (2025-10-12)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Semantic Search with chromadb and metadata export as dataframe #27</li> </ul>"},{"location":"changelog/#v020-2025-10-10","title":"v0.2.0 (2025-10-10)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v010-2025-10-10","title":"v0.1.0 (2025-10-10)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Create an MCP server #20</li> <li>Feature request: Add Linear and Logistic regression #18</li> <li>Feature: Command line interface and Documentation. #12</li> </ul> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"contributing/","title":"Contributing to <code>crisp-t</code>","text":""},{"location":"contributing/#please-note","title":"Please note:","text":"<ul> <li>(Optional) We adopt Git Flow. Most feature branches are pushed to the repository and deleted when merged to develop branch.</li> <li>(Important): Submit pull requests to the develop branch or feature/ branches</li> <li>Use GitHub Issues for feature requests and bug reports. Include as much information as possible while reporting bugs.</li> </ul>"},{"location":"contributing/#contributing-step-by-step","title":"Contributing (Step-by-step)","text":"<ol> <li> <p>Fork the repo and clone it to your local computer, and set up the upstream remote:</p> <pre><code>git clone https://github.com/dermatologist/crisp-t.git\ncd crisp-t\ngit remote add upstream https://github.com/dermatologist/crisp-t.git\n</code></pre> </li> <li> <p>Checkout out a new local branch based on your master and update it to the latest (BRANCH-123 is the branch name, You can name it whatever you want. Try to give it a meaningful name. If you are fixing an issue, please include the issue #).</p> <pre><code>git checkout -b BRANCH-123 develop\ngit clean -df\ngit pull --rebase upstream develop\n</code></pre> </li> </ol> <p>Please keep your code clean. If you find another bug, you want to fix while being in a new branch, please fix it in a separated branch instead.</p> <ol> <li> <p>Push the branch to your fork. Treat it as a backup.</p> <pre><code>git push origin BRANCH-123\n</code></pre> </li> <li> <p>Code</p> </li> <li> <p>Adhere to common conventions you see in the existing code.</p> </li> <li> <p>Include tests as much as possible, and ensure they pass.</p> </li> <li> <p>Commit to your branch</p> <pre><code> git commit -m \"BRANCH-123: Put change summary here (can be a ticket title)\"\n</code></pre> </li> </ol> <p>NEVER leave the commit message blank! Provide a detailed, clear, and complete description of your commit!</p> <ol> <li> <p>Update your branch to the latest code.</p> <pre><code>git pull --rebase upstream develop\n</code></pre> </li> <li> <p>Important If you have made many commits, please squash them into atomic units of work. (Most Git GUIs such as sourcetree and smartgit offer a squash option)</p> <pre><code>git checkout develop\ngit pull --rebase upstream develop\ngit merge --squash BRANCH-123\ngit commit -m \"fix: 123\"\n</code></pre> </li> </ol> <p>Push changes to your fork:</p> <pre><code>    git push\n</code></pre> <ol> <li>Issue a Pull Request</li> </ol> <p>In order to make a pull request:   * Click \"Pull Request\".   * Choose the develop branch   * Click 'Create pull request'   * Fill in some details about your potential patch including a meaningful title.   * Click \"Create pull request\".</p> <p>Thanks for that -- we'll get to your pull request ASAP. We love pull requests!</p>"},{"location":"contributing/#feedback","title":"Feedback","text":"<p>If you need to contact me, see my contact details on my profile page.</p>"},{"location":"modules/","title":"Modules","text":"<p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"modules/#cli.main","title":"<code>main(verbose, covid, inp, out, csv, num, rec, unstructured, filters, codedict, topics, assign, cat, summary, sentiment, sentence, nlp, nnet, cls, knn, kmeans, cart, pca, regression, lstm, ml, visualize, ignore, include, outcome, source, sources, print_args, clear)</code>","text":"<p>CRISP-T: Cross Industry Standard Process for Triangulation.</p> <p>A comprehensive framework for analyzing textual and numerical data using advanced NLP, machine learning, and statistical techniques.</p> Source code in <code>src/crisp_t/cli.py</code> <pre><code>@click.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Print verbose messages.\")\n@click.option(\n    \"--covid\", \"-cf\", default=\"\", help=\"Download COVID narratives from the website\"\n)\n@click.option(\"--inp\", \"-i\", help=\"Load corpus from a folder containing corpus.json\")\n@click.option(\"--out\", \"-o\", help=\"Write corpus to a folder as corpus.json\")\n@click.option(\"--csv\", default=\"\", help=\"CSV file name\")\n@click.option(\n    \"--num\", \"-n\", default=3, help=\"N (clusters/epochs, etc, depending on context)\"\n)\n@click.option(\"--rec\", \"-r\", default=3, help=\"Record or top_n (based on context)\")\n@click.option(\n    \"--unstructured\",\n    \"-t\",\n    multiple=True,\n    help=\"Csv columns with text data that needs to be treated as text. (Ex. Free text comments)\",\n)\n@click.option(\n    \"--filters\",\n    \"-f\",\n    multiple=True,\n    help=\"Filters to apply as key=value (can be used multiple times)\",\n)\n@click.option(\"--codedict\", is_flag=True, help=\"Generate coding dictionary\")\n@click.option(\"--topics\", is_flag=True, help=\"Generate topic model\")\n@click.option(\"--assign\", is_flag=True, help=\"Assign documents to topics\")\n@click.option(\n    \"--cat\", is_flag=True, help=\"List categories of entire corpus or individual docs\"\n)\n@click.option(\n    \"--summary\",\n    is_flag=True,\n    help=\"Generate summary for entire corpus or individual docs\",\n)\n@click.option(\n    \"--sentiment\",\n    is_flag=True,\n    help=\"Generate sentiment score for entire corpus or individual docs\",\n)\n@click.option(\n    \"--sentence\",\n    is_flag=True,\n    default=False,\n    help=\"Generate sentence-level scores when applicable\",\n)\n@click.option(\"--nlp\", is_flag=True, help=\"Generate all NLP reports\")\n@click.option(\"--ml\", is_flag=True, help=\"Generate all ML reports\")\n@click.option(\"--nnet\", is_flag=True, help=\"Display accuracy of a neural network model\")\n@click.option(\n    \"--cls\",\n    is_flag=True,\n    help=\"Display confusion matrix from classifiers (SVM, Decision Tree)\",\n)\n@click.option(\"--knn\", is_flag=True, help=\"Display nearest neighbours\")\n@click.option(\"--kmeans\", is_flag=True, help=\"Display KMeans clusters\")\n@click.option(\"--cart\", is_flag=True, help=\"Display Association Rules\")\n@click.option(\"--pca\", is_flag=True, help=\"Display PCA\")\n@click.option(\n    \"--regression\", is_flag=True, help=\"Display linear or logistic regression results\"\n)\n@click.option(\"--lstm\", is_flag=True, help=\"Train LSTM model on text data to predict outcome variable\")\n@click.option(\"--visualize\", is_flag=True, help=\"Visualize words, topics or wordcloud\")\n@click.option(\n    \"--ignore\",\n    default=\"\",\n    help=\"Comma separated ignore words or columns depending on context\",\n)\n@click.option(\n    \"--include\", default=\"\", help=\"Comma separated columns to include from csv\"\n)\n@click.option(\"--outcome\", default=\"\", help=\"Outcome variable for ML tasks\")\n@click.option(\"--source\", \"-s\", help=\"Source URL or directory path to read data from\")\n@click.option(\"--print\", \"-p\", \"print_args\", multiple=True, help=\"Display corpus information. Usage: --print documents --print 10, or quoted: --print 'documents 10'\")\n@click.option(\n    \"--sources\",\n    multiple=True,\n    help=\"Multiple sources (URLs or directories) to read data from; can be used multiple times\",\n)\n@click.option(\"--clear\", is_flag=True, help=\"Clear cache before running analysis\")\ndef main(\n    verbose,\n    covid,\n    inp,\n    out,\n    csv,\n    num,\n    rec,\n    unstructured,\n    filters,\n    codedict,\n    topics,\n    assign,\n    cat,\n    summary,\n    sentiment,\n    sentence,\n    nlp,\n    nnet,\n    cls,\n    knn,\n    kmeans,\n    cart,\n    pca,\n    regression,\n    lstm,\n    ml,\n    visualize,\n    ignore,\n    include,\n    outcome,\n    source,\n    sources,\n    print_args,\n    clear,\n):\n    \"\"\"CRISP-T: Cross Industry Standard Process for Triangulation.\n\n    A comprehensive framework for analyzing textual and numerical data using\n    advanced NLP, machine learning, and statistical techniques.\n    \"\"\"\n\n    if verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n        click.echo(\"Verbose mode enabled\")\n\n    click.echo(\"_________________________________________\")\n    click.echo(\"CRISP-T: Qualitative Research Analysis Framework\")\n    click.echo(f\"Version: {__version__}\")\n    click.echo(\"_________________________________________\")\n\n    # Initialize components\n    read_data = ReadData()\n    corpus = None\n    text_analyzer = None\n    csv_analyzer = None\n    ml_analyzer = None\n\n    if clear:\n        _clear_cache()\n\n    try:\n        # Handle COVID data download\n        if covid:\n            if not source:\n                raise click.ClickException(\n                    \"--source (output folder) is required when using --covid.\"\n                )\n            click.echo(f\"Downloading COVID narratives from: {covid} to {source}\")\n            try:\n                from .utils import QRUtils\n\n                QRUtils.read_covid_narratives(source, covid)\n                click.echo(f\"\u2713 COVID narratives downloaded to {source}\")\n            except Exception as e:\n                raise click.ClickException(f\"COVID download failed: {e}\")\n\n        # Build corpus using helpers (source preferred over inp)\n        # if not source or inp, use default folders or env vars\n        try:\n            text_cols = \",\".join(unstructured) if unstructured else \"\"\n            corpus = initialize_corpus(\n                source=source,\n                inp=inp,\n                comma_separated_text_columns=text_cols,\n                comma_separated_ignore_words=(ignore if ignore else None),\n            )\n            # If filters were provided with ':' while using --source, emit guidance message\n            if source and filters:\n                if any(\":\" in flt and \"=\" not in flt for flt in filters):\n                    click.echo(\"Filters are not supported when using --source\")\n        except click.ClickException:\n            raise\n        except Exception as e:\n            click.echo(f\"\u2717 Error initializing corpus: {e}\", err=True)\n            logger.error(f\"Failed to initialize corpus: {e}\")\n            return\n\n        # Handle multiple sources (unchanged behavior, but no filters applied here)\n        if sources and not corpus:\n            loaded_any = False\n            for src in sources:\n                click.echo(f\"Reading data from source: {src}\")\n                try:\n                    read_data.read_source(\n                        src, comma_separated_ignore_words=ignore if ignore else None\n                    )\n                    loaded_any = True\n                except Exception as e:\n                    logger.error(f\"Failed to read source {src}: {e}\")\n                    raise click.ClickException(str(e))\n\n            if loaded_any:\n                corpus = read_data.create_corpus(\n                    name=\"Corpus from multiple sources\",\n                    description=f\"Data loaded from {len(sources)} sources\",\n                )\n                click.echo(\n                    f\"\u2713 Successfully loaded {len(corpus.documents)} document(s) from {len(sources)} sources\"\n                )\n                # Filters are not applied for --sources in bulk mode\n\n        # Load csv from corpus.df if available via helper\n        if corpus and getattr(corpus, \"df\", None) is not None:\n            try:\n                text_cols = \",\".join(unstructured) if unstructured else \"\"\n                csv_analyzer = get_csv_analyzer(\n                    corpus,\n                    comma_separated_unstructured_text_columns=text_cols,\n                    comma_separated_ignore_columns=(ignore if ignore else \"\"),\n                    filters=filters,\n                )\n            except Exception as e:\n                click.echo(f\"\u2717 Error preparing CSV analyzer: {e}\", err=True)\n                logger.error(f\"Failed to create CSV analyzer: {e}\")\n                return\n\n        # Load CSV data (deprecated)\n        if csv:\n            click.echo(\n                \"--csv option has been deprecated. Put csv file in --source folder instead.\"\n            )\n\n        # Initialize ML analyzer if available and ML functions are requested\n        if (\n            ML_AVAILABLE\n            and (nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml)\n            and csv_analyzer\n        ):\n            if include:\n                csv_analyzer.comma_separated_include_columns(include)\n            ml_analyzer = ML(csv=csv_analyzer)  # type: ignore\n        else:\n            if (nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml) and not ML_AVAILABLE:\n                click.echo(\"Machine learning features require additional dependencies.\")\n                click.echo(\"Install with: pip install crisp-t[ml]\")\n            if (nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml) and not csv_analyzer:\n                click.echo(\n                    \"ML analysis requires CSV data. Use --csv to provide a data file.\"\n                )\n\n        # Initialize Text analyzer and apply filters using helper if we have a corpus\n        if corpus and not text_analyzer:\n            text_analyzer = get_text_analyzer(corpus, filters=filters)\n\n        # Ensure we have data to work with\n        if not corpus and not csv_analyzer:\n            click.echo(\n                \"No input data provided. Use --inp for text files\"\n            )\n            return\n\n        # Text Analysis Operations\n        if text_analyzer:\n            if nlp or codedict:\n                click.echo(\"\\n=== Generating Coding Dictionary ===\")\n                click.echo(\n                    \"\"\"\n                Coding Dictionary Format:\n                - CATEGORY: Common verbs representing main actions or themes.\n                - PROPERTY: Common nouns associated with each CATEGORY.\n                - DIMENSION: Common adjectives, adverbs, or verbs associated with each PROPERTY.\n\n                Hint:   Use --ignore with a comma-separated list of words to exclude common but uninformative words.\n                        Use --filters to narrow down documents based on metadata.\n                        Use --num to adjust the number of categories displayed.\n                        Use --rec to adjust the number of top items displayed per section.\n                \"\"\"\n                )\n                try:\n                    text_analyzer.make_spacy_doc()\n                    coding_dict = text_analyzer.print_coding_dictionary(\n                        num=num, top_n=rec\n                    )\n                    if out:\n                        _save_output(coding_dict, out, \"coding_dictionary\")\n                except Exception as e:\n                    click.echo(f\"Error generating coding dictionary: {e}\")\n\n            if nlp or topics:\n                click.echo(\"\\n=== Topic Modeling ===\")\n                click.echo(\n                    \"\"\"\n                Topic Modeling Output Format:\n                Each topic is represented as a list of words with associated weights indicating their importance within the topic.\n                Example:\n                Topic 0: 0.116*\"category\" + 0.093*\"comparison\" + 0.070*\"incident\" + ...\n                Hint:   Use --num to adjust the number of topics generated.\n                        Use --filters to narrow down documents based on metadata.\n                        Use --rec to adjust the number of words displayed per topic.\n                \"\"\"\n                )\n                try:\n                    cluster_analyzer = Cluster(corpus=corpus)\n                    cluster_analyzer.build_lda_model(topics=num)\n                    topics_result = cluster_analyzer.print_topics(num_words=rec)\n                    click.echo(\n                        f\"Generated {len(topics_result)} topics as above with the weights in brackets.\"\n                    )\n                    if out:\n                        _save_output(topics_result, out, \"topics\")\n                except Exception as e:\n                    click.echo(f\"Error generating topics: {e}\")\n\n            if nlp or assign:\n                click.echo(\"\\n=== Document-Topic Assignments ===\")\n                click.echo(\n                    \"\"\"\n                Document-Topic Assignment Format:\n                Each document is assigned to the topic it is most associated with, along with the contribution percentage.\n                Hint: --visualize adds a DataFrame to corpus.visualization[\"assign_topics\"] for visualization.\n                \"\"\"\n                )\n                try:\n                    if \"cluster_analyzer\" not in locals():\n                        cluster_analyzer = Cluster(corpus=corpus)\n                        cluster_analyzer.build_lda_model(topics=num)\n                    assignments = cluster_analyzer.format_topics_sentences(\n                        visualize=visualize\n                    )\n                    document_assignments = cluster_analyzer.print_clusters()\n                    click.echo(f\"Assigned {len(assignments)} documents to topics\")\n                    if out:\n                        _save_output(assignments, out, \"topic_assignments\")\n                except Exception as e:\n                    click.echo(f\"Error assigning topics: {e}\")\n\n            if nlp or cat:\n                click.echo(\"\\n=== Category Analysis ===\")\n                click.echo(\n                    \"\"\"\n                Category Analysis Output Format:\n                           A list of common concepts or themes in \"bag_of_terms\" with corresponding weights.\n                Hint:   Use --num to adjust the number of categories displayed.\n                        Use --filters to narrow down documents based on metadata.\n                \"\"\"\n                )\n                try:\n                    text_analyzer.make_spacy_doc()\n                    categories = text_analyzer.print_categories(num=num)\n                    if out:\n                        _save_output(categories, out, \"categories\")\n                except Exception as e:\n                    click.echo(f\"Error generating categories: {e}\")\n\n            if nlp or summary:\n                click.echo(\"\\n=== Text Summarization ===\")\n                click.echo(\n                    \"\"\"\n                Text Summarization Output Format: A list of important sentences representing the main points of the text.\n                Hint:   Use --num to adjust the number of sentences in the summary.\n                        Use --filters to narrow down documents based on metadata.\n                \"\"\"\n                )\n                try:\n                    text_analyzer.make_spacy_doc()\n                    summary_result = text_analyzer.generate_summary(weight=num)\n                    click.echo(summary_result)\n                    if out:\n                        _save_output(summary_result, out, \"summary\")\n                except Exception as e:\n                    click.echo(f\"Error generating summary: {e}\")\n\n            if nlp or sentiment:\n                click.echo(\"\\n=== Sentiment Analysis ===\")\n                click.echo(\n                    \"\"\"\n                Sentiment Analysis Output Format:\n                           neg, neu, pos, compound scores.\n                Hint:   Use --filters to narrow down documents based on metadata.\n                        Use --sentence to get document-level sentiment scores.\n                \"\"\"\n                )\n                try:\n                    sentiment_analyzer = Sentiment(corpus=corpus)  # type: ignore\n                    sentiment_results = sentiment_analyzer.get_sentiment(\n                        documents=sentence, verbose=verbose\n                    )\n                    click.echo(sentiment_results)\n                    if out:\n                        _save_output(sentiment_results, out, \"sentiment\")\n                except Exception as e:\n                    click.echo(f\"Error generating sentiment analysis: {e}\")\n\n        # Machine Learning Operations\n        if ml_analyzer and ML_AVAILABLE:\n            target_col = outcome\n\n            if kmeans or ml:\n                click.echo(\"\\n=== K-Means Clustering ===\")\n                click.echo(\n                    \"\"\"\n                           K-Means clustering removes non-numeric columns.\n                           Additionally it removes NaN values.\n                           So combining with other ML options may not work as expected.\n                Hint:   Use --num to adjust the number of clusters generated.\n                \"\"\"\n                )\n                csv_analyzer.retain_numeric_columns_only()\n                csv_analyzer.drop_na()\n                _ml_analyzer = ML(csv=csv_analyzer)\n                clusters, members = _ml_analyzer.get_kmeans(\n                    number_of_clusters=num, verbose=verbose\n                )\n                _ml_analyzer.profile(members, number_of_clusters=num)\n                if out:\n                    _save_output(\n                        {\"clusters\": clusters, \"members\": members}, out, \"kmeans\"\n                    )\n\n            if (cls or ml) and target_col:\n                click.echo(\"\\n=== Classifier Evaluation ===\")\n                click.echo(\n                    \"\"\"\n                           Classifier\n                            - SVM: Support Vector Machine classifier with confusion matrix output.\n                            - Decision Tree: Decision Tree classifier with feature importance output.\n                Hint:   Use --outcome to specify the target variable for classification.\n                        Use --rec to adjust the number of top important features displayed.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for classification tasks\"\n                    )\n                click.echo(\"\\n=== SVM ===\")\n                try:\n                    confusion_matrix = ml_analyzer.svm_confusion_matrix(\n                        y=target_col, test_size=0.25\n                    )\n                    click.echo(\n                        ml_analyzer.format_confusion_matrix_to_human_readable(\n                            confusion_matrix\n                        )\n                    )\n                    if out:\n                        _save_output(confusion_matrix, out, \"svm_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing SVM classification: {e}\")\n                click.echo(\"\\n=== Decision Tree Classification ===\")\n                try:\n                    cm, importance = ml_analyzer.get_decision_tree_classes(\n                        y=target_col, top_n=rec\n                    )\n                    click.echo(\"\\n=== Feature Importance ===\")\n                    click.echo(\n                        ml_analyzer.format_confusion_matrix_to_human_readable(cm)\n                    )\n                    if out:\n                        _save_output(cm, out, \"decision_tree_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing Decision Tree classification: {e}\")\n\n            if (nnet or ml) and target_col:\n                click.echo(\"\\n=== Neural Network Classification Accuracy ===\")\n                click.echo(\n                    \"\"\"\n                            Neural Network classifier with accuracy output.\n                Hint:   Use --outcome to specify the target variable for classification.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for neural network tasks\"\n                    )\n                try:\n                    predictions = ml_analyzer.get_nnet_predictions(y=target_col)\n                    if out:\n                        _save_output(predictions, out, \"nnet_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing Neural Network classification: {e}\")\n\n            if (knn or ml) and target_col:\n                click.echo(\"\\n=== K-Nearest Neighbors ===\")\n                click.echo(\n                    \"\"\"\n                           K-Nearest Neighbors search results.\n                Hint:   Use --outcome to specify the target variable for KNN search.\n                        Use --rec to specify the record number to search from (1-based index).\n                        Use --num to specify the number of nearest neighbors to retrieve.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for KNN search tasks\"\n                    )\n                if rec &lt; 1:\n                    raise click.ClickException(\n                        \"--rec must be a positive integer (1-based index)\"\n                    )\n                try:\n                    knn_results = ml_analyzer.knn_search(y=target_col, n=num, r=rec)\n                    if out:\n                        _save_output(knn_results, out, \"knn_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing K-Nearest Neighbors search: {e}\")\n\n            if (cart or ml) and target_col:\n                click.echo(\"\\n=== Association Rules (CART) ===\")\n                click.echo(\n                    \"\"\"\n                           Association Rules using the Apriori algorithm.\n                Hint:   Use --outcome to specify the target variable to remove from features.\n                        Use --num to specify the minimum support (between 1 and 99).\n                        Use --rec to specify the minimum threshold for the rules (between 1 and 99).\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for association rules tasks\"\n                    )\n                if not (1 &lt;= num &lt;= 99):\n                    raise click.ClickException(\n                        \"--num must be between 1 and 99 for min_support\"\n                    )\n                if not (1 &lt;= rec &lt;= 99):\n                    raise click.ClickException(\n                        \"--rec must be between 1 and 99 for min_threshold\"\n                    )\n                _min_support = float(num / 100)\n                _min_threshold = float(rec / 100)\n                click.echo(\n                    f\"Using min_support={_min_support:.2f} and min_threshold={_min_threshold:.2f}\"\n                )\n                try:\n                    apriori_results = ml_analyzer.get_apriori(\n                        y=target_col,\n                        min_support=_min_support,\n                        min_threshold=_min_threshold,\n                    )\n                    click.echo(apriori_results)\n                    if out:\n                        _save_output(apriori_results, out, \"association_rules\")\n                except Exception as e:\n                    click.echo(f\"Error generating association rules: {e}\")\n\n            if (pca or ml) and target_col:\n                click.echo(\"\\n=== Principal Component Analysis ===\")\n                click.echo(\n                    \"\"\"\n                           Principal Component Analysis (PCA) results.\n                Hint:   Use --outcome to specify the target variable to remove from features.\n                        Use --num to specify the number of principal components to generate.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                try:\n                    pca_results = ml_analyzer.get_pca(y=target_col, n=num)\n                    if out:\n                        _save_output(pca_results, out, \"pca_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing Principal Component Analysis: {e}\")\n\n            if (regression or ml) and target_col:\n                click.echo(\"\\n=== Regression Analysis ===\")\n                click.echo(\n                    \"\"\"\n                           Regression Analysis (Linear or Logistic Regression).\n                           Automatically detects binary outcomes for logistic regression.\n                           Otherwise uses linear regression for continuous outcomes.\n                Hint:   Use --outcome to specify the target variable for regression.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                try:\n                    regression_results = ml_analyzer.get_regression(y=target_col)\n                    if out:\n                        _save_output(regression_results, out, \"regression_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing regression analysis: {e}\")\n\n            if (lstm or ml) and target_col:\n                click.echo(\"\\n=== LSTM Text Classification ===\")\n                click.echo(\n                    \"\"\"\n                           LSTM (Long Short-Term Memory) model for text-based prediction.\n                           Tests if text documents converge towards predicting the outcome variable.\n                           Requires both text documents and an 'id' column to align texts with outcome.\n                Hint:   Use --outcome to specify the target variable for LSTM prediction.\n                        The outcome should be binary (two classes).\n                        Ensure documents have IDs matching the 'id' column in your data.\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for LSTM prediction tasks\"\n                    )\n                try:\n                    lstm_results = ml_analyzer.get_lstm_predictions(y=target_col)\n                    if out:\n                        _save_output(lstm_results, out, \"lstm_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing LSTM prediction: {e}\")\n\n        elif (nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml) and not ML_AVAILABLE:\n            click.echo(\"Machine learning features require additional dependencies.\")\n            click.echo(\"Install with: pip install crisp-t[ml]\")\n\n        # Save corpus and csv if output path is specified\n        if out and corpus:\n            if filters and inp and out and inp == out:\n                raise click.ClickException(\n                    \"--out cannot be the same as --inp when using --filters. Please specify a different output folder to avoid overwriting input data.\"\n                )\n            if filters and ((not inp) or (not out)):\n                raise click.ClickException(\n                    \"Both --inp and --out must be specified when using --filters.\"\n                )\n            output_path = pathlib.Path(out)\n            # Allow both directory and a file path '.../corpus.json'\n            if output_path.suffix:\n                # Ensure parent exists\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n                save_base = output_path\n            else:\n                output_path.mkdir(parents=True, exist_ok=True)\n                save_base = output_path / \"corpus.json\"\n            read_data.write_corpus_to_json(str(save_base), corpus=corpus)\n            click.echo(f\"\u2713 Corpus and csv saved to {save_base}\")\n\n        if print_args and corpus:\n            click.echo(\"\\n=== Corpus Details ===\")\n            # Join the print arguments into a single string\n            print_command = \" \".join(print_args) if print_args else None\n            if print_command:\n                click.echo(corpus.pretty_print(show=print_command))\n\n        click.echo(\"\\n=== Analysis Complete ===\")\n\n    except click.ClickException:\n        # Let Click handle and set non-zero exit code\n        raise\n    except Exception as e:\n        # Convert unexpected exceptions to ClickException for non-zero exit code\n        if verbose:\n            import traceback\n\n            traceback.print_exc()\n        raise click.ClickException(str(e))\n</code></pre>"},{"location":"modules/#read_data.ReadData","title":"<code>ReadData</code>","text":"Source code in <code>src/crisp_t/read_data.py</code> <pre><code>class ReadData:\n\n    def __init__(self, corpus: Corpus | None = None, source=None):\n        self._corpus = corpus\n        self._source = source\n        self._documents = []\n        self._df = pd.DataFrame()\n\n    @property\n    def corpus(self):\n        \"\"\"\n        Get the corpus.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        self._corpus.documents = self._documents\n        self._corpus.df = self._df\n        return self._corpus\n\n    @property\n    def documents(self):\n        \"\"\"\n        Get the documents.\n        \"\"\"\n        if not self._documents:\n            raise ValueError(\"No documents found. Please read data first.\")\n        return self._documents\n\n    @property\n    def df(self):\n        \"\"\"\n        Get the dataframe.\n        \"\"\"\n        if self._df is None:\n            raise ValueError(\"No dataframe found. Please read data first.\")\n        return self._df\n\n    @corpus.setter\n    def corpus(self, value):\n        \"\"\"\n        Set the corpus.\n        \"\"\"\n        if not isinstance(value, Corpus):\n            raise ValueError(\"Value must be a Corpus object.\")\n        self._corpus = value\n\n    @documents.setter\n    def documents(self, value):\n        \"\"\"\n        Set the documents.\n        \"\"\"\n        if not isinstance(value, list):\n            raise ValueError(\"Value must be a list of Document objects.\")\n        for document in value:\n            if not isinstance(document, Document):\n                raise ValueError(\"Value must be a list of Document objects.\")\n        self._documents = value\n\n    @df.setter\n    def df(self, value):\n        \"\"\"\n        Set the dataframe.\n        \"\"\"\n        if not isinstance(value, pd.DataFrame):\n            raise ValueError(\"Value must be a pandas DataFrame.\")\n        self._df = value\n\n    def pretty_print(self):\n        \"\"\"\n        Pretty print the corpus.\n        \"\"\"\n        if not self._corpus:\n            self.create_corpus()\n        if self._corpus:\n            print(\n                self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"})\n            )\n            logger.info(\n                \"Corpus: %s\",\n                self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"}),\n            )\n        else:\n            logger.error(\"No corpus available to pretty print.\")\n\n    # TODO: Enforce only one corpus (Singleton pattern)\n    def create_corpus(self, name=None, description=None):\n        \"\"\"\n        Create a corpus from the documents and dataframe.\n        \"\"\"\n        if not self._documents:\n            raise ValueError(\"No documents found. Please read data first.\")\n        if self._corpus:\n            self._corpus.documents = self._documents\n            self._corpus.df = self._df\n        else:\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n            corpus_id = f\"corpus_{timestamp}\"\n            self._corpus = Corpus(\n                documents=self._documents,\n                df=self._df,\n                visualization={},\n                metadata={},\n                id=corpus_id,\n                score=0.0,\n                name=name,\n                description=description,\n            )\n        return self._corpus\n\n    def get_documents_from_corpus(self):\n        \"\"\"\n        Get the documents from the corpus.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        return self._corpus.documents\n\n    def get_document_by_id(self, doc_id):\n        \"\"\"\n        Get a document from the corpus by its ID. Uses parallel search for large corpora.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        documents = self._corpus.documents\n        if len(documents) &lt; 10:\n            for document in tqdm(documents, desc=\"Searching documents\", disable=True):\n                if document.id == doc_id:\n                    return document\n        else:\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(lambda doc: doc.id == doc_id, document): i\n                    for i, document in enumerate(documents)\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Searching documents (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        i = futures[future]\n                        found = future.result()\n                        pbar.update(1)\n                        if found:\n                            return documents[i]\n        raise ValueError(\"Document not found: %s\" % doc_id)\n\n    def write_corpus_to_json(self, file_path=\"\", corpus=None):\n        \"\"\"\n        Write the corpus to a json file.\n\n        Accepts either a directory path or an explicit file path ending with\n        'corpus.json'. In both cases, a sibling 'corpus_df.csv' will be written\n        next to the json if a DataFrame is available.\n        \"\"\"\n        from pathlib import Path\n\n        path = Path(file_path)\n        # Determine targets\n        if path.suffix:  # treat as explicit file path\n            file_name = path\n            df_name = path.with_name(\"corpus_df.csv\")\n        else:\n            file_name = path / \"corpus.json\"\n            df_name = path / \"corpus_df.csv\"\n\n        corp = corpus if corpus is not None else self._corpus\n        if not corp:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        file_name.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_name, \"w\") as f:\n            json.dump(corp.model_dump(exclude={\"df\", \"visualization\"}), f, indent=4)\n        if corp.df is not None and isinstance(corp.df, pd.DataFrame):\n            if not corp.df.empty:\n                corp.df.to_csv(df_name, index=False)\n        logger.info(\"Corpus written to %s\", file_name)\n\n    # @lru_cache(maxsize=3)\n    def read_corpus_from_json(self, file_path=\"\", comma_separated_ignore_words=\"\"):\n        \"\"\"\n        Read the corpus from a json file. Parallelizes ignore word removal for large corpora.\n        \"\"\"\n        from pathlib import Path\n\n        file_path = Path(file_path)\n        file_name = file_path / \"corpus.json\"\n        df_name = file_path / \"corpus_df.csv\"\n        if self._source:\n            file_name = Path(self._source) / file_name\n        if not file_name.exists():\n            raise ValueError(f\"File not found: {file_name}\")\n        with open(file_name, \"r\") as f:\n            data = json.load(f)\n            self._corpus = Corpus.model_validate(data)\n            logger.info(f\"Corpus read from {file_name}\")\n        if df_name.exists():\n            self._corpus.df = pd.read_csv(df_name)\n        else:\n            self._corpus.df = None\n        # Remove ignore words from self._corpus.documents text\n        documents = self._corpus.documents\n        if len(documents) &lt; 10:\n            processed_docs = []\n            for document in tqdm(documents, desc=\"Processing documents\", disable=True):\n                if comma_separated_ignore_words:\n                    for word in comma_separated_ignore_words.split(\",\"):\n                        document.text = re.sub(\n                            r\"\\b\" + word.strip() + r\"\\b\",\n                            \"\",\n                            document.text,\n                            flags=re.IGNORECASE,\n                        )\n                processed_docs.append(document)\n        else:\n\n            def process_doc(document):\n                if comma_separated_ignore_words:\n                    for word in comma_separated_ignore_words.split(\",\"):\n                        document.text = re.sub(\n                            r\"\\b\" + word.strip() + r\"\\b\",\n                            \"\",\n                            document.text,\n                            flags=re.IGNORECASE,\n                        )\n                return document\n\n            processed_docs = []\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(process_doc, document): document\n                    for document in documents\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Processing documents (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        processed_docs.append(future.result())\n                        pbar.update(1)\n        self._corpus.documents = processed_docs\n        return self._corpus\n\n    # @lru_cache(maxsize=3)\n    def read_csv_to_corpus(\n        self,\n        file_name,\n        comma_separated_ignore_words=None,\n        comma_separated_text_columns=\"\",\n        id_column=\"\",\n    ):\n        \"\"\"\n        Read the corpus from a csv file. Parallelizes document creation for large CSVs.\n        \"\"\"\n        from pathlib import Path\n\n        file_name = Path(file_name)\n        if not file_name.exists():\n            raise ValueError(f\"File not found: {file_name}\")\n        df = pd.read_csv(file_name)\n        original_df = df.copy()\n        if comma_separated_text_columns:\n            text_columns = comma_separated_text_columns.split(\",\")\n        else:\n            text_columns = []\n        # remove text columns from the dataframe\n        for column in text_columns:\n            if column in df.columns:\n                df.drop(column, axis=1, inplace=True)\n        # Set self._df to the numeric part after dropping text columns\n        self._df = df.copy()\n        rows = list(original_df.iterrows())\n\n        def create_document(args):\n            index, row = args\n            read_from_file = \"\"\n            for column in text_columns:\n                read_from_file += f\"{row[column]} \"\n            # remove comma separated ignore words\n            if comma_separated_ignore_words:\n                for word in comma_separated_ignore_words.split(\",\"):\n                    read_from_file = re.sub(\n                        r\"\\b\" + word.strip() + r\"\\b\",\n                        \"\",\n                        read_from_file,\n                        flags=re.IGNORECASE,\n                    )\n            _document = Document(\n                text=read_from_file,\n                metadata={\n                    \"source\": str(file_name),\n                    \"file_name\": str(file_name),\n                    \"row\": index,\n                    \"id\": (\n                        row[id_column]\n                        if (id_column != \"\" and id_column in original_df.columns)\n                        else index\n                    ),\n                },\n                id=str(index),\n                score=0.0,\n                name=\"\",\n                description=\"\",\n            )\n            return read_from_file, _document\n\n        if len(rows) &lt; 10:\n            results = [\n                create_document(args)\n                for args in tqdm(rows, desc=\"Reading CSV rows\", disable=True)\n            ]\n        else:\n\n            results = []\n            # import multiprocessing\n\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(create_document, args): args for args in rows\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Reading CSV rows (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        results.append(future.result())\n                        pbar.update(1)\n\n        if len(results) &lt; 10:\n            for read_from_file, _document in tqdm(\n                results, desc=\"Finalizing corpus\", disable=True\n            ):\n                self._documents.append(_document)\n        else:\n\n            # import multiprocessing\n\n            n_cores = multiprocessing.cpu_count()\n            with tqdm(\n                results,\n                total=len(results),\n                desc=f\"Finalizing corpus (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for read_from_file, _document in pbar:\n                    self._documents.append(_document)\n        logger.info(f\"Corpus read from {file_name}\")\n        self.create_corpus()\n        return self._corpus\n\n    def read_source(\n        self, source, comma_separated_ignore_words=None, comma_separated_text_columns=\"\"\n    ):\n        _CSV_EXISTS = False\n        # if source is a url\n        if source.startswith(\"http://\") or source.startswith(\"https://\"):\n            response = requests.get(source)\n            if response.status_code == 200:\n                read_from_file = response.text\n                # remove comma separated ignore words\n                if comma_separated_ignore_words:\n                    for word in comma_separated_ignore_words.split(\",\"):\n                        read_from_file = re.sub(\n                            r\"\\b\" + word.strip() + r\"\\b\",\n                            \"\",\n                            read_from_file,\n                            flags=re.IGNORECASE,\n                        )\n                # self._content removed\n                _document = Document(\n                    text=read_from_file,\n                    metadata={\"source\": source},\n                    id=source,\n                    score=0.0,\n                    name=\"\",\n                    description=\"\",\n                )\n                self._documents.append(_document)\n        elif os.path.exists(source):\n            source_path = Path(source)\n            self._source = source\n            logger.info(f\"Reading data from folder: {source}\")\n            file_list = os.listdir(source)\n            for file_name in tqdm(\n                file_list, desc=\"Reading files\", disable=len(file_list) &lt; 10\n            ):\n                file_path = source_path / file_name\n                if file_name.endswith(\".txt\"):\n                    with open(file_path, \"r\") as f:\n                        read_from_file = f.read()\n                        # remove comma separated ignore words\n                        if comma_separated_ignore_words:\n                            for word in comma_separated_ignore_words.split(\",\"):\n                                read_from_file = re.sub(\n                                    r\"\\b\" + word.strip() + r\"\\b\",\n                                    \"\",\n                                    read_from_file,\n                                    flags=re.IGNORECASE,\n                                )\n                        # self._content removed\n                        _document = Document(\n                            text=read_from_file,\n                            metadata={\n                                \"source\": str(file_path),\n                                \"file_name\": file_name,\n                            },\n                            id=file_name,\n                            score=0.0,\n                            name=\"\",\n                            description=\"\",\n                        )\n                        self._documents.append(_document)\n                if file_name.endswith(\".pdf\"):\n                    with open(file_path, \"rb\") as f:\n                        reader = PdfReader(f)\n                        read_from_file = \"\"\n                        for page in tqdm(\n                            reader.pages,\n                            desc=f\"Reading PDF {file_name}\",\n                            leave=False,\n                            disable=len(reader.pages) &lt; 10,\n                        ):\n                            read_from_file += page.extract_text()\n                        # remove comma separated ignore words\n                        if comma_separated_ignore_words:\n                            for word in comma_separated_ignore_words.split(\",\"):\n                                read_from_file = re.sub(\n                                    r\"\\b\" + word.strip() + r\"\\b\",\n                                    \"\",\n                                    read_from_file,\n                                    flags=re.IGNORECASE,\n                                )\n                        # self._content removed\n                        _document = Document(\n                            text=read_from_file,\n                            metadata={\n                                \"source\": str(file_path),\n                                \"file_name\": file_name,\n                            },\n                            id=file_name,\n                            score=0.0,\n                            name=\"\",\n                            description=\"\",\n                        )\n                        self._documents.append(_document)\n                if file_name.endswith(\".csv\") and comma_separated_text_columns == \"\":\n                    logger.info(f\"Reading CSV file: {file_path}\")\n                    self._df = Csv().read_csv(file_path)\n                    logger.info(f\"CSV file read with shape: {self._df.shape}\")\n                    _CSV_EXISTS = True\n                if file_name.endswith(\".csv\") and comma_separated_text_columns != \"\":\n                    logger.info(f\"Reading CSV file to corpus: {file_path}\")\n                    self.read_csv_to_corpus(\n                        file_path,\n                        comma_separated_ignore_words,\n                        comma_separated_text_columns,\n                    )\n                    logger.info(\n                        f\"CSV file read to corpus with documents: {len(self._documents)}\"\n                    )\n                    _CSV_EXISTS = True\n            if not _CSV_EXISTS:\n                # create a simple csv with columns: id, number, text\n                # and fill it with random data\n                _csv = \"\"\"\nid,number,response\n1,100,Sample text one\n2,200,Sample text two\n3,300,Sample text three\n4,400,Sample text four\n\"\"\"\n                # write the csv to a temp file\n                with tempfile.NamedTemporaryFile(\n                    mode=\"w+\", delete=False, suffix=\".csv\"\n                ) as temp_csv:\n                    temp_csv.write(_csv)\n                    temp_csv_path = temp_csv.name\n                logger.info(f\"No CSV found. Created temp CSV file: {temp_csv_path}\")\n                self._df = Csv().read_csv(temp_csv_path)\n                logger.info(f\"CSV file read with shape: {self._df.shape}\")\n                # remove the temp file\n                os.remove(temp_csv_path)\n\n        else:\n            raise ValueError(f\"Source not found: {source}\")\n\n    def corpus_as_dataframe(self):\n        \"\"\"\n        Convert the corpus to a pandas dataframe. Parallelizes for large corpora.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        documents = self._corpus.documents\n        if len(documents) &lt; 10:\n            data = [\n                document.model_dump()\n                for document in tqdm(\n                    documents, desc=\"Converting to dataframe\", disable=True\n                )\n            ]\n        else:\n            data = []\n\n            def dump_doc(document):\n                return document.model_dump()\n\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(dump_doc, document): document\n                    for document in documents\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Converting to dataframe (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        data.append(future.result())\n                        pbar.update(1)\n        df = pd.DataFrame(data)\n        return df\n</code></pre>"},{"location":"modules/#read_data.ReadData.corpus","title":"<code>corpus</code>  <code>property</code> <code>writable</code>","text":"<p>Get the corpus.</p>"},{"location":"modules/#read_data.ReadData.df","title":"<code>df</code>  <code>property</code> <code>writable</code>","text":"<p>Get the dataframe.</p>"},{"location":"modules/#read_data.ReadData.documents","title":"<code>documents</code>  <code>property</code> <code>writable</code>","text":"<p>Get the documents.</p>"},{"location":"modules/#read_data.ReadData.corpus_as_dataframe","title":"<code>corpus_as_dataframe()</code>","text":"<p>Convert the corpus to a pandas dataframe. Parallelizes for large corpora.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def corpus_as_dataframe(self):\n    \"\"\"\n    Convert the corpus to a pandas dataframe. Parallelizes for large corpora.\n    \"\"\"\n    if not self._corpus:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    documents = self._corpus.documents\n    if len(documents) &lt; 10:\n        data = [\n            document.model_dump()\n            for document in tqdm(\n                documents, desc=\"Converting to dataframe\", disable=True\n            )\n        ]\n    else:\n        data = []\n\n        def dump_doc(document):\n            return document.model_dump()\n\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(dump_doc, document): document\n                for document in documents\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Converting to dataframe (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    data.append(future.result())\n                    pbar.update(1)\n    df = pd.DataFrame(data)\n    return df\n</code></pre>"},{"location":"modules/#read_data.ReadData.create_corpus","title":"<code>create_corpus(name=None, description=None)</code>","text":"<p>Create a corpus from the documents and dataframe.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def create_corpus(self, name=None, description=None):\n    \"\"\"\n    Create a corpus from the documents and dataframe.\n    \"\"\"\n    if not self._documents:\n        raise ValueError(\"No documents found. Please read data first.\")\n    if self._corpus:\n        self._corpus.documents = self._documents\n        self._corpus.df = self._df\n    else:\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n        corpus_id = f\"corpus_{timestamp}\"\n        self._corpus = Corpus(\n            documents=self._documents,\n            df=self._df,\n            visualization={},\n            metadata={},\n            id=corpus_id,\n            score=0.0,\n            name=name,\n            description=description,\n        )\n    return self._corpus\n</code></pre>"},{"location":"modules/#read_data.ReadData.get_document_by_id","title":"<code>get_document_by_id(doc_id)</code>","text":"<p>Get a document from the corpus by its ID. Uses parallel search for large corpora.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def get_document_by_id(self, doc_id):\n    \"\"\"\n    Get a document from the corpus by its ID. Uses parallel search for large corpora.\n    \"\"\"\n    if not self._corpus:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    documents = self._corpus.documents\n    if len(documents) &lt; 10:\n        for document in tqdm(documents, desc=\"Searching documents\", disable=True):\n            if document.id == doc_id:\n                return document\n    else:\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(lambda doc: doc.id == doc_id, document): i\n                for i, document in enumerate(documents)\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Searching documents (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    i = futures[future]\n                    found = future.result()\n                    pbar.update(1)\n                    if found:\n                        return documents[i]\n    raise ValueError(\"Document not found: %s\" % doc_id)\n</code></pre>"},{"location":"modules/#read_data.ReadData.get_documents_from_corpus","title":"<code>get_documents_from_corpus()</code>","text":"<p>Get the documents from the corpus.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def get_documents_from_corpus(self):\n    \"\"\"\n    Get the documents from the corpus.\n    \"\"\"\n    if not self._corpus:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    return self._corpus.documents\n</code></pre>"},{"location":"modules/#read_data.ReadData.pretty_print","title":"<code>pretty_print()</code>","text":"<p>Pretty print the corpus.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def pretty_print(self):\n    \"\"\"\n    Pretty print the corpus.\n    \"\"\"\n    if not self._corpus:\n        self.create_corpus()\n    if self._corpus:\n        print(\n            self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"})\n        )\n        logger.info(\n            \"Corpus: %s\",\n            self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"}),\n        )\n    else:\n        logger.error(\"No corpus available to pretty print.\")\n</code></pre>"},{"location":"modules/#read_data.ReadData.read_corpus_from_json","title":"<code>read_corpus_from_json(file_path='', comma_separated_ignore_words='')</code>","text":"<p>Read the corpus from a json file. Parallelizes ignore word removal for large corpora.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def read_corpus_from_json(self, file_path=\"\", comma_separated_ignore_words=\"\"):\n    \"\"\"\n    Read the corpus from a json file. Parallelizes ignore word removal for large corpora.\n    \"\"\"\n    from pathlib import Path\n\n    file_path = Path(file_path)\n    file_name = file_path / \"corpus.json\"\n    df_name = file_path / \"corpus_df.csv\"\n    if self._source:\n        file_name = Path(self._source) / file_name\n    if not file_name.exists():\n        raise ValueError(f\"File not found: {file_name}\")\n    with open(file_name, \"r\") as f:\n        data = json.load(f)\n        self._corpus = Corpus.model_validate(data)\n        logger.info(f\"Corpus read from {file_name}\")\n    if df_name.exists():\n        self._corpus.df = pd.read_csv(df_name)\n    else:\n        self._corpus.df = None\n    # Remove ignore words from self._corpus.documents text\n    documents = self._corpus.documents\n    if len(documents) &lt; 10:\n        processed_docs = []\n        for document in tqdm(documents, desc=\"Processing documents\", disable=True):\n            if comma_separated_ignore_words:\n                for word in comma_separated_ignore_words.split(\",\"):\n                    document.text = re.sub(\n                        r\"\\b\" + word.strip() + r\"\\b\",\n                        \"\",\n                        document.text,\n                        flags=re.IGNORECASE,\n                    )\n            processed_docs.append(document)\n    else:\n\n        def process_doc(document):\n            if comma_separated_ignore_words:\n                for word in comma_separated_ignore_words.split(\",\"):\n                    document.text = re.sub(\n                        r\"\\b\" + word.strip() + r\"\\b\",\n                        \"\",\n                        document.text,\n                        flags=re.IGNORECASE,\n                    )\n            return document\n\n        processed_docs = []\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(process_doc, document): document\n                for document in documents\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Processing documents (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    processed_docs.append(future.result())\n                    pbar.update(1)\n    self._corpus.documents = processed_docs\n    return self._corpus\n</code></pre>"},{"location":"modules/#read_data.ReadData.read_csv_to_corpus","title":"<code>read_csv_to_corpus(file_name, comma_separated_ignore_words=None, comma_separated_text_columns='', id_column='')</code>","text":"<p>Read the corpus from a csv file. Parallelizes document creation for large CSVs.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def read_csv_to_corpus(\n    self,\n    file_name,\n    comma_separated_ignore_words=None,\n    comma_separated_text_columns=\"\",\n    id_column=\"\",\n):\n    \"\"\"\n    Read the corpus from a csv file. Parallelizes document creation for large CSVs.\n    \"\"\"\n    from pathlib import Path\n\n    file_name = Path(file_name)\n    if not file_name.exists():\n        raise ValueError(f\"File not found: {file_name}\")\n    df = pd.read_csv(file_name)\n    original_df = df.copy()\n    if comma_separated_text_columns:\n        text_columns = comma_separated_text_columns.split(\",\")\n    else:\n        text_columns = []\n    # remove text columns from the dataframe\n    for column in text_columns:\n        if column in df.columns:\n            df.drop(column, axis=1, inplace=True)\n    # Set self._df to the numeric part after dropping text columns\n    self._df = df.copy()\n    rows = list(original_df.iterrows())\n\n    def create_document(args):\n        index, row = args\n        read_from_file = \"\"\n        for column in text_columns:\n            read_from_file += f\"{row[column]} \"\n        # remove comma separated ignore words\n        if comma_separated_ignore_words:\n            for word in comma_separated_ignore_words.split(\",\"):\n                read_from_file = re.sub(\n                    r\"\\b\" + word.strip() + r\"\\b\",\n                    \"\",\n                    read_from_file,\n                    flags=re.IGNORECASE,\n                )\n        _document = Document(\n            text=read_from_file,\n            metadata={\n                \"source\": str(file_name),\n                \"file_name\": str(file_name),\n                \"row\": index,\n                \"id\": (\n                    row[id_column]\n                    if (id_column != \"\" and id_column in original_df.columns)\n                    else index\n                ),\n            },\n            id=str(index),\n            score=0.0,\n            name=\"\",\n            description=\"\",\n        )\n        return read_from_file, _document\n\n    if len(rows) &lt; 10:\n        results = [\n            create_document(args)\n            for args in tqdm(rows, desc=\"Reading CSV rows\", disable=True)\n        ]\n    else:\n\n        results = []\n        # import multiprocessing\n\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(create_document, args): args for args in rows\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Reading CSV rows (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    results.append(future.result())\n                    pbar.update(1)\n\n    if len(results) &lt; 10:\n        for read_from_file, _document in tqdm(\n            results, desc=\"Finalizing corpus\", disable=True\n        ):\n            self._documents.append(_document)\n    else:\n\n        # import multiprocessing\n\n        n_cores = multiprocessing.cpu_count()\n        with tqdm(\n            results,\n            total=len(results),\n            desc=f\"Finalizing corpus (parallel, {n_cores} cores)\",\n        ) as pbar:\n            for read_from_file, _document in pbar:\n                self._documents.append(_document)\n    logger.info(f\"Corpus read from {file_name}\")\n    self.create_corpus()\n    return self._corpus\n</code></pre>"},{"location":"modules/#read_data.ReadData.write_corpus_to_json","title":"<code>write_corpus_to_json(file_path='', corpus=None)</code>","text":"<p>Write the corpus to a json file.</p> <p>Accepts either a directory path or an explicit file path ending with 'corpus.json'. In both cases, a sibling 'corpus_df.csv' will be written next to the json if a DataFrame is available.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def write_corpus_to_json(self, file_path=\"\", corpus=None):\n    \"\"\"\n    Write the corpus to a json file.\n\n    Accepts either a directory path or an explicit file path ending with\n    'corpus.json'. In both cases, a sibling 'corpus_df.csv' will be written\n    next to the json if a DataFrame is available.\n    \"\"\"\n    from pathlib import Path\n\n    path = Path(file_path)\n    # Determine targets\n    if path.suffix:  # treat as explicit file path\n        file_name = path\n        df_name = path.with_name(\"corpus_df.csv\")\n    else:\n        file_name = path / \"corpus.json\"\n        df_name = path / \"corpus_df.csv\"\n\n    corp = corpus if corpus is not None else self._corpus\n    if not corp:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    file_name.parent.mkdir(parents=True, exist_ok=True)\n    with open(file_name, \"w\") as f:\n        json.dump(corp.model_dump(exclude={\"df\", \"visualization\"}), f, indent=4)\n    if corp.df is not None and isinstance(corp.df, pd.DataFrame):\n        if not corp.df.empty:\n            corp.df.to_csv(df_name, index=False)\n    logger.info(\"Corpus written to %s\", file_name)\n</code></pre>"},{"location":"modules/#corpuscli.main","title":"<code>main(verbose, id, name, description, docs, remove_docs, metas, relationships, clear_rel, print_corpus, out, inp, df_cols, df_row_count, df_row, doc_ids, doc_id, print_relationships, relationships_for_keyword, semantic, similar_docs, num, semantic_chunks, rec, metadata_df, metadata_keys, tdabm)</code>","text":"<p>CRISP-T Corpus CLI: create and manipulate a corpus quickly from the command line.</p> Source code in <code>src/crisp_t/corpuscli.py</code> <pre><code>@click.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Print verbose messages.\")\n@click.option(\"--id\", help=\"Unique identifier for the corpus.\")\n@click.option(\"--name\", default=None, help=\"Name of the corpus.\")\n@click.option(\"--description\", default=None, help=\"Description of the corpus.\")\n@click.option(\n    \"--doc\",\n    \"docs\",\n    multiple=True,\n    help=(\n        \"Add a document as 'id|name|text' (or 'id|text' if name omitted). \"\n        \"Can be used multiple times.\"\n    ),\n)\n@click.option(\n    \"--remove-doc\",\n    \"remove_docs\",\n    multiple=True,\n    help=\"Remove a document by its ID (can be used multiple times).\",\n)\n@click.option(\n    \"--meta\",\n    \"metas\",\n    multiple=True,\n    help=\"Add or update corpus metadata as key=value (can be used multiple times).\",\n)\n@click.option(\n    \"--add-rel\",\n    \"relationships\",\n    multiple=True,\n    help=(\n        \"Add a relationship as 'first|second|relation' (e.g., text:term|numb:col|correlates).\"\n    ),\n)\n@click.option(\n    \"--clear-rel\",\n    is_flag=True,\n    help=\"Clear all relationships in the corpus metadata.\",\n)\n@click.option(\"--print\", \"print_corpus\", is_flag=True, help=\"Pretty print the corpus\")\n@click.option(\n    \"--out\", default=None, help=\"Write corpus to a folder or file as corpus.json (save)\"\n)\n@click.option(\n    \"--inp\",\n    default=None,\n    help=\"Load corpus from a folder or file containing corpus.json (load)\",\n)\n# New options for Corpus methods\n@click.option(\"--df-cols\", is_flag=True, help=\"Print all DataFrame column names.\")\n@click.option(\"--df-row-count\", is_flag=True, help=\"Print number of rows in DataFrame.\")\n@click.option(\"--df-row\", default=None, type=int, help=\"Print DataFrame row by index.\")\n@click.option(\"--doc-ids\", is_flag=True, help=\"Print all document IDs in the corpus.\")\n@click.option(\"--doc-id\", default=None, help=\"Print document by ID.\")\n@click.option(\n    \"--relationships\",\n    \"print_relationships\",\n    is_flag=True,\n    help=\"Print all relationships in the corpus.\",\n)\n@click.option(\n    \"--relationships-for-keyword\",\n    default=None,\n    help=\"Print all relationships involving a specific keyword.\",\n)\n@click.option(\n    \"--semantic\",\n    default=None,\n    help=\"Perform semantic search with the given query string. Returns similar documents.\",\n)\n@click.option(\n    \"--similar-docs\",\n    default=None,\n    help=\"Find documents similar to a comma-separated list of document IDs. Use with --num and --rec. Useful for literature reviews.\",\n)\n@click.option(\n    \"--num\",\n    default=5,\n    type=int,\n    help=\"Number of results to return (default: 5). Used for semantic search and similar documents search.\",\n)\n@click.option(\n    \"--semantic-chunks\",\n    default=None,\n    help=\"Perform semantic search on document chunks. Returns matching chunks for a specific document. Use with --doc-id and --rec (threshold).\",\n)\n@click.option(\n    \"--rec\",\n    default=0.4,\n    type=float,\n    help=\"Threshold for semantic search (0-1, default: 0.4). Only chunks with similarity above this value are returned.\",\n)\n@click.option(\n    \"--metadata-df\",\n    is_flag=True,\n    help=\"Export collection metadata as DataFrame. Requires semantic search to be initialized first.\",\n)\n@click.option(\n    \"--metadata-keys\",\n    default=None,\n    help=\"Comma-separated list of metadata keys to include in DataFrame export.\",\n)\n@click.option(\n    \"--tdabm\",\n    default=None,\n    help=\"Perform TDABM analysis. Format: 'y_variable:x_variables:radius' (e.g., 'satisfaction:age,income:0.3'). Radius defaults to 0.3 if omitted.\",\n)\ndef main(\n    verbose: bool,\n    id: Optional[str],\n    name: Optional[str],\n    description: Optional[str],\n    docs: tuple[str, ...],\n    remove_docs: tuple[str, ...],\n    metas: tuple[str, ...],\n    relationships: tuple[str, ...],\n    clear_rel: bool,\n    print_corpus: bool,\n    out: Optional[str],\n    inp: Optional[str],\n    df_cols: bool,\n    df_row_count: bool,\n    df_row: Optional[int],\n    doc_ids: bool,\n    doc_id: Optional[str],\n    print_relationships: bool,\n    relationships_for_keyword: Optional[str],\n    semantic: Optional[str],\n    similar_docs: Optional[str],\n    num: int,\n    semantic_chunks: Optional[str],\n    rec: float,\n    metadata_df: bool,\n    metadata_keys: Optional[str],\n    tdabm: Optional[str],\n):\n    \"\"\"\n    CRISP-T Corpus CLI: create and manipulate a corpus quickly from the command line.\n    \"\"\"\n    logging.basicConfig(level=(logging.DEBUG if verbose else logging.WARNING))\n    logger = logging.getLogger(__name__)\n\n    if verbose:\n        click.echo(\"Verbose mode enabled\")\n\n    click.echo(\"_________________________________________\")\n    click.echo(\"CRISP-T: Corpus CLI\")\n    click.echo(\"_________________________________________\")\n\n    # Load corpus from --inp if provided\n    corpus = initialize_corpus(inp=inp)\n    if not corpus:\n        # Build initial corpus from CLI args\n        if not id:\n            raise click.ClickException(\"--id is required when not using --inp.\")\n        corpus = Corpus(\n            id=id,\n            name=name,\n            description=description,\n            score=None,\n            documents=[],\n            df=None,\n            visualization={},\n            metadata={},\n        )\n\n    # Add documents\n    for d in docs:\n        doc_id, doc_name, doc_text = _parse_doc(d)\n        document = Document(\n            id=doc_id,\n            name=doc_name,\n            description=None,\n            score=0.0,\n            text=doc_text,\n            metadata={},\n        )\n        corpus.add_document(document)\n    if docs:\n        click.echo(f\"\u2713 Added {len(docs)} document(s)\")\n\n    # Remove documents\n    for rid in remove_docs:\n        corpus.remove_document_by_id(rid)\n    if remove_docs:\n        click.echo(f\"\u2713 Removed {len(remove_docs)} document(s)\")\n\n    # Update metadata\n    for m in metas:\n        k, v = _parse_kv(m)\n        corpus.update_metadata(k, v)\n    if metas:\n        click.echo(f\"\u2713 Updated metadata entries: {len(metas)}\")\n\n    # Relationships\n    for r in relationships:\n        first, second, relation = _parse_relationship(r)\n        corpus.add_relationship(first, second, relation)\n    if relationships:\n        click.echo(f\"\u2713 Added {len(relationships)} relationship(s)\")\n    if clear_rel:\n        corpus.clear_relationships()\n        click.echo(\"\u2713 Cleared relationships\")\n\n    # Print DataFrame column names\n    if df_cols:\n        cols = corpus.get_all_df_column_names()\n        click.echo(f\"DataFrame columns: {cols}\")\n\n    # Print DataFrame row count\n    if df_row_count:\n        count = corpus.get_row_count()\n        click.echo(f\"DataFrame row count: {count}\")\n\n    # Print DataFrame row by index\n    if df_row is not None:\n        row = corpus.get_row_by_index(df_row)\n        if row is not None:\n            click.echo(f\"DataFrame row {df_row}: {row.to_dict()}\")\n        else:\n            click.echo(f\"No row at index {df_row}\")\n\n    # Print all document IDs\n    if doc_ids:\n        ids = corpus.get_all_document_ids()\n        click.echo(f\"Document IDs: {ids}\")\n\n    # Print document by ID\n    if doc_id:\n        doc = corpus.get_document_by_id(doc_id)\n        if doc:\n            click.echo(f\"Document {doc_id}: {doc.model_dump()}\")\n        else:\n            click.echo(f\"No document found with ID {doc_id}\")\n            exit(0)\n\n    # Print relationships\n    if print_relationships:\n        rels = corpus.get_relationships()\n        click.echo(f\"Relationships: {rels}\")\n\n    # Print relationships for keyword\n    if relationships_for_keyword:\n        rels = corpus.get_all_relationships_for_keyword(relationships_for_keyword)\n        click.echo(f\"Relationships for keyword '{relationships_for_keyword}': {rels}\")\n\n    # Semantic search\n    if semantic:\n        try:\n            from .semantic import Semantic\n\n            click.echo(f\"\\nPerforming semantic search for: '{semantic}'\")\n            # Try with default embeddings first, fall back to simple embeddings\n            try:\n                semantic_analyzer = Semantic(corpus)\n            except Exception as network_error:\n                # If network error or download fails, try simple embeddings\n                if \"address\" in str(network_error).lower() or \"download\" in str(network_error).lower():\n                    click.echo(\"Note: Using simple embeddings (network unavailable)\")\n                    semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                else:\n                    raise\n            corpus = semantic_analyzer.get_similar(semantic, n_results=num)\n            click.echo(f\"\u2713 Found {len(corpus.documents)} similar documents\")\n            click.echo(\n                f\"Hint: Use --out to save the filtered corpus, or --print to view results\"\n            )\n        except ImportError as e:\n            click.echo(f\"Error: {e}\")\n            click.echo(\"Install chromadb with: pip install chromadb\")\n        except Exception as e:\n            click.echo(f\"Error during semantic search: {e}\")\n\n    # Find similar documents\n    if similar_docs:\n        try:\n            from .semantic import Semantic\n\n            click.echo(f\"\\nFinding documents similar to: '{similar_docs}'\")\n            click.echo(f\"Number of results: {num}\")\n            # Convert rec to 0-1 range if needed (for similar_docs, threshold is 0-1)\n            threshold = rec / 10.0 if rec &gt; 1.0 else rec\n            click.echo(f\"Similarity threshold: {threshold}\")\n\n            # Try with default embeddings first, fall back to simple embeddings\n            try:\n                semantic_analyzer = Semantic(corpus)\n            except Exception as network_error:\n                # If network error or download fails, try simple embeddings\n                if \"address\" in str(network_error).lower() or \"download\" in str(network_error).lower():\n                    click.echo(\"Note: Using simple embeddings (network unavailable)\")\n                    semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                else:\n                    raise\n\n            # Get similar document IDs\n            similar_doc_ids = semantic_analyzer.get_similar_documents(\n                document_ids=similar_docs,\n                n_results=num,\n                threshold=threshold\n            )\n\n            click.echo(f\"\u2713 Found {len(similar_doc_ids)} similar documents\")\n            if similar_doc_ids:\n                click.echo(\"\\nSimilar Document IDs:\")\n                for doc_id in similar_doc_ids:\n                    doc = corpus.get_document_by_id(doc_id)\n                    doc_name = f\" ({doc.name})\" if doc and doc.name else \"\"\n                    click.echo(f\"  - {doc_id}{doc_name}\")\n                click.echo(\"\\nHint: Use --doc-id to view individual documents\")\n                click.echo(\"Hint: This feature is useful for literature reviews to find similar documents\")\n            else:\n                click.echo(\"No similar documents found above the threshold.\")\n                click.echo(\"Hint: Try lowering the threshold with --rec\")\n\n        except ImportError as e:\n            click.echo(f\"Error: {e}\")\n            click.echo(\"Install chromadb with: pip install chromadb\")\n        except Exception as e:\n            click.echo(f\"Error finding similar documents: {e}\")\n\n\n    # Semantic chunk search\n    if semantic_chunks:\n        if not doc_id:\n            click.echo(\"Error: --doc-id is required when using --semantic-chunks\")\n        else:\n            try:\n                from .semantic import Semantic\n\n                click.echo(f\"\\nPerforming semantic chunk search for: '{semantic_chunks}'\")\n                click.echo(f\"Document ID: {doc_id}\")\n                click.echo(f\"Threshold: {rec}\")\n\n                # Try with default embeddings first, fall back to simple embeddings\n                try:\n                    semantic_analyzer = Semantic(corpus)\n                except Exception as network_error:\n                    # If network error or download fails, try simple embeddings\n                    if \"address\" in str(network_error).lower() or \"download\" in str(network_error).lower():\n                        click.echo(\"Note: Using simple embeddings (network unavailable)\")\n                        semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                    else:\n                        raise\n\n                # Get similar chunks\n                chunks = semantic_analyzer.get_similar_chunks(\n                    query=semantic_chunks,\n                    doc_id=doc_id,\n                    threshold=rec,\n                    n_results=20  # Get more chunks to filter by threshold\n                )\n\n                click.echo(f\"\u2713 Found {len(chunks)} matching chunks\")\n                click.echo(\"\\nMatching chunks:\")\n                click.echo(\"=\" * 60)\n                for i, chunk in enumerate(chunks, 1):\n                    click.echo(f\"\\nChunk {i}:\")\n                    click.echo(chunk)\n                    click.echo(\"-\" * 60)\n\n                if len(chunks) == 0:\n                    click.echo(\"No chunks matched the query above the threshold.\")\n                    click.echo(\"Hint: Try lowering the threshold with --rec or use a different query.\")\n                else:\n                    click.echo(f\"\\nHint: These {len(chunks)} chunks can be used for coding/annotating the document.\")\n                    click.echo(\"Hint: Adjust --rec threshold to get more or fewer results.\")\n\n            except ImportError as e:\n                click.echo(f\"Error: {e}\")\n                click.echo(\"Install chromadb with: pip install chromadb\")\n            except Exception as e:\n                click.echo(f\"Error during semantic chunk search: {e}\")\n\n    # Export metadata as DataFrame\n    if metadata_df:\n        try:\n            from .semantic import Semantic\n\n            click.echo(\"\\nExporting metadata as DataFrame...\")\n            # Try with default embeddings first, fall back to simple embeddings\n            try:\n                semantic_analyzer = Semantic(corpus)\n            except Exception as network_error:\n                # If network error or download fails, try simple embeddings\n                if \"address\" in str(network_error).lower() or \"download\" in str(network_error).lower():\n                    click.echo(\"Note: Using simple embeddings (network unavailable)\")\n                    semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                else:\n                    raise\n            # Parse metadata_keys if provided\n            keys_list = None\n            if metadata_keys:\n                keys_list = [k.strip() for k in metadata_keys.split(\",\")]\n            corpus = semantic_analyzer.get_df(metadata_keys=keys_list)\n            click.echo(\"\u2713 Metadata exported to DataFrame\")\n            if corpus.df is not None:\n                click.echo(f\"DataFrame shape: {corpus.df.shape}\")\n                click.echo(f\"Columns: {list(corpus.df.columns)}\")\n            click.echo(\"Hint: Use --out to save the corpus with the updated DataFrame\")\n        except ImportError as e:\n            click.echo(f\"Error: {e}\")\n            click.echo(\"Install chromadb with: pip install chromadb\")\n        except Exception as e:\n            click.echo(f\"Error exporting metadata: {e}\")\n\n    # TDABM analysis\n    if tdabm:\n        try:\n            # Parse tdabm parameter: y_variable:x_variables:radius\n            parts = tdabm.split(\":\")\n            if len(parts) &lt; 2:\n                raise click.ClickException(\n                    \"Invalid --tdabm format. Use 'y_variable:x_variables:radius' \"\n                    \"(e.g., 'satisfaction:age,income:0.3'). Radius defaults to 0.3 if omitted.\"\n                )\n\n            y_var = parts[0].strip()\n            x_vars = parts[1].strip()\n            radius = 0.3  # default\n\n            if len(parts) &gt;= 3:\n                try:\n                    radius = float(parts[2].strip())\n                except ValueError:\n                    raise click.ClickException(f\"Invalid radius value: '{parts[2]}'. Must be a number.\")\n\n            click.echo(f\"\\nPerforming TDABM analysis...\")\n            click.echo(f\"  Y variable: {y_var}\")\n            click.echo(f\"  X variables: {x_vars}\")\n            click.echo(f\"  Radius: {radius}\")\n\n            tdabm_analyzer = Tdabm(corpus)\n            result = tdabm_analyzer.generate_tdabm(y=y_var, x_variables=x_vars, radius=radius)\n\n            click.echo(\"\\n\" + result)\n            click.echo(\"\\nHint: TDABM results stored in corpus metadata['tdabm']\")\n            click.echo(\"Hint: Use --out to save the corpus with TDABM metadata\")\n            click.echo(\"Hint: Use 'crispviz --tdabm' to visualize the results\")\n\n        except ValueError as e:\n            click.echo(f\"Error: {e}\")\n            click.echo(\"Hint: Ensure your corpus has a DataFrame with the specified variables\")\n            click.echo(\"Hint: Y variable must be continuous (not binary)\")\n            click.echo(\"Hint: X variables must be numeric/ordinal\")\n        except Exception as e:\n            click.echo(f\"Error during TDABM analysis: {e}\")\n\n    # Save corpus to --out if provided\n    if out:\n        from .read_data import ReadData\n\n        rd = ReadData(corpus=corpus)\n        rd.write_corpus_to_json(out, corpus=corpus)\n        click.echo(f\"\u2713 Corpus saved to {out}\")\n\n    if print_corpus:\n        click.echo(\"\\n=== Corpus Details ===\")\n        corpus.pretty_print()\n\n    logger.info(\"Corpus CLI finished\")\n</code></pre>"},{"location":"modules/#vizcli.main","title":"<code>main(verbose, inp, out, bins, topics_num, top_n, corr_columns, freq, by_topic, wordcloud, ldavis, top_terms, corr_heatmap, tdabm)</code>","text":"<p>CRISP-T: Visualization CLI</p> <p>Build corpus (source preferred over inp), optionally handle multiple sources, and export selected visualizations as PNG files into the output directory.</p> Source code in <code>src/crisp_t/vizcli.py</code> <pre><code>@click.command()\n@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Print verbose messages.\")\n@click.option(\"--inp\", \"-i\", help=\"Load corpus from a folder containing corpus.json\")\n@click.option(\n    \"--out\",\n    \"-o\",\n    help=\"Output directory where PNG images will be written\",\n)\n@click.option(\n    \"--bins\", default=100, show_default=True, help=\"Number of bins for distributions\"\n)\n@click.option(\n    \"--topics-num\",\n    default=8,\n    show_default=True,\n    help=\"Number of topics for LDA when required (default 8 as per Mettler et al. 2025)\",\n)\n@click.option(\n    \"--top-n\",\n    default=20,\n    show_default=True,\n    help=\"Top N terms to show in top-terms chart\",\n)\n@click.option(\n    \"--corr-columns\",\n    default=\"\",\n    help=\"Comma separated numeric columns for correlation heatmap; if empty, auto-select\",\n)\n@click.option(\"--freq\", is_flag=True, help=\"Export: word frequency distribution\")\n@click.option(\n    \"--by-topic\",\n    is_flag=True,\n    help=\"Export: distribution by dominant topic (requires LDA)\",\n)\n@click.option(\n    \"--wordcloud\", is_flag=True, help=\"Export: topic wordcloud (requires LDA)\"\n)\n@click.option(\n    \"--ldavis\", is_flag=True, help=\"Export: interactive LDA visualization HTML (requires LDA)\"\n)\n@click.option(\n    \"--top-terms\", is_flag=True, help=\"Export: top terms bar chart (computed from text)\"\n)\n@click.option(\n    \"--corr-heatmap\",\n    is_flag=True,\n    help=\"Export: correlation heatmap (from CSV numeric columns)\",\n)\n@click.option(\n    \"--tdabm\",\n    is_flag=True,\n    help=\"Export: TDABM visualization (requires TDABM analysis in corpus metadata)\",\n)\ndef main(\n    verbose: bool,\n    inp: Optional[str],\n    out: str,\n    bins: int,\n    topics_num: int,\n    top_n: int,\n    corr_columns: str,\n    freq: bool,\n    by_topic: bool,\n    wordcloud: bool,\n    ldavis: bool,\n    top_terms: bool,\n    corr_heatmap: bool,\n    tdabm: bool,\n):\n    \"\"\"CRISP-T: Visualization CLI\n\n    Build corpus (source preferred over inp), optionally handle multiple sources,\n    and export selected visualizations as PNG files into the output directory.\n    \"\"\"\n\n    if verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n        click.echo(\"Verbose mode enabled\")\n\n    click.echo(\"_________________________________________\")\n    click.echo(\"CRISP-T: Visualizations\")\n    click.echo(f\"Version: {__version__}\")\n    click.echo(\"_________________________________________\")\n\n    out_dir = Path(out)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # Initialize components\n    read_data = ReadData()\n    corpus = None\n\n    corpus = initialize_corpus(inp=inp)\n\n    if not corpus:\n        raise click.ClickException(\"No input provided. Use --source/--sources or --inp\")\n\n    viz = QRVisualize(corpus=corpus)\n\n    # Helper: build LDA if by-topic or wordcloud requested\n    cluster_instance = None\n    def ensure_topics():\n        nonlocal cluster_instance\n        if cluster_instance is None:\n            cluster_instance = Cluster(corpus=corpus)\n            cluster_instance.build_lda_model(topics=topics_num)\n            # Populate visualization structures used by QRVisualize\n            cluster_instance.format_topics_sentences(visualize=True)\n        return cluster_instance\n\n    # 1) Word frequency distribution\n    if freq:\n        df_text = pd.DataFrame(\n            {\"Text\": [getattr(doc, \"text\", \"\") or \"\" for doc in corpus.documents]}\n        )\n        out_path = out_dir / \"word_frequency.png\"\n        viz.plot_frequency_distribution_of_words(\n            df=df_text, folder_path=str(out_path), bins=bins, show=False\n        )\n        click.echo(f\"Saved: {out_path}\")\n\n    # 2) Distribution by topic (requires topics)\n    if by_topic:\n        ensure_topics()\n        out_path = out_dir / \"by_topic.png\"\n        viz.plot_distribution_by_topic(\n            df=None, folder_path=str(out_path), bins=bins, show=False\n        )\n        click.echo(f\"Saved: {out_path}\")\n\n    # 3) Topic wordcloud (requires topics)\n    if wordcloud:\n        ensure_topics()\n        out_path = out_dir / \"wordcloud.png\"\n        viz.plot_wordcloud(topics=None, folder_path=str(out_path), show=False)\n        click.echo(f\"Saved: {out_path}\")\n\n    # 3.5) LDA visualization (requires topics)\n    if ldavis:\n        cluster = ensure_topics()\n        out_path = out_dir / \"lda_visualization.html\"\n        try:\n            viz.get_lda_viz(\n                lda_model=cluster._lda_model,\n                corpus_bow=cluster._bag_of_words,\n                dictionary=cluster._dictionary,\n                folder_path=str(out_path),\n                show=False\n            )\n            click.echo(f\"Saved: {out_path}\")\n        except ImportError as e:\n            click.echo(f\"Warning: {e}\")\n        except Exception as e:\n            click.echo(f\"Error generating LDA visualization: {e}\")\n\n    # 4) Top terms (compute from text directly)\n    if top_terms:\n        texts = [getattr(doc, \"text\", \"\") or \"\" for doc in corpus.documents]\n        tokens = []\n        for t in texts:\n            tokens.extend((t or \"\").lower().split())\n        freq_map = Counter(tokens)\n        if not freq_map:\n            click.echo(\"No tokens found to plot top terms.\")\n        else:\n            df_terms = pd.DataFrame(\n                {\n                    \"term\": list(freq_map.keys()),\n                    \"frequency\": list(freq_map.values()),\n                }\n            )\n            # QRVisualize sorts internally; we just pass full DF\n            out_path = out_dir / \"top_terms.png\"\n            viz.plot_top_terms(\n                df=df_terms, top_n=top_n, folder_path=str(out_path), show=False\n            )\n            click.echo(f\"Saved: {out_path}\")\n\n    # 5) Correlation heatmap\n    if corr_heatmap:\n        if getattr(corpus, \"df\", None) is None or corpus.df.empty:\n            click.echo(\"No CSV data available for correlation heatmap; skipping.\")\n        else:\n            df0 = corpus.df.copy()\n            # If user specified columns, attempt to use them; else let visualize auto-select\n            cols = (\n                [c.strip() for c in corr_columns.split(\",\") if c.strip()]\n                if corr_columns\n                else None\n            )\n            out_path = out_dir / \"corr_heatmap.png\"\n            if cols:\n                # Pass subset to avoid rename ambiguity\n                sub = (\n                    df0[cols].copy().select_dtypes(include=[\"number\"])\n                )  # ensure numeric\n                viz.plot_correlation_heatmap(\n                    df=sub, columns=None, folder_path=str(out_path), show=False\n                )\n            else:\n                viz.plot_correlation_heatmap(\n                    df=df0, columns=None, folder_path=str(out_path), show=False\n                )\n            click.echo(f\"Saved: {out_path}\")\n\n    # TDABM visualization\n    if tdabm:\n        if 'tdabm' not in corpus.metadata:\n            click.echo(\"Warning: No TDABM data found in corpus metadata.\")\n            click.echo(\"Hint: Run TDABM analysis first with: crispt --tdabm y_var:x_vars:radius --inp &lt;corpus_dir&gt;\")\n        else:\n            out_path = out_dir / \"tdabm.png\"\n            try:\n                viz.draw_tdabm(corpus=corpus, folder_path=str(out_path), show=False)\n                click.echo(f\"Saved: {out_path}\")\n            except Exception as e:\n                click.echo(f\"Error generating TDABM visualization: {e}\")\n                logger.error(f\"TDABM visualization error: {e}\", exc_info=True)\n\n    click.echo(\"\\n=== Visualization Complete ===\")\n</code></pre>"},{"location":"modules/#model.corpus.Corpus","title":"<code>Corpus</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corpus model for storing a collection of documents.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>class Corpus(BaseModel):\n    \"\"\"\n    Corpus model for storing a collection of documents.\n    \"\"\"\n\n    id: str = Field(..., description=\"Unique identifier for the corpus.\")\n    name: Optional[str] = Field(None, description=\"Name of the corpus.\")\n    description: Optional[str] = Field(None, description=\"Description of the corpus.\")\n    score: Optional[float] = Field(\n        None, description=\"Score associated with the corpus.\"\n    )\n    documents: list[Document] = Field(\n        default_factory=list, description=\"List of documents in the corpus.\"\n    )\n    df: Optional[pd.DataFrame] = Field(\n        None, description=\"Numeric data associated with the corpus.\"\n    )\n    visualization: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Visualization data associated with the corpus.\"\n    )\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True\n    )  # required for pandas DataFrame\n    metadata: dict = Field(\n        default_factory=dict, description=\"Metadata associated with the corpus.\"\n    )\n\n    def pretty_print(self, show=\"all\"):\n        \"\"\"\n        Print the corpus information in a human-readable format.\n\n        Args:\n            show: Display option. Can be:\n                - \"all\": Show all corpus information\n                - \"documents\": Show first 5 documents\n                - \"documents N\": Show first N documents (e.g., \"documents 10\")\n                - \"documents metadata\": Show document-specific metadata\n                - \"dataframe\": Show DataFrame head\n                - \"dataframe metadata\": Show DataFrame metadata columns (metadata_*)\n                - \"dataframe stats\": Show DataFrame statistics\n                - \"metadata\": Show all corpus metadata\n                - \"metadata KEY\": Show specific metadata key (e.g., \"metadata pca\")\n                - \"stats\": Show DataFrame statistics (deprecated, use \"dataframe stats\")\n        \"\"\"\n        # Color codes for terminal output\n        BLUE = \"\\033[94m\"\n        GREEN = \"\\033[92m\"\n        YELLOW = \"\\033[93m\"\n        CYAN = \"\\033[96m\"\n        MAGENTA = \"\\033[95m\"\n        RED = \"\\033[91m\"\n        RESET = \"\\033[0m\"\n        BOLD = \"\\033[1m\"\n\n        # Parse the show parameter to support subcommands\n        parts = show.split(maxsplit=1)\n        main_command = parts[0]\n        sub_command = parts[1] if len(parts) &gt; 1 else None\n\n        # Print basic corpus info for most commands\n        if main_command in [\"all\", \"documents\", \"dataframe\", \"metadata\"]:\n            print(f\"{BOLD}{BLUE}Corpus ID:{RESET} {self.id}\")\n            print(f\"{BOLD}{BLUE}Name:{RESET} {self.name}\")\n            print(f\"{BOLD}{BLUE}Description:{RESET} {self.description}\")\n\n        # Handle documents command\n        if main_command in [\"all\", \"documents\"]:\n            if sub_command == \"metadata\":\n                # Show document-specific metadata\n                print(f\"\\n{BOLD}{GREEN}=== Document Metadata ==={RESET}\")\n                if not self.documents:\n                    print(\"No documents in corpus\")\n                else:\n                    for i, doc in enumerate(self.documents, 1):\n                        print(f\"\\n{CYAN}Document {i}:{RESET}\")\n                        print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                        print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                        if doc.metadata:\n                            print(f\"  {BOLD}Metadata:{RESET}\")\n                            for key, value in doc.metadata.items():\n                                # Truncate long values\n                                val_str = str(value)\n                                if len(val_str) &gt; 100:\n                                    val_str = val_str[:97] + \"...\"\n                                print(f\"    {YELLOW}{key}:{RESET} {val_str}\")\n                        else:\n                            print(f\"  {BOLD}Metadata:{RESET} (none)\")\n            else:\n                # Determine how many documents to show\n                num_docs = 5  # default\n                if sub_command:\n                    try:\n                        num_docs = int(sub_command)\n                    except ValueError:\n                        print(f\"{RED}Invalid number for documents: {sub_command}. Using default (5).{RESET}\")\n\n                print(f\"\\n{BOLD}{GREEN}=== Documents ==={RESET}\")\n                print(f\"Total documents: {len(self.documents)}\")\n                print(f\"Showing first {min(num_docs, len(self.documents))} document(s):\\n\")\n\n                for i, doc in enumerate(self.documents[:num_docs], 1):\n                    print(f\"{CYAN}Document {i}:{RESET}\")\n                    print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                    print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                    # Show a snippet of text if available\n                    if hasattr(doc, 'text') and doc.text:\n                        text_snippet = doc.text[:200] + \"...\" if len(doc.text) &gt; 200 else doc.text\n                        print(f\"  {BOLD}Text:{RESET} {text_snippet}\")\n                    print()\n\n        # Handle dataframe command\n        if main_command in [\"all\", \"dataframe\"]:\n            if self.df is not None:\n                if sub_command == \"metadata\":\n                    # Show DataFrame metadata columns (columns starting with metadata_)\n                    print(f\"\\n{BOLD}{GREEN}=== DataFrame Metadata Columns ==={RESET}\")\n                    metadata_cols = [col for col in self.df.columns if col.startswith(\"metadata_\")]\n                    if metadata_cols:\n                        print(f\"Found {len(metadata_cols)} metadata column(s):\")\n                        for col in metadata_cols:\n                            print(f\"  {YELLOW}{col}{RESET}\")\n                            # Show some statistics for the metadata column\n                            print(f\"    Non-null values: {self.df[col].notna().sum()}\")\n                            print(f\"    Null values: {self.df[col].isna().sum()}\")\n                            # Show unique values if not too many\n                            unique_count = self.df[col].nunique()\n                            if unique_count &lt;= 10:\n                                print(f\"    Unique values ({unique_count}): {list(self.df[col].unique())}\")\n                            else:\n                                print(f\"    Unique values: {unique_count}\")\n                    else:\n                        print(\"No metadata columns found (columns starting with 'metadata_')\")\n                elif sub_command == \"stats\":\n                    # Show DataFrame statistics\n                    self._print_dataframe_stats()\n                else:\n                    # Show DataFrame head\n                    print(f\"\\n{BOLD}{GREEN}=== DataFrame ==={RESET}\")\n                    print(f\"Shape: {self.df.shape}\")\n                    print(f\"Columns: {list(self.df.columns)}\")\n                    print(\"\\nFirst few rows:\")\n                    print(self.df.head())\n            else:\n                if main_command == \"dataframe\":\n                    print(f\"\\n{BOLD}{RED}No DataFrame available{RESET}\")\n\n        # Handle metadata command\n        if main_command in [\"all\", \"metadata\"]:\n            if sub_command:\n                # Show specific metadata key\n                print(f\"\\n{BOLD}{GREEN}=== Metadata: {sub_command} ==={RESET}\")\n                if sub_command in self.metadata:\n                    value = self.metadata[sub_command]\n                    # Format the output based on the type of value\n                    if isinstance(value, dict):\n                        for k, v in value.items():\n                            print(f\"{YELLOW}{k}:{RESET} {v}\")\n                    elif isinstance(value, list):\n                        for i, item in enumerate(value, 1):\n                            print(f\"{i}. {item}\")\n                    else:\n                        print(value)\n                else:\n                    print(f\"{RED}Metadata key '{sub_command}' not found{RESET}\")\n                    available_keys = list(self.metadata.keys())\n                    if available_keys:\n                        print(f\"Available keys: {', '.join(available_keys)}\")\n            else:\n                # Show all metadata\n                print(f\"\\n{BOLD}{GREEN}=== Corpus Metadata ==={RESET}\")\n                if not self.metadata:\n                    print(\"No metadata available\")\n                else:\n                    for key, value in self.metadata.items():\n                        print(f\"\\n{MAGENTA}{key}:{RESET}\")\n                        # Truncate long values\n                        val_str = str(value)\n                        if len(val_str) &gt; 500:\n                            val_str = val_str[:497] + \"...\"\n                        print(f\"  {val_str}\")\n\n        # Handle stats command (deprecated, redirect to dataframe stats)\n        if main_command == \"stats\":\n            print(f\"{YELLOW}Note: 'stats' is deprecated. Use 'dataframe stats' instead.{RESET}\")\n            if self.df is not None:\n                self._print_dataframe_stats()\n            else:\n                print(f\"{RED}No DataFrame available{RESET}\")\n\n        print(f\"\\n{BOLD}Display completed for '{show}'{RESET}\")\n\n    def _print_dataframe_stats(self):\n        \"\"\"Helper method to print DataFrame statistics.\"\"\"\n        YELLOW = \"\\033[93m\"\n        BOLD = \"\\033[1m\"\n        RESET = \"\\033[0m\"\n        GREEN = \"\\033[92m\"\n\n        print(f\"\\n{BOLD}{GREEN}=== DataFrame Statistics ==={RESET}\")\n        print(self.df.describe())\n        print(f\"\\n{BOLD}Distinct values per column:{RESET}\")\n        for col in self.df.columns:\n            nunique = self.df[col].nunique()\n            print(f\"  {YELLOW}{col}:{RESET} {nunique} distinct value(s)\")\n            # If distinct values &lt; 10, show value counts\n            if nunique &lt;= 10:\n                print(f\"    Value counts:\")\n                for val, count in self.df[col].value_counts().items():\n                    print(f\"      {val}: {count}\")\n                print()\n    def get_all_df_column_names(self):\n        \"\"\"\n        Get a list of all column names in the DataFrame.\n\n        Returns:\n            List of column names.\n        \"\"\"\n        if self.df is not None:\n            return self.df.columns.tolist()\n        return []\n\n    def get_descriptive_statistics(self):\n        \"\"\"\n        Get descriptive statistics of the DataFrame.\n\n        Returns:\n            DataFrame containing descriptive statistics, or None if DataFrame is None.\n        \"\"\"\n        if self.df is not None:\n            return self.df.describe()\n        return None\n\n    def get_row_count(self):\n        \"\"\"\n        Get the number of rows in the DataFrame.\n\n        Returns:\n            Number of rows in the DataFrame, or 0 if DataFrame is None.\n        \"\"\"\n        if self.df is not None:\n            return len(self.df)\n        return 0\n\n    def get_row_by_index(self, index: int) -&gt; Optional[pd.Series]:\n        \"\"\"\n        Get a row from the DataFrame by its index.\n\n        Args:\n            index: Index of the row to retrieve.\n        Returns:\n            Row as a pandas Series if index is valid, else None.\n        \"\"\"\n        if self.df is not None and 0 &lt;= index &lt; len(self.df):\n            return self.df.iloc[index]\n        return None\n\n    def get_all_document_ids(self):\n        \"\"\"\n        Get a list of all document IDs in the corpus.\n\n        Returns:\n            List of document IDs.\n        \"\"\"\n        return [doc.id for doc in self.documents]\n\n    def get_document_by_id(self, document_id: str) -&gt; Optional[Document]:\n        \"\"\"\n        Get a document by its ID.\n\n        Args:\n            document_id: ID of the document to retrieve.\n\n        Returns:\n            Document object if found, else None.\n        \"\"\"\n        for doc in self.documents:\n            if doc.id == document_id:\n                return doc\n        return None\n\n    def add_document(self, document: Document):\n        \"\"\"\n        Add a document to the corpus.\n\n        Args:\n            document: Document object to add.\n        \"\"\"\n        self.documents.append(document)\n\n    def remove_document_by_id(self, document_id: str):\n        \"\"\"\n        Remove a document from the corpus by its ID.\n\n        Args:\n            document_id: ID of the document to remove.\n        \"\"\"\n        self.documents = [\n            doc for doc in self.documents if doc.id != document_id\n        ]\n\n    def update_metadata(self, key: str, value: Any):\n        \"\"\"\n        Update the metadata of the corpus.\n\n        Args:\n            key: Metadata key to update.\n            value: New value for the metadata key.\n        \"\"\"\n        self.metadata[key] = value\n\n    def add_relationship(self, first: str, second: str, relation: str):\n        \"\"\"\n        Add a relationship between two documents in the corpus.\n\n        Args:\n            first: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n            second: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n            relation: Description of the relationship. (One of \"correlates\", \"similar to\", \"cites\", \"references\", \"contradicts\", etc.)\n        \"\"\"\n        if \"relationships\" not in self.metadata:\n            self.metadata[\"relationships\"] = []\n        self.metadata[\"relationships\"].append(\n            {\"first\": first, \"second\": second, \"relation\": relation}\n        )\n\n    def clear_relationships(self):\n        \"\"\"\n        Clear all relationships in the corpus metadata.\n        \"\"\"\n        if \"relationships\" in self.metadata:\n            self.metadata[\"relationships\"] = []\n\n    def get_relationships(self):\n        \"\"\"\n        Get all relationships in the corpus metadata.\n\n        Returns:\n            List of relationships, or empty list if none exist.\n        \"\"\"\n        return self.metadata.get(\"relationships\", [])\n\n    def get_all_relationships_for_keyword(self, keyword: str):\n        \"\"\"\n        Get all relationships involving a specific keyword.\n\n        Args:\n            keyword: Keyword to search for in relationships.\n\n        Returns:\n            List of relationships involving the keyword.\n        \"\"\"\n        rels = self.get_relationships()\n        return [\n            rel\n            for rel in rels\n            if keyword in rel[\"first\"] or keyword in rel[\"second\"]\n        ]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.add_document","title":"<code>add_document(document)</code>","text":"<p>Add a document to the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document object to add.</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def add_document(self, document: Document):\n    \"\"\"\n    Add a document to the corpus.\n\n    Args:\n        document: Document object to add.\n    \"\"\"\n    self.documents.append(document)\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.add_relationship","title":"<code>add_relationship(first, second, relation)</code>","text":"<p>Add a relationship between two documents in the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>str</code> <p>keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column</p> required <code>second</code> <code>str</code> <p>keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column</p> required <code>relation</code> <code>str</code> <p>Description of the relationship. (One of \"correlates\", \"similar to\", \"cites\", \"references\", \"contradicts\", etc.)</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def add_relationship(self, first: str, second: str, relation: str):\n    \"\"\"\n    Add a relationship between two documents in the corpus.\n\n    Args:\n        first: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n        second: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n        relation: Description of the relationship. (One of \"correlates\", \"similar to\", \"cites\", \"references\", \"contradicts\", etc.)\n    \"\"\"\n    if \"relationships\" not in self.metadata:\n        self.metadata[\"relationships\"] = []\n    self.metadata[\"relationships\"].append(\n        {\"first\": first, \"second\": second, \"relation\": relation}\n    )\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.clear_relationships","title":"<code>clear_relationships()</code>","text":"<p>Clear all relationships in the corpus metadata.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def clear_relationships(self):\n    \"\"\"\n    Clear all relationships in the corpus metadata.\n    \"\"\"\n    if \"relationships\" in self.metadata:\n        self.metadata[\"relationships\"] = []\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_all_df_column_names","title":"<code>get_all_df_column_names()</code>","text":"<p>Get a list of all column names in the DataFrame.</p> <p>Returns:</p> Type Description <p>List of column names.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_all_df_column_names(self):\n    \"\"\"\n    Get a list of all column names in the DataFrame.\n\n    Returns:\n        List of column names.\n    \"\"\"\n    if self.df is not None:\n        return self.df.columns.tolist()\n    return []\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_all_document_ids","title":"<code>get_all_document_ids()</code>","text":"<p>Get a list of all document IDs in the corpus.</p> <p>Returns:</p> Type Description <p>List of document IDs.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_all_document_ids(self):\n    \"\"\"\n    Get a list of all document IDs in the corpus.\n\n    Returns:\n        List of document IDs.\n    \"\"\"\n    return [doc.id for doc in self.documents]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_all_relationships_for_keyword","title":"<code>get_all_relationships_for_keyword(keyword)</code>","text":"<p>Get all relationships involving a specific keyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>Keyword to search for in relationships.</p> required <p>Returns:</p> Type Description <p>List of relationships involving the keyword.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_all_relationships_for_keyword(self, keyword: str):\n    \"\"\"\n    Get all relationships involving a specific keyword.\n\n    Args:\n        keyword: Keyword to search for in relationships.\n\n    Returns:\n        List of relationships involving the keyword.\n    \"\"\"\n    rels = self.get_relationships()\n    return [\n        rel\n        for rel in rels\n        if keyword in rel[\"first\"] or keyword in rel[\"second\"]\n    ]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_descriptive_statistics","title":"<code>get_descriptive_statistics()</code>","text":"<p>Get descriptive statistics of the DataFrame.</p> <p>Returns:</p> Type Description <p>DataFrame containing descriptive statistics, or None if DataFrame is None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_descriptive_statistics(self):\n    \"\"\"\n    Get descriptive statistics of the DataFrame.\n\n    Returns:\n        DataFrame containing descriptive statistics, or None if DataFrame is None.\n    \"\"\"\n    if self.df is not None:\n        return self.df.describe()\n    return None\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_document_by_id","title":"<code>get_document_by_id(document_id)</code>","text":"<p>Get a document by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>str</code> <p>ID of the document to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object if found, else None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_document_by_id(self, document_id: str) -&gt; Optional[Document]:\n    \"\"\"\n    Get a document by its ID.\n\n    Args:\n        document_id: ID of the document to retrieve.\n\n    Returns:\n        Document object if found, else None.\n    \"\"\"\n    for doc in self.documents:\n        if doc.id == document_id:\n            return doc\n    return None\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_relationships","title":"<code>get_relationships()</code>","text":"<p>Get all relationships in the corpus metadata.</p> <p>Returns:</p> Type Description <p>List of relationships, or empty list if none exist.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_relationships(self):\n    \"\"\"\n    Get all relationships in the corpus metadata.\n\n    Returns:\n        List of relationships, or empty list if none exist.\n    \"\"\"\n    return self.metadata.get(\"relationships\", [])\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_row_by_index","title":"<code>get_row_by_index(index)</code>","text":"<p>Get a row from the DataFrame by its index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the row to retrieve.</p> required <p>Returns:     Row as a pandas Series if index is valid, else None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_row_by_index(self, index: int) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Get a row from the DataFrame by its index.\n\n    Args:\n        index: Index of the row to retrieve.\n    Returns:\n        Row as a pandas Series if index is valid, else None.\n    \"\"\"\n    if self.df is not None and 0 &lt;= index &lt; len(self.df):\n        return self.df.iloc[index]\n    return None\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_row_count","title":"<code>get_row_count()</code>","text":"<p>Get the number of rows in the DataFrame.</p> <p>Returns:</p> Type Description <p>Number of rows in the DataFrame, or 0 if DataFrame is None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_row_count(self):\n    \"\"\"\n    Get the number of rows in the DataFrame.\n\n    Returns:\n        Number of rows in the DataFrame, or 0 if DataFrame is None.\n    \"\"\"\n    if self.df is not None:\n        return len(self.df)\n    return 0\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.pretty_print","title":"<code>pretty_print(show='all')</code>","text":"<p>Print the corpus information in a human-readable format.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <p>Display option. Can be: - \"all\": Show all corpus information - \"documents\": Show first 5 documents - \"documents N\": Show first N documents (e.g., \"documents 10\") - \"documents metadata\": Show document-specific metadata - \"dataframe\": Show DataFrame head - \"dataframe metadata\": Show DataFrame metadata columns (metadata_*) - \"dataframe stats\": Show DataFrame statistics - \"metadata\": Show all corpus metadata - \"metadata KEY\": Show specific metadata key (e.g., \"metadata pca\") - \"stats\": Show DataFrame statistics (deprecated, use \"dataframe stats\")</p> <code>'all'</code> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def pretty_print(self, show=\"all\"):\n    \"\"\"\n    Print the corpus information in a human-readable format.\n\n    Args:\n        show: Display option. Can be:\n            - \"all\": Show all corpus information\n            - \"documents\": Show first 5 documents\n            - \"documents N\": Show first N documents (e.g., \"documents 10\")\n            - \"documents metadata\": Show document-specific metadata\n            - \"dataframe\": Show DataFrame head\n            - \"dataframe metadata\": Show DataFrame metadata columns (metadata_*)\n            - \"dataframe stats\": Show DataFrame statistics\n            - \"metadata\": Show all corpus metadata\n            - \"metadata KEY\": Show specific metadata key (e.g., \"metadata pca\")\n            - \"stats\": Show DataFrame statistics (deprecated, use \"dataframe stats\")\n    \"\"\"\n    # Color codes for terminal output\n    BLUE = \"\\033[94m\"\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    CYAN = \"\\033[96m\"\n    MAGENTA = \"\\033[95m\"\n    RED = \"\\033[91m\"\n    RESET = \"\\033[0m\"\n    BOLD = \"\\033[1m\"\n\n    # Parse the show parameter to support subcommands\n    parts = show.split(maxsplit=1)\n    main_command = parts[0]\n    sub_command = parts[1] if len(parts) &gt; 1 else None\n\n    # Print basic corpus info for most commands\n    if main_command in [\"all\", \"documents\", \"dataframe\", \"metadata\"]:\n        print(f\"{BOLD}{BLUE}Corpus ID:{RESET} {self.id}\")\n        print(f\"{BOLD}{BLUE}Name:{RESET} {self.name}\")\n        print(f\"{BOLD}{BLUE}Description:{RESET} {self.description}\")\n\n    # Handle documents command\n    if main_command in [\"all\", \"documents\"]:\n        if sub_command == \"metadata\":\n            # Show document-specific metadata\n            print(f\"\\n{BOLD}{GREEN}=== Document Metadata ==={RESET}\")\n            if not self.documents:\n                print(\"No documents in corpus\")\n            else:\n                for i, doc in enumerate(self.documents, 1):\n                    print(f\"\\n{CYAN}Document {i}:{RESET}\")\n                    print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                    print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                    if doc.metadata:\n                        print(f\"  {BOLD}Metadata:{RESET}\")\n                        for key, value in doc.metadata.items():\n                            # Truncate long values\n                            val_str = str(value)\n                            if len(val_str) &gt; 100:\n                                val_str = val_str[:97] + \"...\"\n                            print(f\"    {YELLOW}{key}:{RESET} {val_str}\")\n                    else:\n                        print(f\"  {BOLD}Metadata:{RESET} (none)\")\n        else:\n            # Determine how many documents to show\n            num_docs = 5  # default\n            if sub_command:\n                try:\n                    num_docs = int(sub_command)\n                except ValueError:\n                    print(f\"{RED}Invalid number for documents: {sub_command}. Using default (5).{RESET}\")\n\n            print(f\"\\n{BOLD}{GREEN}=== Documents ==={RESET}\")\n            print(f\"Total documents: {len(self.documents)}\")\n            print(f\"Showing first {min(num_docs, len(self.documents))} document(s):\\n\")\n\n            for i, doc in enumerate(self.documents[:num_docs], 1):\n                print(f\"{CYAN}Document {i}:{RESET}\")\n                print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                # Show a snippet of text if available\n                if hasattr(doc, 'text') and doc.text:\n                    text_snippet = doc.text[:200] + \"...\" if len(doc.text) &gt; 200 else doc.text\n                    print(f\"  {BOLD}Text:{RESET} {text_snippet}\")\n                print()\n\n    # Handle dataframe command\n    if main_command in [\"all\", \"dataframe\"]:\n        if self.df is not None:\n            if sub_command == \"metadata\":\n                # Show DataFrame metadata columns (columns starting with metadata_)\n                print(f\"\\n{BOLD}{GREEN}=== DataFrame Metadata Columns ==={RESET}\")\n                metadata_cols = [col for col in self.df.columns if col.startswith(\"metadata_\")]\n                if metadata_cols:\n                    print(f\"Found {len(metadata_cols)} metadata column(s):\")\n                    for col in metadata_cols:\n                        print(f\"  {YELLOW}{col}{RESET}\")\n                        # Show some statistics for the metadata column\n                        print(f\"    Non-null values: {self.df[col].notna().sum()}\")\n                        print(f\"    Null values: {self.df[col].isna().sum()}\")\n                        # Show unique values if not too many\n                        unique_count = self.df[col].nunique()\n                        if unique_count &lt;= 10:\n                            print(f\"    Unique values ({unique_count}): {list(self.df[col].unique())}\")\n                        else:\n                            print(f\"    Unique values: {unique_count}\")\n                else:\n                    print(\"No metadata columns found (columns starting with 'metadata_')\")\n            elif sub_command == \"stats\":\n                # Show DataFrame statistics\n                self._print_dataframe_stats()\n            else:\n                # Show DataFrame head\n                print(f\"\\n{BOLD}{GREEN}=== DataFrame ==={RESET}\")\n                print(f\"Shape: {self.df.shape}\")\n                print(f\"Columns: {list(self.df.columns)}\")\n                print(\"\\nFirst few rows:\")\n                print(self.df.head())\n        else:\n            if main_command == \"dataframe\":\n                print(f\"\\n{BOLD}{RED}No DataFrame available{RESET}\")\n\n    # Handle metadata command\n    if main_command in [\"all\", \"metadata\"]:\n        if sub_command:\n            # Show specific metadata key\n            print(f\"\\n{BOLD}{GREEN}=== Metadata: {sub_command} ==={RESET}\")\n            if sub_command in self.metadata:\n                value = self.metadata[sub_command]\n                # Format the output based on the type of value\n                if isinstance(value, dict):\n                    for k, v in value.items():\n                        print(f\"{YELLOW}{k}:{RESET} {v}\")\n                elif isinstance(value, list):\n                    for i, item in enumerate(value, 1):\n                        print(f\"{i}. {item}\")\n                else:\n                    print(value)\n            else:\n                print(f\"{RED}Metadata key '{sub_command}' not found{RESET}\")\n                available_keys = list(self.metadata.keys())\n                if available_keys:\n                    print(f\"Available keys: {', '.join(available_keys)}\")\n        else:\n            # Show all metadata\n            print(f\"\\n{BOLD}{GREEN}=== Corpus Metadata ==={RESET}\")\n            if not self.metadata:\n                print(\"No metadata available\")\n            else:\n                for key, value in self.metadata.items():\n                    print(f\"\\n{MAGENTA}{key}:{RESET}\")\n                    # Truncate long values\n                    val_str = str(value)\n                    if len(val_str) &gt; 500:\n                        val_str = val_str[:497] + \"...\"\n                    print(f\"  {val_str}\")\n\n    # Handle stats command (deprecated, redirect to dataframe stats)\n    if main_command == \"stats\":\n        print(f\"{YELLOW}Note: 'stats' is deprecated. Use 'dataframe stats' instead.{RESET}\")\n        if self.df is not None:\n            self._print_dataframe_stats()\n        else:\n            print(f\"{RED}No DataFrame available{RESET}\")\n\n    print(f\"\\n{BOLD}Display completed for '{show}'{RESET}\")\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.remove_document_by_id","title":"<code>remove_document_by_id(document_id)</code>","text":"<p>Remove a document from the corpus by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>str</code> <p>ID of the document to remove.</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def remove_document_by_id(self, document_id: str):\n    \"\"\"\n    Remove a document from the corpus by its ID.\n\n    Args:\n        document_id: ID of the document to remove.\n    \"\"\"\n    self.documents = [\n        doc for doc in self.documents if doc.id != document_id\n    ]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.update_metadata","title":"<code>update_metadata(key, value)</code>","text":"<p>Update the metadata of the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Metadata key to update.</p> required <code>value</code> <code>Any</code> <p>New value for the metadata key.</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def update_metadata(self, key: str, value: Any):\n    \"\"\"\n    Update the metadata of the corpus.\n\n    Args:\n        key: Metadata key to update.\n        value: New value for the metadata key.\n    \"\"\"\n    self.metadata[key] = value\n</code></pre>"},{"location":"modules/#model.document.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document model for storing text and metadata.</p> Source code in <code>src/crisp_t/model/document.py</code> <pre><code>class Document(BaseModel):\n    \"\"\"\n    Document model for storing text and metadata.\n    \"\"\"\n    id: str = Field(..., description=\"Unique identifier for the document.\")\n    name: Optional[str] = Field(None, description=\"Name of the corpus.\")\n    description: Optional[str] = Field(None, description=\"Description of the corpus.\")\n    score: float = Field(0.0, description=\"Score associated with the document.\")\n    text: str = Field(..., description=\"The text content of the document.\")\n    metadata: dict = Field(\n        default_factory=dict, description=\"Metadata associated with the document.\"\n    )\n\n    def pretty_print(self):\n        \"\"\"\n        Print the document information in a human-readable format.\n        \"\"\"\n        print(f\"Document ID: {self.id}\")\n        print(f\"Name: {self.name}\")\n        print(f\"Description: {self.description}\")\n        print(f\"Score: {self.score}\")\n        print(f\"Text: {self.text[:100]}...\")  # Print first 100 characters of text\n        print(f\"Metadata: {self.metadata}\")\n        print(f\"Length of Text: {len(self.text)} characters\")\n        print(f\"Number of Metadata Entries: {len(self.metadata)}\")\n</code></pre>"},{"location":"modules/#model.document.Document.pretty_print","title":"<code>pretty_print()</code>","text":"<p>Print the document information in a human-readable format.</p> Source code in <code>src/crisp_t/model/document.py</code> <pre><code>def pretty_print(self):\n    \"\"\"\n    Print the document information in a human-readable format.\n    \"\"\"\n    print(f\"Document ID: {self.id}\")\n    print(f\"Name: {self.name}\")\n    print(f\"Description: {self.description}\")\n    print(f\"Score: {self.score}\")\n    print(f\"Text: {self.text[:100]}...\")  # Print first 100 characters of text\n    print(f\"Metadata: {self.metadata}\")\n    print(f\"Length of Text: {len(self.text)} characters\")\n    print(f\"Number of Metadata Entries: {len(self.metadata)}\")\n</code></pre>"},{"location":"modules/#csv.Csv","title":"<code>Csv</code>","text":"Source code in <code>src/crisp_t/csv.py</code> <pre><code>class Csv:\n\n    def __init__(\n        self,\n        corpus: Optional[Corpus] = None,\n        comma_separated_text_columns: str = \"\",\n        comma_separated_ignore_columns: str = \"\",\n        id_column: str = \"id\",\n    ):\n        \"\"\"\n        Initialize the Csv object.\n        \"\"\"\n        self._corpus = corpus\n        if self._corpus is None:\n            self._df = pd.DataFrame()\n            logger.info(\"No corpus provided. Creating an empty DataFrame.\")\n        else:\n            self._df = self._corpus.df\n            if self._df is None:\n                logger.info(\"No DataFrame found in the corpus. Creating a new one.\")\n                self._df = pd.DataFrame()\n        self._df_original = self._df.copy()\n        self._comma_separated_text_columns = comma_separated_text_columns\n        self._comma_separated_ignore_columns = comma_separated_ignore_columns\n        self._id_column = id_column\n        self._X = None\n        self._y = None\n        self._X_original = None\n        self._y_original = None\n        self._id_column = id_column\n\n    @property\n    def corpus(self) -&gt; Optional[Corpus]:\n        if self._corpus is not None and self._df is not None:\n            self._corpus.df = self._df\n        return self._corpus\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        if self._df is None:\n            return pd.DataFrame()\n        return self._df\n\n    @property\n    def comma_separated_text_columns(self) -&gt; str:\n        return self._comma_separated_text_columns\n\n    @property\n    def comma_separated_ignore_columns(self) -&gt; str:\n        return self._comma_separated_ignore_columns\n\n    @comma_separated_ignore_columns.setter\n    def comma_separated_ignore_columns(self, value: str) -&gt; None:\n        self._comma_separated_ignore_columns = value\n        logger.info(\"Comma-separated ignore columns set successfully.\")\n        logger.debug(\n            f\"Comma-separated ignore columns: {self._comma_separated_ignore_columns}\"\n        )\n        self._process_columns()\n\n    @property\n    def id_column(self) -&gt; str:\n        return self._id_column\n\n    @corpus.setter\n    def corpus(self, value: Corpus) -&gt; None:\n        self._corpus = value\n        if self._corpus is not None:\n            self._df = self._corpus.df\n            if self._df is None:\n                logger.info(\"No DataFrame found in the corpus. Creating a new one.\")\n                self._df = pd.DataFrame()\n            self._df_original = self._df.copy()\n            logger.info(\"Corpus set successfully.\")\n            logger.debug(f\"DataFrame content: {self._df.head()}\")\n            logger.debug(f\"DataFrame shape: {self._df.shape}\")\n            logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n        else:\n            logger.error(\"Failed to set corpus. Corpus is None.\")\n\n    @df.setter\n    def df(self, value: pd.DataFrame) -&gt; None:\n        self._df = value\n        logger.info(\"DataFrame set successfully.\")\n        logger.debug(f\"DataFrame content: {self._df.head()}\")\n        logger.debug(f\"DataFrame shape: {self._df.shape}\")\n        logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n\n    @comma_separated_text_columns.setter\n    def comma_separated_text_columns(self, value: str) -&gt; None:\n        self._comma_separated_text_columns = value\n        logger.info(\"Comma-separated text columns set successfully.\")\n        logger.debug(\n            f\"Comma-separated text columns: {self._comma_separated_text_columns}\"\n        )\n        self._process_columns()\n\n    @id_column.setter\n    def id_column(self, value: str) -&gt; None:\n        self._id_column = value\n        # Add id column to the list of ignored columns\n        ignore_cols = [\n            col\n            for col in self._comma_separated_ignore_columns.split(\",\")\n            if col.strip()\n        ]\n        if value not in ignore_cols:\n            ignore_cols.append(value)\n            self._comma_separated_ignore_columns = \",\".join(ignore_cols)\n            logger.debug(\n                f\"ID column '{value}' added to ignore columns: {self._comma_separated_ignore_columns}\"\n            )\n        logger.info(\"ID column set successfully.\")\n        logger.debug(f\"ID column: {self._id_column}\")\n\n    # TODO remove @deprecated\n    #! Do not use\n    def read_csv(self, file_path: str):\n        \"\"\"\n        Read a CSV file and create a DataFrame.\n        \"\"\"\n        try:\n            self._df = pd.read_csv(file_path)\n            logger.info(f\"CSV file {file_path} read successfully.\")\n            logger.debug(f\"DataFrame content: {self._df.head()}\")\n            logger.debug(f\"DataFrame shape: {self._df.shape}\")\n            logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n        except Exception as e:\n            logger.error(f\"Error reading CSV file: {e}\")\n            raise\n        return self._process_columns()\n\n    def _process_columns(self):\n        # ignore comma-separated ignore columns\n        if self._comma_separated_ignore_columns:\n            ignore_columns = [\n                col.strip()\n                for col in self._comma_separated_ignore_columns.split(\",\")\n                if col.strip()\n            ]\n            self._df.drop(columns=ignore_columns, inplace=True, errors=\"ignore\")\n            logger.info(\n                f\"Ignored columns: {ignore_columns}. Updated DataFrame shape: {self._df.shape}\"\n            )\n            logger.debug(f\"DataFrame content after dropping columns: {self._df.head()}\")\n        # ignore comma-separated text columns\n        if self._comma_separated_text_columns:\n            text_columns = [\n                col.strip()\n                for col in self._comma_separated_text_columns.split(\",\")\n                if col.strip()\n            ]\n            for col in text_columns:\n                if col in self._df.columns:\n                    self._df[col] = self._df[col].astype(str)\n                    logger.info(f\"Column {col} converted to string.\")\n                    logger.debug(f\"Column {col} content: {self._df[col].head()}\")\n                else:\n                    logger.warning(f\"Column {col} not found in DataFrame.\")\n        # ignore all columns with names starting with \"metadata_\"\n        self._df = self._df.loc[:, ~self._df.columns.str.startswith(\"metadata_\")]\n        return self._df\n\n    def write_csv(self, file_path: str, index: bool = False) -&gt; None:\n        if self._df is not None:\n            self._df.to_csv(file_path, index=index)\n            logger.info(f\"DataFrame written to {file_path}\")\n            logger.debug(f\"DataFrame content: {self._df.head()}\")\n            logger.debug(f\"Index: {index}\")\n        else:\n            logger.error(\"DataFrame is None. Cannot write to CSV.\")\n\n    def mark_missing(self):\n        \"\"\"Mark missing values in the DataFrame.\n        Missing values are considered as empty strings and are replaced with NaN.\n        Rows with NaN values are then dropped from the DataFrame.\n        \"\"\"\n        if self._df is not None:\n            self._df.replace(\"\", np.nan, inplace=True)\n            self._df.dropna(inplace=True)\n        else:\n            logger.error(\"DataFrame is None. Cannot mark missing values.\")\n\n    def mark_duplicates(self):\n        \"\"\"Mark duplicate rows in the DataFrame.\n        Duplicate rows are identified and dropped from the DataFrame.\n        \"\"\"\n        if self._df is not None:\n            self._df.drop_duplicates(inplace=True)\n        else:\n            logger.error(\"DataFrame is None. Cannot mark duplicates.\")\n\n    def restore_df(self):\n        self._df = self._df_original.copy()\n\n    def get_shape(self):\n        if self._df is not None:\n            return self._df.shape\n        else:\n            logger.error(\"DataFrame is None. Cannot get shape.\")\n            return None\n\n    def get_columns(self):\n        \"\"\"Get the list of columns in the DataFrame.\"\"\"\n        if self._df is not None:\n            return self._df.columns.tolist()\n        else:\n            logger.error(\"DataFrame is None. Cannot get columns.\")\n            return []\n\n    def get_column_types(self):\n        \"\"\"Get the data types of columns in the DataFrame.\"\"\"\n        if self._df is not None:\n            return self._df.dtypes.to_dict()\n        else:\n            logger.error(\"DataFrame is None. Cannot get column types.\")\n            return {}\n\n    def get_column_values(self, column_name: str):\n        \"\"\"Get the unique values in a column of the DataFrame.\"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            return self._df[column_name].tolist()\n        else:\n            logger.error(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n            return None\n\n    def retain_numeric_columns_only(self):\n        \"\"\"Retain only numeric columns in the DataFrame.\"\"\"\n        if self._df is not None:\n            self._df = self._df.select_dtypes(include=[np.number])\n            logger.info(\"DataFrame filtered to numeric columns only.\")\n        else:\n            logger.error(\"DataFrame is None. Cannot filter to numeric columns.\")\n\n    def comma_separated_include_columns(self, include_cols: str = \"\"):\n        \"\"\"Retain only specified columns in the DataFrame.\"\"\"\n        if include_cols == \"\":\n            return\n        if self._df is not None:\n            cols = [\n                col.strip()\n                for col in include_cols.split(\",\")\n                if col.strip() and col in self._df.columns\n            ]\n            self._df = self._df[cols]\n            logger.info(f\"DataFrame filtered to include columns: {cols}\")\n        else:\n            logger.error(\"DataFrame is None. Cannot filter to include columns.\")\n\n    def read_xy(self, y: str):\n        \"\"\"\n        Read X and y variables from the DataFrame.\n        \"\"\"\n        if self._df is None:\n            logger.error(\"DataFrame is None. Cannot read X and y.\")\n            return None, None\n        # Split into X and y\n        if y == \"\":\n            self._y = None\n        else:\n            self._y = self._df[y]\n        if y != \"\":\n            self._X = self._df.drop(columns=[y])\n        else:\n            self._X = self._df.copy()\n        logger.info(f\"X and y variables set. X shape: {self._X.shape}\")\n        return self._X, self._y\n\n    def drop_na(self):\n        \"\"\"Drop rows with any NA values from the DataFrame.\"\"\"\n        if self._df is not None:\n            self._df.dropna(inplace=True)\n            logger.info(\"Missing values dropped from DataFrame.\")\n        else:\n            logger.error(\"DataFrame is None. Cannot drop missing values.\")\n\n    def oversample(self, mcp: bool = False):\n        self._X_original = self._X\n        self._y_original = self._y\n        try:\n            from imblearn.over_sampling import RandomOverSampler\n\n            ros = RandomOverSampler(random_state=0)\n        except ImportError:\n            logger.info(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            return\n\n        result = ros.fit_resample(self._X, self._y)\n        if len(result) == 2:\n            X, y = result\n        elif len(result) == 3:\n            X, y, _ = result\n        else:\n            logger.error(\"Unexpected number of values returned from fit_resample.\")\n            return\n        self._X = X\n        self._y = y\n        if mcp:\n            return f\"Oversampling completed. New X shape: {self._X.shape}\"\n        return X, y\n\n    def restore_oversample(self, mcp: bool = False):\n        self._X = self._X_original\n        self._y = self._y_original\n        if mcp:\n            return f\"Oversampling restored. X shape: {self._X.shape}, y shape: {self._y.shape}\"  # type: ignore\n\n    def prepare_data(self, y: str, oversample=False, one_hot_encode_all=False):\n        self.mark_missing()\n        if oversample:\n            self.oversample()\n        self.one_hot_encode_strings_in_df()\n        if one_hot_encode_all:\n            self.one_hot_encode_all_columns()\n        return self.read_xy(y)\n\n    def bin_a_column(self, column_name: str, bins: int = 2):\n        \"\"\"Bin a numeric column into specified number of bins.\"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            if pd.api.types.is_numeric_dtype(self._df[column_name]):\n                self._df[column_name] = pd.cut(\n                    self._df[column_name], bins=bins, labels=False\n                )\n                logger.info(f\"Column {column_name} binned into {bins} bins.\")\n                return \"I have binned the column. Please proceed.\"\n            else:\n                logger.warning(f\"Column {column_name} is not numeric. Cannot bin.\")\n        else:\n            logger.warning(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n        return \"I cannot bin the column. Please check the logs for more information.\"\n\n    def one_hot_encode_column(self, column_name: str):\n        \"\"\"One-hot encode a specific column in the DataFrame.\n        This method converts a categorical column into one-hot encoded columns.\n        Used when # ValueError: could not convert string to float.\n        \"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            if pd.api.types.is_object_dtype(self._df[column_name]):\n                self._df = pd.get_dummies(\n                    self._df, columns=[column_name], drop_first=True\n                )\n                logger.info(f\"One-hot encoding applied to column {column_name}.\")\n                return \"I have one-hot encoded the column. Please proceed.\"\n            else:\n                logger.warning(f\"Column {column_name} is not of object type.\")\n        else:\n            logger.error(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n        return \"I cannot one-hot encode the column. Please check the logs for more information.\"\n\n    def one_hot_encode_strings_in_df(self, n=10, filter_high_cardinality=False):\n        \"\"\"One-hot encode string (object) columns in the DataFrame.\n        This method converts categorical string columns into one-hot encoded columns.\n        Columns with more than n unique values can be optionally filtered out.\n        Used when # ValueError: could not convert string to float.\n        \"\"\"\n        if self._df is not None:\n            categorical_cols = self._df.select_dtypes(\n                include=[\"object\"]\n            ).columns.tolist()\n            # Remove categorical columns with more than n unique values\n            if filter_high_cardinality:\n                categorical_cols = [\n                    col for col in categorical_cols if self._df[col].nunique() &lt;= n\n                ]\n            if categorical_cols:\n                self._df = pd.get_dummies(\n                    self._df, columns=categorical_cols, drop_first=True\n                )\n                logger.info(\"One-hot encoding applied to string columns.\")\n            else:\n                logger.info(\"No string (object) columns found for one-hot encoding.\")\n        else:\n            logger.error(\"DataFrame is None. Cannot apply one-hot encoding.\")\n\n    def one_hot_encode_all_columns(self):\n        \"\"\"One-hot encode all columns in the DataFrame.\n        This method converts all values in the DataFrame to boolean values.\n        Used for apriori algorithm which requires boolean values.\n        \"\"\"\n        if self._df is not None:\n\n            def to_one_hot(x):\n                if x in [1, True]:\n                    return True\n                elif x in [0, False]:\n                    return False\n                else:\n                    # logger.warning(\n                    #     f\"Unexpected value '{x}' encountered during one-hot encoding; mapping to 1.\"\n                    # )\n                    return True\n\n            self._df = self._df.applymap(to_one_hot)  # type: ignore\n\n    def filter_rows_by_column_value(\n        self, column_name: str, value, mcp: bool = False\n    ):\n        \"\"\"Select rows from the DataFrame where the specified column matches the given value.\n        Additionally, filter self._corpus.documents by id_column if present in DataFrame.\n        \"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            selected_df = self._df[self._df[column_name] == value]\n            if selected_df.empty:\n                # try int search\n                try:\n                    selected_df = self._df[self._df[column_name] == int(value)]\n                except (ValueError, TypeError):\n                    logger.warning(\n                        f\"Could not convert value '{value}' to int for column '{column_name}'.\"\n                    )\n            logger.info(\n                f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n            )\n            self._df = selected_df\n\n            # Check for id_column in DataFrame\n            if (\n                self._corpus is not None\n                and hasattr(self._corpus, \"df\")\n                and self._id_column in self._corpus.df.columns\n            ):\n                logger.info(f\"id_column '{self._id_column}' exists in DataFrame.\")\n                valid_ids = set(self._corpus.df[self._id_column].tolist())\n                if (\n                    hasattr(self._corpus, \"documents\")\n                    and self._corpus.documents is not None\n                ):\n                    filtered_docs = [\n                        doc\n                        for doc in self._corpus.documents\n                        if getattr(doc, self._id_column, None) in valid_ids\n                    ]\n                    self._corpus.documents = filtered_docs\n            else:\n                logger.warning(f\"id_column '{self._id_column}' does not exist in DataFrame.\")\n\n            if mcp:\n                return f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n        else:\n            logger.warning(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n            if mcp:\n                return (\n                    f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n                )\n            return pd.DataFrame()\n</code></pre>"},{"location":"modules/#csv.Csv.__init__","title":"<code>__init__(corpus=None, comma_separated_text_columns='', comma_separated_ignore_columns='', id_column='id')</code>","text":"<p>Initialize the Csv object.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def __init__(\n    self,\n    corpus: Optional[Corpus] = None,\n    comma_separated_text_columns: str = \"\",\n    comma_separated_ignore_columns: str = \"\",\n    id_column: str = \"id\",\n):\n    \"\"\"\n    Initialize the Csv object.\n    \"\"\"\n    self._corpus = corpus\n    if self._corpus is None:\n        self._df = pd.DataFrame()\n        logger.info(\"No corpus provided. Creating an empty DataFrame.\")\n    else:\n        self._df = self._corpus.df\n        if self._df is None:\n            logger.info(\"No DataFrame found in the corpus. Creating a new one.\")\n            self._df = pd.DataFrame()\n    self._df_original = self._df.copy()\n    self._comma_separated_text_columns = comma_separated_text_columns\n    self._comma_separated_ignore_columns = comma_separated_ignore_columns\n    self._id_column = id_column\n    self._X = None\n    self._y = None\n    self._X_original = None\n    self._y_original = None\n    self._id_column = id_column\n</code></pre>"},{"location":"modules/#csv.Csv.bin_a_column","title":"<code>bin_a_column(column_name, bins=2)</code>","text":"<p>Bin a numeric column into specified number of bins.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def bin_a_column(self, column_name: str, bins: int = 2):\n    \"\"\"Bin a numeric column into specified number of bins.\"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        if pd.api.types.is_numeric_dtype(self._df[column_name]):\n            self._df[column_name] = pd.cut(\n                self._df[column_name], bins=bins, labels=False\n            )\n            logger.info(f\"Column {column_name} binned into {bins} bins.\")\n            return \"I have binned the column. Please proceed.\"\n        else:\n            logger.warning(f\"Column {column_name} is not numeric. Cannot bin.\")\n    else:\n        logger.warning(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n    return \"I cannot bin the column. Please check the logs for more information.\"\n</code></pre>"},{"location":"modules/#csv.Csv.comma_separated_include_columns","title":"<code>comma_separated_include_columns(include_cols='')</code>","text":"<p>Retain only specified columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def comma_separated_include_columns(self, include_cols: str = \"\"):\n    \"\"\"Retain only specified columns in the DataFrame.\"\"\"\n    if include_cols == \"\":\n        return\n    if self._df is not None:\n        cols = [\n            col.strip()\n            for col in include_cols.split(\",\")\n            if col.strip() and col in self._df.columns\n        ]\n        self._df = self._df[cols]\n        logger.info(f\"DataFrame filtered to include columns: {cols}\")\n    else:\n        logger.error(\"DataFrame is None. Cannot filter to include columns.\")\n</code></pre>"},{"location":"modules/#csv.Csv.drop_na","title":"<code>drop_na()</code>","text":"<p>Drop rows with any NA values from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def drop_na(self):\n    \"\"\"Drop rows with any NA values from the DataFrame.\"\"\"\n    if self._df is not None:\n        self._df.dropna(inplace=True)\n        logger.info(\"Missing values dropped from DataFrame.\")\n    else:\n        logger.error(\"DataFrame is None. Cannot drop missing values.\")\n</code></pre>"},{"location":"modules/#csv.Csv.filter_rows_by_column_value","title":"<code>filter_rows_by_column_value(column_name, value, mcp=False)</code>","text":"<p>Select rows from the DataFrame where the specified column matches the given value. Additionally, filter self._corpus.documents by id_column if present in DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def filter_rows_by_column_value(\n    self, column_name: str, value, mcp: bool = False\n):\n    \"\"\"Select rows from the DataFrame where the specified column matches the given value.\n    Additionally, filter self._corpus.documents by id_column if present in DataFrame.\n    \"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        selected_df = self._df[self._df[column_name] == value]\n        if selected_df.empty:\n            # try int search\n            try:\n                selected_df = self._df[self._df[column_name] == int(value)]\n            except (ValueError, TypeError):\n                logger.warning(\n                    f\"Could not convert value '{value}' to int for column '{column_name}'.\"\n                )\n        logger.info(\n            f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n        )\n        self._df = selected_df\n\n        # Check for id_column in DataFrame\n        if (\n            self._corpus is not None\n            and hasattr(self._corpus, \"df\")\n            and self._id_column in self._corpus.df.columns\n        ):\n            logger.info(f\"id_column '{self._id_column}' exists in DataFrame.\")\n            valid_ids = set(self._corpus.df[self._id_column].tolist())\n            if (\n                hasattr(self._corpus, \"documents\")\n                and self._corpus.documents is not None\n            ):\n                filtered_docs = [\n                    doc\n                    for doc in self._corpus.documents\n                    if getattr(doc, self._id_column, None) in valid_ids\n                ]\n                self._corpus.documents = filtered_docs\n        else:\n            logger.warning(f\"id_column '{self._id_column}' does not exist in DataFrame.\")\n\n        if mcp:\n            return f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n    else:\n        logger.warning(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n        if mcp:\n            return (\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n        return pd.DataFrame()\n</code></pre>"},{"location":"modules/#csv.Csv.get_column_types","title":"<code>get_column_types()</code>","text":"<p>Get the data types of columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def get_column_types(self):\n    \"\"\"Get the data types of columns in the DataFrame.\"\"\"\n    if self._df is not None:\n        return self._df.dtypes.to_dict()\n    else:\n        logger.error(\"DataFrame is None. Cannot get column types.\")\n        return {}\n</code></pre>"},{"location":"modules/#csv.Csv.get_column_values","title":"<code>get_column_values(column_name)</code>","text":"<p>Get the unique values in a column of the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def get_column_values(self, column_name: str):\n    \"\"\"Get the unique values in a column of the DataFrame.\"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        return self._df[column_name].tolist()\n    else:\n        logger.error(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n        return None\n</code></pre>"},{"location":"modules/#csv.Csv.get_columns","title":"<code>get_columns()</code>","text":"<p>Get the list of columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def get_columns(self):\n    \"\"\"Get the list of columns in the DataFrame.\"\"\"\n    if self._df is not None:\n        return self._df.columns.tolist()\n    else:\n        logger.error(\"DataFrame is None. Cannot get columns.\")\n        return []\n</code></pre>"},{"location":"modules/#csv.Csv.mark_duplicates","title":"<code>mark_duplicates()</code>","text":"<p>Mark duplicate rows in the DataFrame. Duplicate rows are identified and dropped from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def mark_duplicates(self):\n    \"\"\"Mark duplicate rows in the DataFrame.\n    Duplicate rows are identified and dropped from the DataFrame.\n    \"\"\"\n    if self._df is not None:\n        self._df.drop_duplicates(inplace=True)\n    else:\n        logger.error(\"DataFrame is None. Cannot mark duplicates.\")\n</code></pre>"},{"location":"modules/#csv.Csv.mark_missing","title":"<code>mark_missing()</code>","text":"<p>Mark missing values in the DataFrame. Missing values are considered as empty strings and are replaced with NaN. Rows with NaN values are then dropped from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def mark_missing(self):\n    \"\"\"Mark missing values in the DataFrame.\n    Missing values are considered as empty strings and are replaced with NaN.\n    Rows with NaN values are then dropped from the DataFrame.\n    \"\"\"\n    if self._df is not None:\n        self._df.replace(\"\", np.nan, inplace=True)\n        self._df.dropna(inplace=True)\n    else:\n        logger.error(\"DataFrame is None. Cannot mark missing values.\")\n</code></pre>"},{"location":"modules/#csv.Csv.one_hot_encode_all_columns","title":"<code>one_hot_encode_all_columns()</code>","text":"<p>One-hot encode all columns in the DataFrame. This method converts all values in the DataFrame to boolean values. Used for apriori algorithm which requires boolean values.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def one_hot_encode_all_columns(self):\n    \"\"\"One-hot encode all columns in the DataFrame.\n    This method converts all values in the DataFrame to boolean values.\n    Used for apriori algorithm which requires boolean values.\n    \"\"\"\n    if self._df is not None:\n\n        def to_one_hot(x):\n            if x in [1, True]:\n                return True\n            elif x in [0, False]:\n                return False\n            else:\n                # logger.warning(\n                #     f\"Unexpected value '{x}' encountered during one-hot encoding; mapping to 1.\"\n                # )\n                return True\n\n        self._df = self._df.applymap(to_one_hot)  # type: ignore\n</code></pre>"},{"location":"modules/#csv.Csv.one_hot_encode_column","title":"<code>one_hot_encode_column(column_name)</code>","text":"<p>One-hot encode a specific column in the DataFrame. This method converts a categorical column into one-hot encoded columns. Used when # ValueError: could not convert string to float.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def one_hot_encode_column(self, column_name: str):\n    \"\"\"One-hot encode a specific column in the DataFrame.\n    This method converts a categorical column into one-hot encoded columns.\n    Used when # ValueError: could not convert string to float.\n    \"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        if pd.api.types.is_object_dtype(self._df[column_name]):\n            self._df = pd.get_dummies(\n                self._df, columns=[column_name], drop_first=True\n            )\n            logger.info(f\"One-hot encoding applied to column {column_name}.\")\n            return \"I have one-hot encoded the column. Please proceed.\"\n        else:\n            logger.warning(f\"Column {column_name} is not of object type.\")\n    else:\n        logger.error(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n    return \"I cannot one-hot encode the column. Please check the logs for more information.\"\n</code></pre>"},{"location":"modules/#csv.Csv.one_hot_encode_strings_in_df","title":"<code>one_hot_encode_strings_in_df(n=10, filter_high_cardinality=False)</code>","text":"<p>One-hot encode string (object) columns in the DataFrame. This method converts categorical string columns into one-hot encoded columns. Columns with more than n unique values can be optionally filtered out. Used when # ValueError: could not convert string to float.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def one_hot_encode_strings_in_df(self, n=10, filter_high_cardinality=False):\n    \"\"\"One-hot encode string (object) columns in the DataFrame.\n    This method converts categorical string columns into one-hot encoded columns.\n    Columns with more than n unique values can be optionally filtered out.\n    Used when # ValueError: could not convert string to float.\n    \"\"\"\n    if self._df is not None:\n        categorical_cols = self._df.select_dtypes(\n            include=[\"object\"]\n        ).columns.tolist()\n        # Remove categorical columns with more than n unique values\n        if filter_high_cardinality:\n            categorical_cols = [\n                col for col in categorical_cols if self._df[col].nunique() &lt;= n\n            ]\n        if categorical_cols:\n            self._df = pd.get_dummies(\n                self._df, columns=categorical_cols, drop_first=True\n            )\n            logger.info(\"One-hot encoding applied to string columns.\")\n        else:\n            logger.info(\"No string (object) columns found for one-hot encoding.\")\n    else:\n        logger.error(\"DataFrame is None. Cannot apply one-hot encoding.\")\n</code></pre>"},{"location":"modules/#csv.Csv.read_csv","title":"<code>read_csv(file_path)</code>","text":"<p>Read a CSV file and create a DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def read_csv(self, file_path: str):\n    \"\"\"\n    Read a CSV file and create a DataFrame.\n    \"\"\"\n    try:\n        self._df = pd.read_csv(file_path)\n        logger.info(f\"CSV file {file_path} read successfully.\")\n        logger.debug(f\"DataFrame content: {self._df.head()}\")\n        logger.debug(f\"DataFrame shape: {self._df.shape}\")\n        logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n    except Exception as e:\n        logger.error(f\"Error reading CSV file: {e}\")\n        raise\n    return self._process_columns()\n</code></pre>"},{"location":"modules/#csv.Csv.read_xy","title":"<code>read_xy(y)</code>","text":"<p>Read X and y variables from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def read_xy(self, y: str):\n    \"\"\"\n    Read X and y variables from the DataFrame.\n    \"\"\"\n    if self._df is None:\n        logger.error(\"DataFrame is None. Cannot read X and y.\")\n        return None, None\n    # Split into X and y\n    if y == \"\":\n        self._y = None\n    else:\n        self._y = self._df[y]\n    if y != \"\":\n        self._X = self._df.drop(columns=[y])\n    else:\n        self._X = self._df.copy()\n    logger.info(f\"X and y variables set. X shape: {self._X.shape}\")\n    return self._X, self._y\n</code></pre>"},{"location":"modules/#csv.Csv.retain_numeric_columns_only","title":"<code>retain_numeric_columns_only()</code>","text":"<p>Retain only numeric columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def retain_numeric_columns_only(self):\n    \"\"\"Retain only numeric columns in the DataFrame.\"\"\"\n    if self._df is not None:\n        self._df = self._df.select_dtypes(include=[np.number])\n        logger.info(\"DataFrame filtered to numeric columns only.\")\n    else:\n        logger.error(\"DataFrame is None. Cannot filter to numeric columns.\")\n</code></pre>"},{"location":"modules/#text.Text","title":"<code>Text</code>","text":"Source code in <code>src/crisp_t/text.py</code> <pre><code>class Text:\n\n    def __init__(\n        self, corpus: Optional[Corpus] = None, lang=\"en_core_web_sm\", max_length=1100000\n    ):\n        self._corpus = corpus\n        self._lang = lang\n        self._spacy_manager = SpacyManager(self._lang)\n        self._max_length = max_length\n        self._initial_document_count = len(self._corpus.documents) if corpus else 0  # type: ignore\n\n        self._spacy_doc = None\n        self._lemma = {}\n        self._pos = {}\n        self._pos_ = {}\n        self._word = {}\n        self._sentiment = {}\n        self._tag = {}\n        self._dep = {}\n        self._prob = {}\n        self._idx = {}\n\n    @property\n    def corpus(self):\n        \"\"\"\n        Get the corpus.\n        \"\"\"\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        return self._corpus\n\n    @property\n    def max_length(self):\n        \"\"\"\n        Get the maximum length of the corpus.\n        \"\"\"\n        return self._max_length\n\n    @property\n    def lang(self):\n        \"\"\"\n        Get the language of the corpus.\n        \"\"\"\n        return self._lang\n\n    @property\n    def initial_document_count(self):\n        \"\"\"\n        Get the initial document count.\n        \"\"\"\n        return self._initial_document_count\n\n    @corpus.setter\n    def corpus(self, corpus: Corpus):\n        \"\"\"\n        Set the corpus.\n        \"\"\"\n        if not isinstance(corpus, Corpus):\n            raise ValueError(\"Corpus must be of type Corpus\")\n        self._corpus = corpus\n        spacy_doc, results = self.process_tokens(self._corpus.id if self._corpus else None)\n        self._spacy_doc = spacy_doc\n        self._lemma = results[\"lemma\"]\n        self._pos = results[\"pos\"]\n        self._pos_ = results[\"pos_\"]\n        self._word = results[\"word\"]\n        self._sentiment = results[\"sentiment\"]\n        self._tag = results[\"tag\"]\n        self._dep = results[\"dep\"]\n        self._prob = results[\"prob\"]\n        self._idx = results[\"idx\"]\n\n    @max_length.setter\n    def max_length(self, max_length: int):\n        \"\"\"\n        Set the maximum length of the corpus.\n        \"\"\"\n        if not isinstance(max_length, int):\n            raise ValueError(\"max_length must be an integer\")\n        self._max_length = max_length\n        if self._spacy_doc is not None:\n            self._spacy_doc.max_length = max_length\n\n    @lang.setter\n    def lang(self, lang: str):\n        \"\"\"\n        Set the language of the corpus.\n        \"\"\"\n        if not isinstance(lang, str):\n            raise ValueError(\"lang must be a string\")\n        self._lang = lang\n        spacy_doc, results = self.process_tokens(self._corpus.id if self._corpus else None)\n        self._spacy_doc = spacy_doc\n        self._lemma = results[\"lemma\"]\n        self._pos = results[\"pos\"]\n        self._pos_ = results[\"pos_\"]\n        self._word = results[\"word\"]\n        self._sentiment = results[\"sentiment\"]\n        self._tag = results[\"tag\"]\n        self._dep = results[\"dep\"]\n        self._prob = results[\"prob\"]\n        self._idx = results[\"idx\"]\n\n    def make_spacy_doc(self):\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        text = \"\"\n        for document in tqdm(\n            self._corpus.documents,\n            desc=\"Processing documents\",\n            disable=len(self._corpus.documents) &lt; 10,\n        ):\n            text += self.process_text(document.text) + \" \\n\"\n            metadata = document.metadata\n        nlp = self._spacy_manager.get_model()\n        nlp.max_length = self._max_length\n        if len(text) &gt; self._max_length:\n            logger.warning(\n                f\"Text length {len(text)} exceeds max_length {self._max_length}.\"\n            )\n            text_chunks = [\n                text[i : i + self._max_length]\n                for i in range(0, len(text), self._max_length)\n            ]\n            spacy_docs = []\n            for chunk in tqdm(\n                text_chunks, desc=\"Processing text as chunks of max_length\"\n            ):\n                spacy_doc = nlp(chunk)\n                spacy_docs.append(spacy_doc)\n            self._spacy_doc = spacy_docs[0]\n            for doc in tqdm(spacy_docs[1:], desc=\"Merging spacy docs\"):\n                self._spacy_doc = Doc.from_docs([self._spacy_doc, doc])  # type: ignore\n        else:\n            self._spacy_doc = nlp(text)\n        return self._spacy_doc\n\n    # @lru_cache(maxsize=3)\n    def make_each_document_into_spacy_doc(self, id=\"corpus\"):\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n\n        # ! if cached file exists, load it\n        cache_dir = Path(\"cache\")\n        cache_file = cache_dir / f\"spacy_docs_{id}.pkl\"\n        if cache_file.exists():\n            with open(cache_file, \"rb\") as f:\n                spacy_docs, ids = pickle.load(f)\n            logger.info(\"Loaded cached spacy docs and ids.\")\n            return spacy_docs, ids\n\n        spacy_docs = []\n        ids = []\n        for document in tqdm(\n            self._corpus.documents,\n            desc=\"Creating spacy docs\",\n            disable=len(self._corpus.documents) &lt; 10,\n        ):\n            text = self.process_text(document.text)\n            metadata = document.metadata\n            nlp = self._spacy_manager.get_model()\n            nlp.max_length = self._max_length\n            spacy_doc = nlp(text)\n            spacy_docs.append(spacy_doc)\n            ids.append(document.id)\n\n        # ! dump spacy_docs, ids to a file for caching with the corpus id\n        cache_dir = Path(\"cache\")\n        cache_dir.mkdir(exist_ok=True)\n        cache_file = cache_dir / f\"spacy_docs_{id}.pkl\"\n        with open(cache_file, \"wb\") as f:\n            pickle.dump((spacy_docs, ids), f)\n        return spacy_docs, ids\n\n    def process_text(self, text: str) -&gt; str:\n        \"\"\"\n        Process the text by removing unwanted characters and normalizing it.\n        \"\"\"\n        # Remove unwanted characters\n        text = preprocessing.replace.urls(text)\n        text = preprocessing.replace.emails(text)\n        text = preprocessing.replace.phone_numbers(text)\n        text = preprocessing.replace.currency_symbols(text)\n        text = preprocessing.replace.hashtags(text)\n        text = preprocessing.replace.numbers(text)\n\n        # lowercase the text\n        text = text.lower()\n        return text\n\n    # @lru_cache(maxsize=3)\n    def process_tokens(self, id=\"corpus\"):\n        \"\"\"\n        Process tokens in the spacy document and extract relevant information.\n        \"\"\"\n\n        # ! if cached file exists, load it\n        cache_dir = Path(\"cache\")\n        cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n        if cache_file.exists():\n            with open(cache_file, \"rb\") as f:\n                spacy_doc, results = pickle.load(f)\n            logger.info(\"Loaded cached spacy doc and results.\")\n            return spacy_doc, results\n\n        spacy_doc = self.make_spacy_doc()\n        logger.info(\"Spacy doc created.\")\n\n        n_cores = multiprocessing.cpu_count()\n\n        def process_token(token):\n            if token.is_stop or token.is_digit or token.is_punct or token.is_space:\n                return None\n            if token.like_url or token.like_num or token.like_email:\n                return None\n            if len(token.text) &lt; 3 or token.text.isupper():\n                return None\n            return {\n                \"text\": token.text,\n                \"lemma\": token.lemma_,\n                \"pos\": token.pos_,\n                \"pos_\": token.pos,\n                \"word\": token.lemma_,\n                \"sentiment\": token.sentiment,\n                \"tag\": token.tag_,\n                \"dep\": token.dep_,\n                \"prob\": token.prob,\n                \"idx\": token.idx,\n            }\n\n        tokens = list(spacy_doc)\n        _lemma = {}\n        _pos = {}\n        _pos_ = {}\n        _word = {}\n        _sentiment = {}\n        _tag = {}\n        _dep = {}\n        _prob = {}\n        _idx = {}\n        with ThreadPoolExecutor() as executor:\n            futures = {executor.submit(process_token, token): token for token in tokens}\n            with tqdm(\n                total=len(futures),\n                desc=f\"Processing tokens (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    result = future.result()\n                    if result is not None:\n                        _lemma[result[\"text\"]] = result[\"lemma\"]\n                        _pos[result[\"text\"]] = result[\"pos\"]\n                        _pos_[result[\"text\"]] = result[\"pos_\"]\n                        _word[result[\"text\"]] = result[\"word\"]\n                        _sentiment[result[\"text\"]] = result[\"sentiment\"]\n                        _tag = result[\"tag\"]\n                        _dep = result[\"dep\"]\n                        _prob = result[\"prob\"]\n                        _idx = result[\"idx\"]\n                    pbar.update(1)\n        logger.info(\"Token processing complete.\")\n        results = {\n            \"lemma\": _lemma,\n            \"pos\": _pos,\n            \"pos_\": _pos_,\n            \"word\": _word,\n            \"sentiment\": _sentiment,\n            \"tag\": _tag,\n            \"dep\": _dep,\n            \"prob\": _prob,\n            \"idx\": _idx,\n        }\n        # ! dump spacy_doc, results to a file for caching with the corpus id\n        cache_dir = Path(\"cache\")\n        cache_dir.mkdir(exist_ok=True)\n        cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n        with open(cache_file, \"wb\") as f:\n            pickle.dump((spacy_doc, results), f)\n\n        return spacy_doc, results\n\n    def map_spacy_doc(self):\n        spacy_doc, results = self.process_tokens(self._corpus.id if self._corpus else None)\n        self._spacy_doc = spacy_doc\n        self._lemma = results[\"lemma\"]\n        self._pos = results[\"pos\"]\n        self._pos_ = results[\"pos_\"]\n        self._word = results[\"word\"]\n        self._sentiment = results[\"sentiment\"]\n        self._tag = results[\"tag\"]\n        self._dep = results[\"dep\"]\n        self._prob = results[\"prob\"]\n        self._idx = results[\"idx\"]\n\n    def common_words(self, index=10):\n        self.map_spacy_doc()\n        _words = {}\n        for key, value in self._word.items():\n            _words[value] = _words.get(value, 0) + 1\n        return sorted(_words.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def common_nouns(self, index=10):\n        self.map_spacy_doc()\n        _words = {}\n        for key, value in self._word.items():\n            if self._pos.get(key, None) == \"NOUN\":\n                _words[value] = _words.get(value, 0) + 1\n        return sorted(_words.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def common_verbs(self, index=10):\n        self.map_spacy_doc()\n        _words = {}\n        for key, value in self._word.items():\n            if self._pos.get(key, None) == \"VERB\":\n                _words[value] = _words.get(value, 0) + 1\n        return sorted(_words.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def print_coding_dictionary(self, num=10, top_n=5):\n        \"\"\"Prints a coding dictionary based on common verbs, attributes, and dimensions.\n        \"CATEGORY\" is the common verb\n        \"PROPERTY\" is the common nouns associated with the verb\n        \"DIMENSION\" is the common adjectives/adverbs/verbs associated with the property\n        Args:\n            num (int, optional): Number of common verbs to consider. Defaults to 10.\n            top_n (int, optional): Number of top attributes and dimensions to consider for each verb. Defaults to 5.\n\n        \"\"\"\n        self.map_spacy_doc()\n        output = []\n        coding_dict = []\n        output.append((\"CATEGORY\", \"PROPERTY\", \"DIMENSION\"))\n        verbs = self.common_verbs(num)\n        _verbs = []\n        for verb, freq in verbs:\n            _verbs.append(verb)\n        for verb, freq in verbs:\n            for attribute, f2 in self.attributes(verb, top_n):\n                for dimension, f3 in self.dimensions(attribute, top_n):\n                    if dimension not in _verbs:\n                        output.append((verb, attribute, dimension))\n                        coding_dict.append(f\"{verb} &gt; {attribute} &gt; {dimension}\")\n        # Add coding_dict to corpus metadata\n        if self._corpus is not None:\n            self._corpus.metadata[\"coding_dict\"] = coding_dict\n        print(\"\\n---Coding Dictionary---\")\n        QRUtils.print_table(output)\n        print(\"---------------------------\\n\")\n        return output\n\n    def sentences_with_common_nouns(self, index=10):\n        self.map_spacy_doc()\n        _nouns = self.common_nouns(index)\n        # Let's look at the sentences\n        sents = []\n        # Ensure self._spacy_doc is initialized\n        if self._spacy_doc is None:\n            self._spacy_doc = self.make_spacy_doc()\n        # the \"sents\" property returns spans\n        # spans have indices into the original string\n        # where each index value represents a token\n        for span in self._spacy_doc.sents:\n            # go from the start to the end of each span, returning each token in the sentence\n            # combine each token using join()\n            sent = \" \".join(\n                self._spacy_doc[i].text for i in range(span.start, span.end)\n            ).strip()\n            for noun, freq in _nouns:\n                if noun in sent:\n                    sents.append(sent)\n        return sents\n\n    def spans_with_common_nouns(self, word):\n        self.map_spacy_doc()\n        # Let's look at the sentences\n        spans = []\n        # the \"sents\" property returns spans\n        # spans have indices into the original string\n        # where each index value represents a token\n        if self._spacy_doc is None:\n            self._spacy_doc = self.make_spacy_doc()\n        for span in self._spacy_doc.sents:\n            # go from the start to the end of each span, returning each token in the sentence\n            # combine each token using join()\n            for token in span.text.split():\n                if word in self._word.get(token, \" \"):\n                    spans.append(span)\n        return spans\n\n    def dimensions(self, word, index=3):\n        self.map_spacy_doc()\n        _spans = self.spans_with_common_nouns(word)\n        _ad = {}\n        for span in _spans:\n            for token in span.text.split():\n                if self._pos.get(token, None) == \"ADJ\":\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n                if self._pos.get(token, None) == \"ADV\":\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n                if self._pos.get(token, None) == \"VERB\":\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n        return sorted(_ad.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def attributes(self, word, index=3):\n        self.map_spacy_doc()\n        _spans = self.spans_with_common_nouns(word)\n        _ad = {}\n        for span in _spans:\n            for token in span.text.split():\n                if self._pos.get(token, None) == \"NOUN\" and word not in self._word.get(\n                    token, \"\"\n                ):\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n                    # if self._pos.get(token, None) == 'VERB':\n                    # _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n        return sorted(_ad.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    # filter documents in the corpus based on metadata\n    def filter_documents(self, metadata_key, metadata_value, mcp=False, id_column=\"id\"):\n        \"\"\"\n        Filter documents in the corpus based on metadata.\n        If id_column exists in self._corpus.df, filter the DataFrame to match filtered documents' ids.\n        \"\"\"\n        # * filter does not require spacy mapping\n        # self.map_spacy_doc()\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        filtered_documents = []\n        for document in tqdm(\n            self._corpus.documents,\n            desc=\"Filtering documents\",\n            disable=len(self._corpus.documents) &lt; 10,\n        ):\n            meta_val = document.metadata.get(metadata_key)\n            # Check meta_val is not None and is iterable (str, list, tuple, set)\n            if meta_val is not None and isinstance(meta_val, (str, list, tuple, set)):\n                if metadata_value in meta_val:\n                    filtered_documents.append(document)\n            # Check document.id and document.text are not None and are str\n            if isinstance(document.id, str) and metadata_value in document.id:\n                filtered_documents.append(document)\n            if isinstance(document.name, str) and metadata_value in document.name:\n                filtered_documents.append(document)\n        self._corpus.documents = filtered_documents\n\n        # Check for id_column in self._corpus.df and filter df if present\n        if (\n            hasattr(self._corpus, \"df\")\n            and self._corpus.df is not None\n            and id_column in self._corpus.df.columns\n        ):\n            logger.info(f\"id_column '{id_column}' exists in DataFrame.\")\n            filtered_ids = [doc.id for doc in filtered_documents]\n            # Convert id_column to string before comparison\n            self._corpus.df = self._corpus.df[\n                self._corpus.df[id_column]\n                .astype(str)\n                .isin([str(i) for i in filtered_ids])\n            ]\n        else:\n            logger.warning(f\"id_column '{id_column}' does not exist in DataFrame.\")\n\n        if mcp:\n            return f\"Filtered {len(filtered_documents)} documents with {metadata_key} containing {metadata_value}\"\n        return filtered_documents\n\n    # get the count of documents in the corpus\n    def document_count(self):\n        \"\"\"\n        Get the count of documents in the corpus.\n        \"\"\"\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        return len(self._corpus.documents)\n\n    def generate_summary(self, weight=10):\n        \"\"\"[summary]\n\n        Args:\n            weight (int, optional): Parameter for summary generation weight. Defaults to 10.\n\n        Returns:\n            list: A list of summary lines\n        \"\"\"\n        self.map_spacy_doc()\n        words = self.common_words()\n        spans = []\n        ct = 0\n        for key, value in words:\n            ct += 1\n            if ct &gt; weight:\n                continue\n            for span in self.spans_with_common_nouns(key):\n                spans.append(span.text)\n        if self._corpus is not None:\n            self._corpus.metadata[\"summary\"] = list(\n                dict.fromkeys(spans)\n            )  # remove duplicates\n        return list(dict.fromkeys(spans))  # remove duplicates\n\n    def print_categories(self, spacy_doc=None, num=10):\n        self.map_spacy_doc()\n        bot = self._spacy_doc._.to_bag_of_terms( # type: ignore\n            by=\"lemma_\",\n            weighting=\"freq\",\n            ngs=(1, 2, 3),\n            ents=True,\n            ncs=True,\n            dedupe=True,\n        )\n        categories = sorted(bot.items(), key=lambda x: x[1], reverse=True)[:num]\n        output = []\n        to_return = []\n        print(\"\\n---Categories with count---\")\n        output.append((\"CATEGORY\", \"WEIGHT\"))\n        for category, count in categories:\n            output.append((category, str(count)))\n            to_return.append(category)\n        QRUtils.print_table(output)\n        print(\"---------------------------\\n\")\n        if self._corpus is not None:\n            self._corpus.metadata[\"categories\"] = output\n        return to_return\n\n    def category_basket(self, num=10):\n        item_basket = []\n        spacy_docs, ids = self.make_each_document_into_spacy_doc()\n        for spacy_doc in spacy_docs:\n            item_basket.append(self.print_categories(spacy_doc, num))\n        documents_copy = []\n        documents = self._corpus.documents if self._corpus is not None else []\n        # add cateogies to respective documents\n        for i, document in enumerate(documents):\n            if i &lt; len(item_basket):\n                document.metadata[\"categories\"] = item_basket[i]\n                documents_copy.append(document)\n        # update the corpus with the new documents\n        if self._corpus is not None:\n            self._corpus.documents = documents_copy\n        return item_basket\n        # Example return:\n        # [['GT', 'Strauss', 'coding', 'ground', 'theory', 'seminal', 'Corbin', 'code',\n        # 'structure', 'ground theory'], ['category', 'theory', 'comparison', 'incident',\n        # 'GT', 'structure', 'coding', 'Classical', 'Grounded', 'Theory'],\n        # ['theory', 'GT', 'evaluation'], ['open', 'coding', 'category', 'QRMine',\n        # 'open coding', 'researcher', 'step', 'data', 'break', 'analytically'],\n        # ['ground', 'theory', 'GT', 'ground theory'], ['category', 'comparison', 'incident',\n        # 'category comparison', 'Theory', 'theory']]\n\n    def category_association(self, num=10):\n        \"\"\"Generates the support for itemsets\n\n        Args:\n            num (int, optional): number of categories to generate for each doc in corpus. . Defaults to 10.\n        \"\"\"\n        self.map_spacy_doc()\n        basket = self.category_basket(num)\n        te = TransactionEncoder()\n        te_ary = te.fit(basket).transform(basket)\n        df = pd.DataFrame(te_ary, columns=te.columns_)  # type: ignore\n        _apriori = apriori(df, min_support=0.6, use_colnames=True)\n        # Example\n        #    support      itemsets\n        # 0  0.666667          (GT)\n        # 1  0.833333      (theory)\n        # 2  0.666667  (theory, GT)\n        documents_copy = []\n        documents = self._corpus.documents if self._corpus is not None else []\n        # TODO (Change) Add association rules to each document\n        for i, document in enumerate(documents):\n            if i &lt; len(basket):\n                # ! fix document.metadata[\"association_rules\"] = _apriori #TODO This is a corpus metadata, not a document one\n                documents_copy.append(document)\n        # Add to corpus metadata\n        if self._corpus is not None:\n            self._corpus.metadata[\"association_rules\"] = _apriori\n        # Update the corpus with the new documents\n        if self._corpus is not None:\n            self._corpus.documents = documents_copy\n        return _apriori\n</code></pre>"},{"location":"modules/#text.Text.corpus","title":"<code>corpus</code>  <code>property</code> <code>writable</code>","text":"<p>Get the corpus.</p>"},{"location":"modules/#text.Text.initial_document_count","title":"<code>initial_document_count</code>  <code>property</code>","text":"<p>Get the initial document count.</p>"},{"location":"modules/#text.Text.lang","title":"<code>lang</code>  <code>property</code> <code>writable</code>","text":"<p>Get the language of the corpus.</p>"},{"location":"modules/#text.Text.max_length","title":"<code>max_length</code>  <code>property</code> <code>writable</code>","text":"<p>Get the maximum length of the corpus.</p>"},{"location":"modules/#text.Text.category_association","title":"<code>category_association(num=10)</code>","text":"<p>Generates the support for itemsets</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>number of categories to generate for each doc in corpus. . Defaults to 10.</p> <code>10</code> Source code in <code>src/crisp_t/text.py</code> <pre><code>def category_association(self, num=10):\n    \"\"\"Generates the support for itemsets\n\n    Args:\n        num (int, optional): number of categories to generate for each doc in corpus. . Defaults to 10.\n    \"\"\"\n    self.map_spacy_doc()\n    basket = self.category_basket(num)\n    te = TransactionEncoder()\n    te_ary = te.fit(basket).transform(basket)\n    df = pd.DataFrame(te_ary, columns=te.columns_)  # type: ignore\n    _apriori = apriori(df, min_support=0.6, use_colnames=True)\n    # Example\n    #    support      itemsets\n    # 0  0.666667          (GT)\n    # 1  0.833333      (theory)\n    # 2  0.666667  (theory, GT)\n    documents_copy = []\n    documents = self._corpus.documents if self._corpus is not None else []\n    # TODO (Change) Add association rules to each document\n    for i, document in enumerate(documents):\n        if i &lt; len(basket):\n            # ! fix document.metadata[\"association_rules\"] = _apriori #TODO This is a corpus metadata, not a document one\n            documents_copy.append(document)\n    # Add to corpus metadata\n    if self._corpus is not None:\n        self._corpus.metadata[\"association_rules\"] = _apriori\n    # Update the corpus with the new documents\n    if self._corpus is not None:\n        self._corpus.documents = documents_copy\n    return _apriori\n</code></pre>"},{"location":"modules/#text.Text.document_count","title":"<code>document_count()</code>","text":"<p>Get the count of documents in the corpus.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def document_count(self):\n    \"\"\"\n    Get the count of documents in the corpus.\n    \"\"\"\n    if self._corpus is None:\n        raise ValueError(\"Corpus is not set\")\n    return len(self._corpus.documents)\n</code></pre>"},{"location":"modules/#text.Text.filter_documents","title":"<code>filter_documents(metadata_key, metadata_value, mcp=False, id_column='id')</code>","text":"<p>Filter documents in the corpus based on metadata. If id_column exists in self._corpus.df, filter the DataFrame to match filtered documents' ids.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def filter_documents(self, metadata_key, metadata_value, mcp=False, id_column=\"id\"):\n    \"\"\"\n    Filter documents in the corpus based on metadata.\n    If id_column exists in self._corpus.df, filter the DataFrame to match filtered documents' ids.\n    \"\"\"\n    # * filter does not require spacy mapping\n    # self.map_spacy_doc()\n    if self._corpus is None:\n        raise ValueError(\"Corpus is not set\")\n    filtered_documents = []\n    for document in tqdm(\n        self._corpus.documents,\n        desc=\"Filtering documents\",\n        disable=len(self._corpus.documents) &lt; 10,\n    ):\n        meta_val = document.metadata.get(metadata_key)\n        # Check meta_val is not None and is iterable (str, list, tuple, set)\n        if meta_val is not None and isinstance(meta_val, (str, list, tuple, set)):\n            if metadata_value in meta_val:\n                filtered_documents.append(document)\n        # Check document.id and document.text are not None and are str\n        if isinstance(document.id, str) and metadata_value in document.id:\n            filtered_documents.append(document)\n        if isinstance(document.name, str) and metadata_value in document.name:\n            filtered_documents.append(document)\n    self._corpus.documents = filtered_documents\n\n    # Check for id_column in self._corpus.df and filter df if present\n    if (\n        hasattr(self._corpus, \"df\")\n        and self._corpus.df is not None\n        and id_column in self._corpus.df.columns\n    ):\n        logger.info(f\"id_column '{id_column}' exists in DataFrame.\")\n        filtered_ids = [doc.id for doc in filtered_documents]\n        # Convert id_column to string before comparison\n        self._corpus.df = self._corpus.df[\n            self._corpus.df[id_column]\n            .astype(str)\n            .isin([str(i) for i in filtered_ids])\n        ]\n    else:\n        logger.warning(f\"id_column '{id_column}' does not exist in DataFrame.\")\n\n    if mcp:\n        return f\"Filtered {len(filtered_documents)} documents with {metadata_key} containing {metadata_value}\"\n    return filtered_documents\n</code></pre>"},{"location":"modules/#text.Text.generate_summary","title":"<code>generate_summary(weight=10)</code>","text":"<p>[summary]</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>int</code> <p>Parameter for summary generation weight. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of summary lines</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def generate_summary(self, weight=10):\n    \"\"\"[summary]\n\n    Args:\n        weight (int, optional): Parameter for summary generation weight. Defaults to 10.\n\n    Returns:\n        list: A list of summary lines\n    \"\"\"\n    self.map_spacy_doc()\n    words = self.common_words()\n    spans = []\n    ct = 0\n    for key, value in words:\n        ct += 1\n        if ct &gt; weight:\n            continue\n        for span in self.spans_with_common_nouns(key):\n            spans.append(span.text)\n    if self._corpus is not None:\n        self._corpus.metadata[\"summary\"] = list(\n            dict.fromkeys(spans)\n        )  # remove duplicates\n    return list(dict.fromkeys(spans))  # remove duplicates\n</code></pre>"},{"location":"modules/#text.Text.print_coding_dictionary","title":"<code>print_coding_dictionary(num=10, top_n=5)</code>","text":"<p>Prints a coding dictionary based on common verbs, attributes, and dimensions. \"CATEGORY\" is the common verb \"PROPERTY\" is the common nouns associated with the verb \"DIMENSION\" is the common adjectives/adverbs/verbs associated with the property Args:     num (int, optional): Number of common verbs to consider. Defaults to 10.     top_n (int, optional): Number of top attributes and dimensions to consider for each verb. Defaults to 5.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def print_coding_dictionary(self, num=10, top_n=5):\n    \"\"\"Prints a coding dictionary based on common verbs, attributes, and dimensions.\n    \"CATEGORY\" is the common verb\n    \"PROPERTY\" is the common nouns associated with the verb\n    \"DIMENSION\" is the common adjectives/adverbs/verbs associated with the property\n    Args:\n        num (int, optional): Number of common verbs to consider. Defaults to 10.\n        top_n (int, optional): Number of top attributes and dimensions to consider for each verb. Defaults to 5.\n\n    \"\"\"\n    self.map_spacy_doc()\n    output = []\n    coding_dict = []\n    output.append((\"CATEGORY\", \"PROPERTY\", \"DIMENSION\"))\n    verbs = self.common_verbs(num)\n    _verbs = []\n    for verb, freq in verbs:\n        _verbs.append(verb)\n    for verb, freq in verbs:\n        for attribute, f2 in self.attributes(verb, top_n):\n            for dimension, f3 in self.dimensions(attribute, top_n):\n                if dimension not in _verbs:\n                    output.append((verb, attribute, dimension))\n                    coding_dict.append(f\"{verb} &gt; {attribute} &gt; {dimension}\")\n    # Add coding_dict to corpus metadata\n    if self._corpus is not None:\n        self._corpus.metadata[\"coding_dict\"] = coding_dict\n    print(\"\\n---Coding Dictionary---\")\n    QRUtils.print_table(output)\n    print(\"---------------------------\\n\")\n    return output\n</code></pre>"},{"location":"modules/#text.Text.process_text","title":"<code>process_text(text)</code>","text":"<p>Process the text by removing unwanted characters and normalizing it.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def process_text(self, text: str) -&gt; str:\n    \"\"\"\n    Process the text by removing unwanted characters and normalizing it.\n    \"\"\"\n    # Remove unwanted characters\n    text = preprocessing.replace.urls(text)\n    text = preprocessing.replace.emails(text)\n    text = preprocessing.replace.phone_numbers(text)\n    text = preprocessing.replace.currency_symbols(text)\n    text = preprocessing.replace.hashtags(text)\n    text = preprocessing.replace.numbers(text)\n\n    # lowercase the text\n    text = text.lower()\n    return text\n</code></pre>"},{"location":"modules/#text.Text.process_tokens","title":"<code>process_tokens(id='corpus')</code>","text":"<p>Process tokens in the spacy document and extract relevant information.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def process_tokens(self, id=\"corpus\"):\n    \"\"\"\n    Process tokens in the spacy document and extract relevant information.\n    \"\"\"\n\n    # ! if cached file exists, load it\n    cache_dir = Path(\"cache\")\n    cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n    if cache_file.exists():\n        with open(cache_file, \"rb\") as f:\n            spacy_doc, results = pickle.load(f)\n        logger.info(\"Loaded cached spacy doc and results.\")\n        return spacy_doc, results\n\n    spacy_doc = self.make_spacy_doc()\n    logger.info(\"Spacy doc created.\")\n\n    n_cores = multiprocessing.cpu_count()\n\n    def process_token(token):\n        if token.is_stop or token.is_digit or token.is_punct or token.is_space:\n            return None\n        if token.like_url or token.like_num or token.like_email:\n            return None\n        if len(token.text) &lt; 3 or token.text.isupper():\n            return None\n        return {\n            \"text\": token.text,\n            \"lemma\": token.lemma_,\n            \"pos\": token.pos_,\n            \"pos_\": token.pos,\n            \"word\": token.lemma_,\n            \"sentiment\": token.sentiment,\n            \"tag\": token.tag_,\n            \"dep\": token.dep_,\n            \"prob\": token.prob,\n            \"idx\": token.idx,\n        }\n\n    tokens = list(spacy_doc)\n    _lemma = {}\n    _pos = {}\n    _pos_ = {}\n    _word = {}\n    _sentiment = {}\n    _tag = {}\n    _dep = {}\n    _prob = {}\n    _idx = {}\n    with ThreadPoolExecutor() as executor:\n        futures = {executor.submit(process_token, token): token for token in tokens}\n        with tqdm(\n            total=len(futures),\n            desc=f\"Processing tokens (parallel, {n_cores} cores)\",\n        ) as pbar:\n            for future in as_completed(futures):\n                result = future.result()\n                if result is not None:\n                    _lemma[result[\"text\"]] = result[\"lemma\"]\n                    _pos[result[\"text\"]] = result[\"pos\"]\n                    _pos_[result[\"text\"]] = result[\"pos_\"]\n                    _word[result[\"text\"]] = result[\"word\"]\n                    _sentiment[result[\"text\"]] = result[\"sentiment\"]\n                    _tag = result[\"tag\"]\n                    _dep = result[\"dep\"]\n                    _prob = result[\"prob\"]\n                    _idx = result[\"idx\"]\n                pbar.update(1)\n    logger.info(\"Token processing complete.\")\n    results = {\n        \"lemma\": _lemma,\n        \"pos\": _pos,\n        \"pos_\": _pos_,\n        \"word\": _word,\n        \"sentiment\": _sentiment,\n        \"tag\": _tag,\n        \"dep\": _dep,\n        \"prob\": _prob,\n        \"idx\": _idx,\n    }\n    # ! dump spacy_doc, results to a file for caching with the corpus id\n    cache_dir = Path(\"cache\")\n    cache_dir.mkdir(exist_ok=True)\n    cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n    with open(cache_file, \"wb\") as f:\n        pickle.dump((spacy_doc, results), f)\n\n    return spacy_doc, results\n</code></pre>"},{"location":"modules/#ml.ML","title":"<code>ML</code>","text":"Source code in <code>src/crisp_t/ml.py</code> <pre><code>class ML:\n    def __init__(\n        self,\n        csv: Csv,\n    ):\n        if not ML_INSTALLED:\n            raise ImportError(\"ML dependencies are not installed.\")\n        self._csv = csv\n        self._epochs = 3\n        self._samplesize = 0\n\n    @property\n    def csv(self):\n        return self._csv\n\n    @property\n    def corpus(self):\n        return self._csv.corpus\n\n    @csv.setter\n    def csv(self, value):\n        if isinstance(value, Csv):\n            self._csv = value\n        else:\n            raise ValueError(f\"The input belongs to {type(value)} instead of Csv.\")\n\n    def get_kmeans(self, number_of_clusters=3, seed=42, verbose=True, mcp=False):\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling get_kmeans.\"\n            )\n        X, _ = self._csv.read_xy(\"\")  # No output variable for clustering\n        if X is None:\n            raise ValueError(\n                \"Input features X are None. Cannot perform KMeans clustering.\"\n            )\n        kmeans = KMeans(\n            n_clusters=number_of_clusters, init=\"k-means++\", random_state=seed\n        )\n        self._clusters = kmeans.fit_predict(X)\n        members = self._get_members(self._clusters, number_of_clusters)\n        # Add cluster info to csv to metadata_cluster column\n        if self._csv is not None and getattr(self._csv, \"df\", None) is not None:\n            self._csv.df[\"metadata_cluster\"] = self._clusters\n        if verbose:\n            print(\"KMeans Cluster Centers:\\n\", kmeans.cluster_centers_)\n            print(\n                \"KMeans Inertia (Sum of squared distances to closest cluster center):\\n\",\n                kmeans.inertia_,\n            )\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"kmeans\"] = (\n                    f\"KMeans clustering with {number_of_clusters} clusters. Inertia: {kmeans.inertia_}\"\n                )\n        # Add members info to corpus metadata\n        members_info = \"\\n\".join(\n            [\n                f\"Cluster {i}: {len(members[i])} members\"\n                for i in range(number_of_clusters)\n            ]\n        )\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"kmeans_members\"] = (\n                f\"KMeans clustering members:\\n{members_info}\"\n            )\n        if mcp:\n            return members_info\n        return self._clusters, members\n\n    def _get_members(self, clusters, number_of_clusters=3):\n        _df = self._csv.df\n        self._csv.df = _df\n        members = []\n        for i in range(number_of_clusters):\n            members.append([])\n        for i, cluster in enumerate(clusters):\n            members[cluster].append(i)\n        return members\n\n    def profile(self, members, number_of_clusters=3):\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling profile.\"\n            )\n        _corpus = self._csv.corpus\n        _numeric_clusters = \"\"\n        for i in range(number_of_clusters):\n            print(\"Cluster: \", i)\n            print(\"Cluster Length: \", len(members[i]))\n            print(\"Cluster Members\")\n            if self._csv is not None and getattr(self._csv, \"df\", None) is not None:\n                print(self._csv.df.iloc[members[i], :])\n                print(\"Centroids\")\n                print(self._csv.df.iloc[members[i], :].mean(axis=0))\n                _numeric_clusters += f\"Cluster {i} with {len(members[i])} members\\n has the following centroids (mean values):\\n\"\n                _numeric_clusters += (\n                    f\"{self._csv.df.iloc[members[i], :].mean(axis=0)}\\n\"\n                )\n            else:\n                print(\"DataFrame (self._csv.df) is not set.\")\n        if _corpus is not None:\n            _corpus.metadata[\"numeric_clusters\"] = _numeric_clusters\n            self._csv.corpus = _corpus\n        return members\n\n    def get_nnet_predictions(self, y: str, mcp=False):\n        \"\"\"\n        Extended: Handles binary (BCELoss) and multi-class (CrossEntropyLoss).\n        Returns list of predicted original class labels.\n        \"\"\"\n        if ML_INSTALLED is False:\n            logger.info(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            return None\n\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling profile.\"\n            )\n        _corpus = self._csv.corpus\n\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n\n        unique_classes = np.unique(Y_raw)\n        num_classes = unique_classes.size\n        if num_classes &lt; 2:\n            raise ValueError(f\"Need at least 2 classes; found {num_classes}.\")\n\n        vnum = X_np.shape[1]\n\n        # Binary path\n        if num_classes == 2:\n            # Map to {0.0,1.0} for BCELoss if needed\n            mapping_applied = False\n            class_mapping = {}\n            inverse_mapping = {}\n            # Ensure deterministic order\n            sorted_classes = sorted(unique_classes.tolist())\n            if not (sorted_classes == [0, 1] or sorted_classes == [0.0, 1.0]):\n                class_mapping = {sorted_classes[0]: 0.0, sorted_classes[1]: 1.0}\n                inverse_mapping = {v: k for k, v in class_mapping.items()}\n                Y_mapped = np.vectorize(class_mapping.get)(Y_raw).astype(np.float32)\n                mapping_applied = True\n            else:\n                Y_mapped = Y_raw.astype(np.float32)\n\n            model = NeuralNet(vnum)\n            try:\n                criterion = nn.BCELoss()  # type: ignore\n                optimizer = optim.Adam(model.parameters(), lr=0.001)  # type: ignore\n\n                X_tensor = torch.from_numpy(X_np)  # type: ignore\n                y_tensor = torch.from_numpy(Y_mapped.astype(np.float32)).view(-1, 1)  # type: ignore\n\n                dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n                dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # type: ignore\n            except Exception as e:\n                logger.error(f\"Error occurred while creating DataLoader: {e}\")\n                return None\n\n            for _ in range(self._epochs):\n                for batch_X, batch_y in dataloader:\n                    optimizer.zero_grad()\n                    outputs = model(batch_X)\n                    loss = criterion(outputs, batch_y)\n                    if torch.isnan(loss):  # type: ignore\n                        raise RuntimeError(\"NaN loss encountered.\")\n                    loss.backward()\n                    optimizer.step()\n\n            # Predictions\n            bin_preds_internal = None\n            if torch:\n                with torch.no_grad():\n                    probs = model(torch.from_numpy(X_np)).view(-1).cpu().numpy()\n                bin_preds_internal = (probs &gt;= 0.5).astype(int)\n\n            if mapping_applied:\n                preds = [inverse_mapping[float(p)] for p in bin_preds_internal]  # type: ignore\n                y_eval = np.vectorize(class_mapping.get)(Y_raw).astype(int)\n                preds_eval = bin_preds_internal\n            else:\n                preds = bin_preds_internal.tolist()  # type: ignore\n                y_eval = Y_mapped.astype(int)\n                preds_eval = bin_preds_internal\n\n            accuracy = (preds_eval == y_eval).sum() / len(y_eval)\n            print(\n                f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n            )\n            if _corpus is not None:\n                _corpus.metadata[\"nnet_predictions\"] = (\n                    f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n                )\n            if mcp:\n                return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n            return preds\n\n        # Multi-class path\n        # Map original classes to indices\n        sorted_classes = sorted(unique_classes.tolist())\n        class_to_idx = {c: i for i, c in enumerate(sorted_classes)}\n        idx_to_class = {i: c for c, i in class_to_idx.items()}\n        Y_idx = np.vectorize(class_to_idx.get)(Y_raw).astype(np.int64)\n\n        model = MultiClassNet(vnum, num_classes)\n        criterion = nn.CrossEntropyLoss()  # type: ignore\n        optimizer = optim.Adam(model.parameters(), lr=0.001)  # type: ignore\n\n        X_tensor = torch.from_numpy(X_np)  # type: ignore\n        y_tensor = torch.from_numpy(Y_idx)  # type: ignore\n\n        dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # type: ignore\n\n        for _ in range(self._epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                logits = model(batch_X)\n                loss = criterion(logits, batch_y)\n                if torch.isnan(loss):  # type: ignore\n                    raise RuntimeError(\"NaN loss encountered.\")\n                loss.backward()\n                optimizer.step()\n\n        with torch.no_grad():  # type: ignore\n            logits_full = model(torch.from_numpy(X_np))  # type: ignore\n            pred_indices = torch.argmax(logits_full, dim=1).cpu().numpy()  # type: ignore\n\n        preds = [idx_to_class[i] for i in pred_indices]\n        accuracy = (pred_indices == Y_idx).sum() / len(Y_idx)\n        print(\n            f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n        )\n        if _corpus is not None:\n            _corpus.metadata[\"nnet_predictions\"] = (\n                f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n            )\n        if mcp:\n            return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n        return preds\n\n    def _convert_to_binary(self, Y):\n        unique_values = np.unique(Y)\n        if len(unique_values) != 2:\n            logger.warning(\n                \"Target variable has more than two unique values.\"\n            )\n            # convert unique_values[0] to 0, rest to 1\n            mapping = {val: (0 if val == unique_values[0] else 1) for val in unique_values}\n        else:\n            mapping = {unique_values[0]: 0, unique_values[1]: 1}\n        Y_binary = np.vectorize(mapping.get)(Y)\n        print(f\"Converted target variable to binary using mapping: {mapping}\")\n        return Y_binary\n\n    def svm_confusion_matrix(self, y: str, test_size=0.25, random_state=0, mcp=False):\n        \"\"\"Generate confusion matrix for SVM\n\n        Returns:\n            [list] -- [description]\n        \"\"\"\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n        Y = self._convert_to_binary(Y)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, Y, test_size=test_size, random_state=random_state\n        )\n        sc = StandardScaler()\n        # Issue #22\n        y_test = y_test.astype(\"int\")\n        y_train = y_train.astype(\"int\")\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.transform(X_test)\n        classifier = SVC(kernel=\"linear\", random_state=0)\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        # Issue #22\n        y_pred = y_pred.astype(\"int\")\n        _confusion_matrix = confusion_matrix(y_test, y_pred)\n        print(f\"Confusion Matrix for SVM predicting {y}:\\n{_confusion_matrix}\")\n        # Output\n        # [[2 0]\n        #  [2 0]]\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"svm_confusion_matrix\"] = (\n                f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n            )\n\n        if mcp:\n            return f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n\n        return _confusion_matrix\n\n    def format_confusion_matrix_to_human_readable(\n        self, confusion_matrix: np.ndarray\n    ) -&gt; str:\n        \"\"\"Format the confusion matrix to a human-readable string.\n\n        Args:\n            confusion_matrix (np.ndarray): The confusion matrix to format.\n\n        Returns:\n            str: The formatted confusion matrix with true positive, false positive, true negative, and false negative counts.\n        \"\"\"\n        tn, fp, fn, tp = confusion_matrix.ravel()\n        return (\n            f\"True Positive: {tp}\\n\"\n            f\"False Positive: {fp}\\n\"\n            f\"True Negative: {tn}\\n\"\n            f\"False Negative: {fn}\\n\"\n        )\n\n    # https://stackoverflow.com/questions/45419203/python-numpy-extracting-a-row-from-an-array\n    def knn_search(self, y: str, n=3, r=3, mcp=False):\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n        kdt = KDTree(X_np, leaf_size=2, metric=\"euclidean\")\n        dist, ind = kdt.query(X_np[r - 1 : r, :], k=n)\n        # Display results as human readable (1-based)\n        ind = (ind + 1).tolist()  # Convert to 1-based index\n        dist = dist.tolist()\n        print(\n            f\"\\nKNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\\n\"\n        )\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"knn_search\"] = (\n                f\"KNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\"\n            )\n        if mcp:\n            return f\"KNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\"\n        return dist, ind\n\n    def _process_xy(self, y: str, oversample=False, one_hot_encode_all=False):\n        X, Y = self._csv.prepare_data(\n            y=y, oversample=oversample, one_hot_encode_all=one_hot_encode_all\n        )\n        if X is None or Y is None:\n            raise ValueError(\"prepare_data returned None for X or Y.\")\n\n        # To numpy float32\n        X_np = (\n            X.to_numpy(dtype=np.float32)\n            if hasattr(X, \"to_numpy\")\n            else np.asarray(X, dtype=np.float32)\n        )\n        Y_raw = Y.to_numpy() if hasattr(Y, \"to_numpy\") else np.asarray(Y)\n\n        # Handle NaNs\n        if np.isnan(X_np).any():\n            raise ValueError(\"NaN detected in feature matrix.\")\n        if np.isnan(Y_raw.astype(float, copy=False)).any():\n            raise ValueError(\"NaN detected in target vector.\")\n\n        return X_np, Y_raw, X, Y\n\n    def get_decision_tree_classes(\n        self, y: str, top_n=5, test_size=0.5, random_state=1, mcp=False\n    ):\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n        Y_raw = self._convert_to_binary(Y_raw)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_np, Y_raw, test_size=test_size, random_state=random_state\n        )\n\n        # print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n        # print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n\n        # Train a RandomForestClassifier\n        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n        clf.fit(X_train, y_train)\n\n        # Compute permutation importance\n        results = permutation_importance(\n            clf, X_test, y_test, n_repeats=10, random_state=42\n        )\n\n        # classifier = DecisionTreeClassifier(random_state=random_state) # type: ignore\n        # classifier.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        _confusion_matrix = confusion_matrix(y_test, y_pred)\n        print(\n            f\"Confusion Matrix for Decision Tree predicting {y}:\\n{_confusion_matrix}\"\n        )\n        # Output\n        # [[2 0]\n        #  [2 0]]\n\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"\\nAccuracy: {accuracy}\\n\")\n\n        # Retrieve feature importance scores\n        importance = results.importances_mean\n\n        # Get indices of top N important features\n        top_n_indices = np.argsort(importance)[-top_n:][::-1]\n\n        # Display feature importance\n        print(f\"==== Top {top_n} important features ====\\n\")\n        _importance = \"\"\n        for i, v in enumerate(top_n_indices):\n            print(f\"Feature: {X.columns[v]}, Score: {importance[v]:.5f}\")\n            _importance += f\"Feature: {X.columns[v]}, Score: {importance[v]:.5f}\\n\"\n\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"decision_tree_accuracy\"] = (\n                f\"Decision Tree accuracy for predicting {y}: {accuracy*100:.2f}%\"\n            )\n            self._csv.corpus.metadata[\"decision_tree_confusion_matrix\"] = (\n                f\"Confusion Matrix for Decision Tree predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n            )\n            self._csv.corpus.metadata[\"decision_tree_feature_importance\"] = _importance\n        if mcp:\n            return f\"\"\"\n            Confusion Matrix for Decision Tree predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\\nTop {top_n} important features:\\n{_importance}\n            Accuracy: {accuracy*100:.2f}%\n            \"\"\"\n        return _confusion_matrix, importance\n\n    def get_xgb_classes(\n        self, y: str, oversample=False, test_size=0.25, random_state=0, mcp=False\n    ):\n        try:\n            from xgboost import XGBClassifier  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"XGBoost is not installed. Please install it via `pip install crisp-t[xg]`.\"\n            )\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n        if ML_INSTALLED:\n            # ValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1 2]\n            # convert y to binary\n            Y_binary = (Y_raw == 1).astype(int)\n            X_train, X_test, y_train, y_test = train_test_split(\n                X_np, Y_binary, test_size=test_size, random_state=random_state\n            )\n            classifier = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")  # type: ignore\n            classifier.fit(X_train, y_train)\n            y_pred = classifier.predict(X_test)\n            _confusion_matrix = confusion_matrix(y_test, y_pred)\n            print(f\"Confusion Matrix for XGBoost predicting {y}:\\n{_confusion_matrix}\")\n            # Output\n            # [[2 0]\n            #  [2 0]]\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"xgb_confusion_matrix\"] = (\n                    f\"Confusion Matrix for XGBoost predicting {y}:\\n{_confusion_matrix}\"\n                )\n            if mcp:\n                return f\"\"\"\n                Confusion Matrix for XGBoost predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\n                \"\"\"\n            return _confusion_matrix\n        else:\n            raise ImportError(\"ML dependencies are not installed.\")\n\n    def get_apriori(\n        self, y: str, min_support=0.9, use_colnames=True, min_threshold=0.5, mcp=False\n    ):\n        if ML_INSTALLED:\n            X_np, Y_raw, X, Y = self._process_xy(y=y, one_hot_encode_all=True)\n            frequent_itemsets = apriori(X, min_support=min_support, use_colnames=use_colnames)  # type: ignore\n            # rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_threshold) # type: ignore\n            human_readable = tabulate(\n                frequent_itemsets.head(10), headers=\"keys\", tablefmt=\"pretty\"  # type: ignore\n            )\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"apriori_frequent_itemsets\"] = human_readable\n            if mcp:\n                return f\"Frequent itemsets (top 10):\\n{human_readable}\"\n            return frequent_itemsets  # , rules\n        else:\n            raise ImportError(\"ML dependencies are not installed.\")\n\n    def get_pca(self, y: str, n: int = 3, mcp=False):\n        \"\"\"\n        Perform a manual PCA (no sklearn PCA) on the feature matrix for target y.\n\n        Args:\n            y (str): Target column name (used only for data preparation).\n            n (int): Number of principal components to keep.\n\n        Returns:\n            dict: {\n                'covariance_matrix': cov_mat,\n                'eigenvalues': eig_vals_sorted,\n                'eigenvectors': eig_vecs_sorted,\n                'explained_variance_ratio': var_exp,\n                'cumulative_explained_variance_ratio': cum_var_exp,\n                'projection_matrix': matrix_w,\n                'transformed': X_pca\n            }\n        \"\"\"\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n        X_std = StandardScaler().fit_transform(X_np)\n\n        cov_mat = np.cov(X_std.T)\n        eig_vals, eig_vecs = np.linalg.eigh(cov_mat)  # symmetric matrix -&gt; eigh\n\n        # Sort eigenvalues (and vectors) descending\n        idx = np.argsort(eig_vals)[::-1]\n        eig_vals_sorted = eig_vals[idx]\n        eig_vecs_sorted = eig_vecs[:, idx]\n\n        factors = X_std.shape[1]\n        n = max(1, min(n, factors))\n\n        # Explained variance ratios\n        tot = eig_vals_sorted.sum()\n        var_exp = (eig_vals_sorted / tot) * 100.0\n        cum_var_exp = np.cumsum(var_exp)\n\n        # Projection matrix (first n eigenvectors)\n        matrix_w = eig_vecs_sorted[:, :n]\n\n        # Project data\n        X_pca = X_std @ matrix_w\n\n        # Optional prints (retain original behavior)\n        print(\"Covariance matrix:\\n\", cov_mat)\n        print(\"Eigenvalues (desc):\\n\", eig_vals_sorted)\n        print(\"Explained variance (%):\\n\", var_exp[:n])\n        print(\"Cumulative explained variance (%):\\n\", cum_var_exp[:n])\n        print(\"Projection matrix (W):\\n\", matrix_w)\n        print(\"Transformed (first 5 rows):\\n\", X_pca[:5])\n\n        result = {\n            \"covariance_matrix\": cov_mat,\n            \"eigenvalues\": eig_vals_sorted,\n            \"eigenvectors\": eig_vecs_sorted,\n            \"explained_variance_ratio\": var_exp,\n            \"cumulative_explained_variance_ratio\": cum_var_exp,\n            \"projection_matrix\": matrix_w,\n            \"transformed\": X_pca,\n        }\n\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"pca\"] = (\n                f\"PCA kept {n} components explaining \"\n                f\"{cum_var_exp[n-1]:.2f}% variance.\"\n            )\n        if mcp:\n            return (\n                f\"PCA kept {n} components explaining {cum_var_exp[n-1]:.2f}% variance.\"\n            )\n        return result\n\n    def get_regression(self, y: str, mcp=False):\n        \"\"\"\n        Perform linear or logistic regression based on the outcome variable type.\n\n        If the outcome is binary, fit a logistic regression model.\n        Otherwise, fit a linear regression model.\n\n        Args:\n            y (str): Target column name for the regression.\n\n        Returns:\n            dict: Regression results including coefficients, intercept, and metrics.\n        \"\"\"\n        if ML_INSTALLED is False:\n            logger.info(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            return None\n\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling get_regression.\"\n            )\n\n        X_np, Y_raw, X, Y = self._process_xy(y=y)\n\n        # Check if outcome is binary (logistic) or continuous (linear)\n        unique_values = np.unique(Y_raw)\n        num_unique = len(unique_values)\n\n        # Determine if binary classification or regression\n        is_binary = num_unique == 2\n\n        if is_binary:\n            # Logistic Regression\n            print(f\"\\n=== Logistic Regression for {y} ===\")\n            print(f\"Binary outcome detected with values: {unique_values}\")\n\n            model = LogisticRegression(max_iter=1000, random_state=42)\n            model.fit(X_np, Y_raw)\n\n            # Predictions\n            y_pred = model.predict(X_np)\n\n            # Accuracy\n            accuracy = accuracy_score(Y_raw, y_pred)\n            print(f\"\\nAccuracy: {accuracy*100:.2f}%\")\n\n            # Coefficients and Intercept\n            print(f\"\\nCoefficients:\")\n            for i, coef in enumerate(model.coef_[0]):\n                feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n                print(f\"  {feature_name}: {coef:.5f}\")\n\n            print(f\"\\nIntercept: {model.intercept_[0]:.5f}\")\n\n            coef_str = \"\\n\".join(\n                [\n                    f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                    for i, coef in enumerate(model.coef_[0])\n                ]\n            )\n\n            # Store in metadata\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"logistic_regression_accuracy\"] = (\n                    f\"Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\"\n                )\n                self._csv.corpus.metadata[\"logistic_regression_coefficients\"] = (\n                    f\"Coefficients:\\n{coef_str}\"\n                )\n                self._csv.corpus.metadata[\"logistic_regression_intercept\"] = (\n                    f\"Intercept: {model.intercept_[0]:.5f}\"\n                )\n\n            if mcp:\n                return f\"\"\"\n                Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\n                Coefficients:\n                {coef_str}\n                Intercept: {model.intercept_[0]:.5f}\n                \"\"\"\n            return {\n                \"model_type\": \"logistic\",\n                \"accuracy\": accuracy,\n                \"coefficients\": model.coef_[0],\n                \"intercept\": model.intercept_[0],\n                \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n            }\n        else:\n            # Linear Regression\n            print(f\"\\n=== Linear Regression for {y} ===\")\n            print(f\"Continuous outcome detected with {num_unique} unique values\")\n\n            model = LinearRegression()\n            model.fit(X_np, Y_raw)\n\n            # Predictions\n            y_pred = model.predict(X_np)\n\n            # Metrics\n            mse = mean_squared_error(Y_raw, y_pred)\n            r2 = r2_score(Y_raw, y_pred)\n            print(f\"\\nMean Squared Error (MSE): {mse:.5f}\")\n            print(f\"R\u00b2 Score: {r2:.5f}\")\n\n            # Coefficients and Intercept\n            print(f\"\\nCoefficients:\")\n            for i, coef in enumerate(model.coef_):\n                feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n                print(f\"  {feature_name}: {coef:.5f}\")\n\n            print(f\"\\nIntercept: {model.intercept_:.5f}\")\n\n            coef_str = \"\\n\".join(\n                [\n                    f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                    for i, coef in enumerate(model.coef_)\n                ]\n            )\n\n            # Store in metadata\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"linear_regression_mse\"] = (\n                    f\"Linear Regression MSE for predicting {y}: {mse:.5f}\"\n                )\n                self._csv.corpus.metadata[\"linear_regression_r2\"] = (\n                    f\"Linear Regression R\u00b2 for predicting {y}: {r2:.5f}\"\n                )\n                self._csv.corpus.metadata[\"linear_regression_coefficients\"] = (\n                    f\"Coefficients:\\n{coef_str}\"\n                )\n                self._csv.corpus.metadata[\"linear_regression_intercept\"] = (\n                    f\"Intercept: {model.intercept_:.5f}\"\n                )\n\n            if mcp:\n                return f\"\"\"\n                Linear Regression MSE for predicting {y}: {mse:.5f}\n                R\u00b2: {r2:.5f}\n                Feature Names and Coefficients:\n                {coef_str}\n                Intercept: {model.intercept_:.5f}\n                \"\"\"\n            return {\n                \"model_type\": \"linear\",\n                \"mse\": mse,\n                \"r2\": r2,\n                \"coefficients\": model.coef_,\n                \"intercept\": model.intercept_,\n                \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n            }\n\n    def get_lstm_predictions(self, y: str, mcp=False):\n        \"\"\"\n        Train an LSTM model on text data to predict an outcome variable.\n        This tests if the texts converge towards predicting the outcome.\n\n        Args:\n            y (str): Name of the outcome variable in the DataFrame\n            mcp (bool): If True, return a string format suitable for MCP\n\n        Returns:\n            Evaluation metrics as string (if mcp=True) or dict\n        \"\"\"\n        if ML_INSTALLED is False:\n            logger.error(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            if mcp:\n                return \"ML dependencies are not installed. Please install with: pip install crisp-t[ml]\"\n            return None\n\n        if self._csv is None:\n            logger.error(\"CSV data is not set.\")\n            if mcp:\n                return \"CSV data is not set. Cannot perform LSTM prediction.\"\n            return None\n\n        _corpus = self._csv.corpus\n        if _corpus is None:\n            logger.error(\"Corpus is not available.\")\n            if mcp:\n                return \"Corpus is not available. Cannot perform LSTM prediction.\"\n            return None\n\n        # Check if id_column exists\n        id_column = \"id\"\n        if not hasattr(self._csv, \"df\") or self._csv.df is None:\n            logger.error(\"DataFrame is not available in CSV.\")\n            if mcp:\n                return \"This tool can be used only if texts and outcome variables align. DataFrame is missing.\"\n            return None\n\n        if id_column not in self._csv.df.columns:\n            logger.error(\n                f\"The id_column '{id_column}' does not exist in the DataFrame.\"\n            )\n            if mcp:\n                return f\"This tool can be used only if texts and outcome variables align. The '{id_column}' column is missing from the DataFrame.\"\n            return None\n\n        # Check if outcome variable exists\n        if y not in self._csv.df.columns:\n            logger.error(f\"The outcome variable '{y}' does not exist in the DataFrame.\")\n            if mcp:\n                return f\"The outcome variable '{y}' does not exist in the DataFrame.\"\n            return None\n\n        # Process documents and align with outcome variable\n        try:\n            # Build vocabulary from all documents\n            from collections import Counter\n\n            word_counts = Counter()\n            tokenized_docs = []\n\n            for doc in tqdm(_corpus.documents, desc=\"Tokenizing documents\", disable=len(_corpus.documents) &lt; 10):\n                # Simple tokenization - split on whitespace and lowercase\n                tokens = doc.text.lower().split()\n                tokenized_docs.append(tokens)\n                word_counts.update(tokens)\n\n            # Create vocabulary with most common words (limit to 10000)\n            vocab_size = min(10000, len(word_counts)) + 1  # +1 for padding\n            most_common = word_counts.most_common(vocab_size - 1)\n            word_to_idx = {\n                word: idx + 1 for idx, (word, _) in enumerate(most_common)\n            }  # 0 reserved for padding\n\n            # Convert documents to sequences of indices\n            max_length = 100  # Maximum sequence length\n            sequences = []\n            doc_ids = []\n\n            for doc, tokens in tqdm(zip(_corpus.documents, tokenized_docs), total=len(_corpus.documents), desc=\"Converting to sequences\", disable=len(_corpus.documents) &lt; 10):\n                # Convert tokens to indices\n                seq = [word_to_idx.get(token, 0) for token in tokens]\n                # Pad or truncate to max_length\n                if len(seq) &gt; max_length:\n                    seq = seq[:max_length]\n                else:\n                    seq = seq + [0] * (max_length - len(seq))\n                sequences.append(seq)\n                doc_ids.append(doc.id)\n\n            # Align with outcome variable using id column\n            df = self._csv.df.set_index(id_column)\n\n            aligned_sequences = []\n            aligned_outcomes = []\n\n            df_index_str = list(str(idx) for idx in df.index)\n            for doc_id, seq in zip(doc_ids, sequences):\n                if doc_id in df_index_str:\n                    aligned_sequences.append(seq)\n                    # Select y from df where id_column == doc_id, using string comparison\n                    matched_row = df.loc[\n                        [idx for idx in df.index if str(idx) == str(doc_id)]\n                    ]\n                    if not matched_row.empty:\n                        aligned_outcomes.append(matched_row.iloc[0][y])\n\n            if len(aligned_sequences) == 0:\n                logger.error(\"No documents could be aligned with the outcome variable.\")\n                if mcp:\n                    return \"This tool can be used only if texts and outcome variables align. No matching IDs found.\"\n                return None\n\n            # Convert to tensors\n            X_tensor = torch.LongTensor(aligned_sequences)  # type: ignore\n            y_array = np.array(aligned_outcomes)\n\n            # Handle binary classification\n            unique_values = np.unique(y_array)\n            num_classes = len(unique_values)\n\n            if num_classes &lt; 2:\n                logger.error(\n                    f\"Need at least 2 classes for classification, found {num_classes}\"\n                )\n                if mcp:\n                    return f\"Need at least 2 classes for classification, found {num_classes}\"\n                return None\n\n            # Map to 0/1 for binary classification\n            if num_classes == 2:\n                class_mapping = {unique_values[0]: 0.0, unique_values[1]: 1.0}\n                y_mapped = np.array(\n                    [class_mapping[val] for val in y_array], dtype=np.float32\n                )\n            else:\n                # Multi-class not supported in this simple LSTM implementation\n                logger.error(\n                    \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n                )\n                if mcp:\n                    return \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n                return None\n\n            y_tensor = torch.FloatTensor(y_mapped).view(-1, 1)  # type: ignore\n\n            # Split into train/test\n            from sklearn.model_selection import train_test_split\n\n            indices = list(range(len(X_tensor)))\n            train_idx, test_idx = train_test_split(\n                indices, test_size=0.2, random_state=42\n            )\n\n            X_train = X_tensor[train_idx]\n            y_train = y_tensor[train_idx]\n            X_test = X_tensor[test_idx]\n            y_test = y_tensor[test_idx]\n\n            # Create model\n            model = SimpleLSTM(vocab_size=vocab_size)  # type: ignore\n            criterion = nn.BCELoss()  # type: ignore\n            optimizer = optim.Adam(model.parameters(), lr=0.001)  # type: ignore\n\n            # Create data loaders\n            train_dataset = TensorDataset(X_train, y_train)  # type: ignore\n            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # type: ignore\n\n            # Training\n            epochs = max(self._epochs, 3)  # Use at least 3 epochs for LSTM\n            model.train()\n            for epoch in range(epochs):\n                total_loss = 0\n                for batch_x, batch_y in train_loader:\n                    optimizer.zero_grad()\n                    predictions = model(batch_x)\n                    loss = criterion(predictions, batch_y)\n                    loss.backward()\n                    optimizer.step()\n                    total_loss += loss.item()\n\n                avg_loss = total_loss / len(train_loader)\n                logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n            # Evaluation\n            model.eval()\n            with torch.no_grad():  # type: ignore\n                train_preds = model(X_train)\n                test_preds = model(X_test)\n\n                train_preds_binary = (train_preds &gt;= 0.5).float()\n                test_preds_binary = (test_preds &gt;= 0.5).float()\n\n                train_accuracy = (train_preds_binary == y_train).float().mean().item()\n                test_accuracy = (test_preds_binary == y_test).float().mean().item()\n\n            # Calculate additional metrics for test set\n            y_test_np = y_test.cpu().numpy().flatten()\n            test_preds_np = test_preds_binary.cpu().numpy().flatten()\n\n            # Confusion matrix elements\n            tp = ((test_preds_np == 1) &amp; (y_test_np == 1)).sum()\n            tn = ((test_preds_np == 0) &amp; (y_test_np == 0)).sum()\n            fp = ((test_preds_np == 1) &amp; (y_test_np == 0)).sum()\n            fn = ((test_preds_np == 0) &amp; (y_test_np == 1)).sum()\n\n            # Calculate precision, recall, F1\n            precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n            f1 = (\n                2 * (precision * recall) / (precision + recall)\n                if (precision + recall) &gt; 0\n                else 0\n            )\n\n            result_msg = (\n                f\"LSTM Model Evaluation for predicting '{y}':\\n\"\n                f\"  Vocabulary size: {vocab_size}\\n\"\n                f\"  Training samples: {len(X_train)}, Test samples: {len(X_test)}\\n\"\n                f\"  Epochs: {epochs}\\n\"\n                f\"  Train accuracy: {train_accuracy*100:.2f}%\\n\"\n                f\"  Test accuracy (convergence): {test_accuracy*100:.2f}%\\n\"\n                f\"  True Positive: {tp}, False Positive: {fp}, True Negative: {tn}, False Negative: {fn}\\n\"\n                f\"  Precision: {precision:.3f}\\n\"\n                f\"  Recall: {recall:.3f}\\n\"\n                f\"  F1-Score: {f1:.3f}\\n\"\n            )\n\n            print(f\"\\n{result_msg}\")\n\n            # Store in corpus metadata\n            if _corpus is not None:\n                _corpus.metadata[\"lstm_predictions\"] = result_msg\n\n            if mcp:\n                return result_msg\n\n            return {\n                \"vocab_size\": vocab_size,\n                \"train_samples\": len(X_train),\n                \"test_samples\": len(X_test),\n                \"epochs\": epochs,\n                \"train_accuracy\": train_accuracy,\n                \"test_accuracy\": test_accuracy,\n                \"true_positive\": tp,\n                \"false_positive\": fp,\n                \"true_negative\": tn,\n                \"false_negative\": fn,\n                \"precision\": precision,\n                \"recall\": recall,\n                \"f1_score\": f1,\n            }\n\n        except Exception as e:\n            logger.error(f\"Error in LSTM prediction: {e}\")\n            if mcp:\n                return f\"Error in LSTM prediction: {e}\"\n            return None\n</code></pre>"},{"location":"modules/#ml.ML.format_confusion_matrix_to_human_readable","title":"<code>format_confusion_matrix_to_human_readable(confusion_matrix)</code>","text":"<p>Format the confusion matrix to a human-readable string.</p> <p>Parameters:</p> Name Type Description Default <code>confusion_matrix</code> <code>ndarray</code> <p>The confusion matrix to format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The formatted confusion matrix with true positive, false positive, true negative, and false negative counts.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def format_confusion_matrix_to_human_readable(\n    self, confusion_matrix: np.ndarray\n) -&gt; str:\n    \"\"\"Format the confusion matrix to a human-readable string.\n\n    Args:\n        confusion_matrix (np.ndarray): The confusion matrix to format.\n\n    Returns:\n        str: The formatted confusion matrix with true positive, false positive, true negative, and false negative counts.\n    \"\"\"\n    tn, fp, fn, tp = confusion_matrix.ravel()\n    return (\n        f\"True Positive: {tp}\\n\"\n        f\"False Positive: {fp}\\n\"\n        f\"True Negative: {tn}\\n\"\n        f\"False Negative: {fn}\\n\"\n    )\n</code></pre>"},{"location":"modules/#ml.ML.get_lstm_predictions","title":"<code>get_lstm_predictions(y, mcp=False)</code>","text":"<p>Train an LSTM model on text data to predict an outcome variable. This tests if the texts converge towards predicting the outcome.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Name of the outcome variable in the DataFrame</p> required <code>mcp</code> <code>bool</code> <p>If True, return a string format suitable for MCP</p> <code>False</code> <p>Returns:</p> Type Description <p>Evaluation metrics as string (if mcp=True) or dict</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_lstm_predictions(self, y: str, mcp=False):\n    \"\"\"\n    Train an LSTM model on text data to predict an outcome variable.\n    This tests if the texts converge towards predicting the outcome.\n\n    Args:\n        y (str): Name of the outcome variable in the DataFrame\n        mcp (bool): If True, return a string format suitable for MCP\n\n    Returns:\n        Evaluation metrics as string (if mcp=True) or dict\n    \"\"\"\n    if ML_INSTALLED is False:\n        logger.error(\n            \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n        )\n        if mcp:\n            return \"ML dependencies are not installed. Please install with: pip install crisp-t[ml]\"\n        return None\n\n    if self._csv is None:\n        logger.error(\"CSV data is not set.\")\n        if mcp:\n            return \"CSV data is not set. Cannot perform LSTM prediction.\"\n        return None\n\n    _corpus = self._csv.corpus\n    if _corpus is None:\n        logger.error(\"Corpus is not available.\")\n        if mcp:\n            return \"Corpus is not available. Cannot perform LSTM prediction.\"\n        return None\n\n    # Check if id_column exists\n    id_column = \"id\"\n    if not hasattr(self._csv, \"df\") or self._csv.df is None:\n        logger.error(\"DataFrame is not available in CSV.\")\n        if mcp:\n            return \"This tool can be used only if texts and outcome variables align. DataFrame is missing.\"\n        return None\n\n    if id_column not in self._csv.df.columns:\n        logger.error(\n            f\"The id_column '{id_column}' does not exist in the DataFrame.\"\n        )\n        if mcp:\n            return f\"This tool can be used only if texts and outcome variables align. The '{id_column}' column is missing from the DataFrame.\"\n        return None\n\n    # Check if outcome variable exists\n    if y not in self._csv.df.columns:\n        logger.error(f\"The outcome variable '{y}' does not exist in the DataFrame.\")\n        if mcp:\n            return f\"The outcome variable '{y}' does not exist in the DataFrame.\"\n        return None\n\n    # Process documents and align with outcome variable\n    try:\n        # Build vocabulary from all documents\n        from collections import Counter\n\n        word_counts = Counter()\n        tokenized_docs = []\n\n        for doc in tqdm(_corpus.documents, desc=\"Tokenizing documents\", disable=len(_corpus.documents) &lt; 10):\n            # Simple tokenization - split on whitespace and lowercase\n            tokens = doc.text.lower().split()\n            tokenized_docs.append(tokens)\n            word_counts.update(tokens)\n\n        # Create vocabulary with most common words (limit to 10000)\n        vocab_size = min(10000, len(word_counts)) + 1  # +1 for padding\n        most_common = word_counts.most_common(vocab_size - 1)\n        word_to_idx = {\n            word: idx + 1 for idx, (word, _) in enumerate(most_common)\n        }  # 0 reserved for padding\n\n        # Convert documents to sequences of indices\n        max_length = 100  # Maximum sequence length\n        sequences = []\n        doc_ids = []\n\n        for doc, tokens in tqdm(zip(_corpus.documents, tokenized_docs), total=len(_corpus.documents), desc=\"Converting to sequences\", disable=len(_corpus.documents) &lt; 10):\n            # Convert tokens to indices\n            seq = [word_to_idx.get(token, 0) for token in tokens]\n            # Pad or truncate to max_length\n            if len(seq) &gt; max_length:\n                seq = seq[:max_length]\n            else:\n                seq = seq + [0] * (max_length - len(seq))\n            sequences.append(seq)\n            doc_ids.append(doc.id)\n\n        # Align with outcome variable using id column\n        df = self._csv.df.set_index(id_column)\n\n        aligned_sequences = []\n        aligned_outcomes = []\n\n        df_index_str = list(str(idx) for idx in df.index)\n        for doc_id, seq in zip(doc_ids, sequences):\n            if doc_id in df_index_str:\n                aligned_sequences.append(seq)\n                # Select y from df where id_column == doc_id, using string comparison\n                matched_row = df.loc[\n                    [idx for idx in df.index if str(idx) == str(doc_id)]\n                ]\n                if not matched_row.empty:\n                    aligned_outcomes.append(matched_row.iloc[0][y])\n\n        if len(aligned_sequences) == 0:\n            logger.error(\"No documents could be aligned with the outcome variable.\")\n            if mcp:\n                return \"This tool can be used only if texts and outcome variables align. No matching IDs found.\"\n            return None\n\n        # Convert to tensors\n        X_tensor = torch.LongTensor(aligned_sequences)  # type: ignore\n        y_array = np.array(aligned_outcomes)\n\n        # Handle binary classification\n        unique_values = np.unique(y_array)\n        num_classes = len(unique_values)\n\n        if num_classes &lt; 2:\n            logger.error(\n                f\"Need at least 2 classes for classification, found {num_classes}\"\n            )\n            if mcp:\n                return f\"Need at least 2 classes for classification, found {num_classes}\"\n            return None\n\n        # Map to 0/1 for binary classification\n        if num_classes == 2:\n            class_mapping = {unique_values[0]: 0.0, unique_values[1]: 1.0}\n            y_mapped = np.array(\n                [class_mapping[val] for val in y_array], dtype=np.float32\n            )\n        else:\n            # Multi-class not supported in this simple LSTM implementation\n            logger.error(\n                \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n            )\n            if mcp:\n                return \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n            return None\n\n        y_tensor = torch.FloatTensor(y_mapped).view(-1, 1)  # type: ignore\n\n        # Split into train/test\n        from sklearn.model_selection import train_test_split\n\n        indices = list(range(len(X_tensor)))\n        train_idx, test_idx = train_test_split(\n            indices, test_size=0.2, random_state=42\n        )\n\n        X_train = X_tensor[train_idx]\n        y_train = y_tensor[train_idx]\n        X_test = X_tensor[test_idx]\n        y_test = y_tensor[test_idx]\n\n        # Create model\n        model = SimpleLSTM(vocab_size=vocab_size)  # type: ignore\n        criterion = nn.BCELoss()  # type: ignore\n        optimizer = optim.Adam(model.parameters(), lr=0.001)  # type: ignore\n\n        # Create data loaders\n        train_dataset = TensorDataset(X_train, y_train)  # type: ignore\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # type: ignore\n\n        # Training\n        epochs = max(self._epochs, 3)  # Use at least 3 epochs for LSTM\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch_x, batch_y in train_loader:\n                optimizer.zero_grad()\n                predictions = model(batch_x)\n                loss = criterion(predictions, batch_y)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            avg_loss = total_loss / len(train_loader)\n            logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n        # Evaluation\n        model.eval()\n        with torch.no_grad():  # type: ignore\n            train_preds = model(X_train)\n            test_preds = model(X_test)\n\n            train_preds_binary = (train_preds &gt;= 0.5).float()\n            test_preds_binary = (test_preds &gt;= 0.5).float()\n\n            train_accuracy = (train_preds_binary == y_train).float().mean().item()\n            test_accuracy = (test_preds_binary == y_test).float().mean().item()\n\n        # Calculate additional metrics for test set\n        y_test_np = y_test.cpu().numpy().flatten()\n        test_preds_np = test_preds_binary.cpu().numpy().flatten()\n\n        # Confusion matrix elements\n        tp = ((test_preds_np == 1) &amp; (y_test_np == 1)).sum()\n        tn = ((test_preds_np == 0) &amp; (y_test_np == 0)).sum()\n        fp = ((test_preds_np == 1) &amp; (y_test_np == 0)).sum()\n        fn = ((test_preds_np == 0) &amp; (y_test_np == 1)).sum()\n\n        # Calculate precision, recall, F1\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n        f1 = (\n            2 * (precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0\n        )\n\n        result_msg = (\n            f\"LSTM Model Evaluation for predicting '{y}':\\n\"\n            f\"  Vocabulary size: {vocab_size}\\n\"\n            f\"  Training samples: {len(X_train)}, Test samples: {len(X_test)}\\n\"\n            f\"  Epochs: {epochs}\\n\"\n            f\"  Train accuracy: {train_accuracy*100:.2f}%\\n\"\n            f\"  Test accuracy (convergence): {test_accuracy*100:.2f}%\\n\"\n            f\"  True Positive: {tp}, False Positive: {fp}, True Negative: {tn}, False Negative: {fn}\\n\"\n            f\"  Precision: {precision:.3f}\\n\"\n            f\"  Recall: {recall:.3f}\\n\"\n            f\"  F1-Score: {f1:.3f}\\n\"\n        )\n\n        print(f\"\\n{result_msg}\")\n\n        # Store in corpus metadata\n        if _corpus is not None:\n            _corpus.metadata[\"lstm_predictions\"] = result_msg\n\n        if mcp:\n            return result_msg\n\n        return {\n            \"vocab_size\": vocab_size,\n            \"train_samples\": len(X_train),\n            \"test_samples\": len(X_test),\n            \"epochs\": epochs,\n            \"train_accuracy\": train_accuracy,\n            \"test_accuracy\": test_accuracy,\n            \"true_positive\": tp,\n            \"false_positive\": fp,\n            \"true_negative\": tn,\n            \"false_negative\": fn,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n        }\n\n    except Exception as e:\n        logger.error(f\"Error in LSTM prediction: {e}\")\n        if mcp:\n            return f\"Error in LSTM prediction: {e}\"\n        return None\n</code></pre>"},{"location":"modules/#ml.ML.get_nnet_predictions","title":"<code>get_nnet_predictions(y, mcp=False)</code>","text":"<p>Extended: Handles binary (BCELoss) and multi-class (CrossEntropyLoss). Returns list of predicted original class labels.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_nnet_predictions(self, y: str, mcp=False):\n    \"\"\"\n    Extended: Handles binary (BCELoss) and multi-class (CrossEntropyLoss).\n    Returns list of predicted original class labels.\n    \"\"\"\n    if ML_INSTALLED is False:\n        logger.info(\n            \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n        )\n        return None\n\n    if self._csv is None:\n        raise ValueError(\n            \"CSV data is not set. Please set self.csv before calling profile.\"\n        )\n    _corpus = self._csv.corpus\n\n    X_np, Y_raw, X, Y = self._process_xy(y=y)\n\n    unique_classes = np.unique(Y_raw)\n    num_classes = unique_classes.size\n    if num_classes &lt; 2:\n        raise ValueError(f\"Need at least 2 classes; found {num_classes}.\")\n\n    vnum = X_np.shape[1]\n\n    # Binary path\n    if num_classes == 2:\n        # Map to {0.0,1.0} for BCELoss if needed\n        mapping_applied = False\n        class_mapping = {}\n        inverse_mapping = {}\n        # Ensure deterministic order\n        sorted_classes = sorted(unique_classes.tolist())\n        if not (sorted_classes == [0, 1] or sorted_classes == [0.0, 1.0]):\n            class_mapping = {sorted_classes[0]: 0.0, sorted_classes[1]: 1.0}\n            inverse_mapping = {v: k for k, v in class_mapping.items()}\n            Y_mapped = np.vectorize(class_mapping.get)(Y_raw).astype(np.float32)\n            mapping_applied = True\n        else:\n            Y_mapped = Y_raw.astype(np.float32)\n\n        model = NeuralNet(vnum)\n        try:\n            criterion = nn.BCELoss()  # type: ignore\n            optimizer = optim.Adam(model.parameters(), lr=0.001)  # type: ignore\n\n            X_tensor = torch.from_numpy(X_np)  # type: ignore\n            y_tensor = torch.from_numpy(Y_mapped.astype(np.float32)).view(-1, 1)  # type: ignore\n\n            dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n            dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # type: ignore\n        except Exception as e:\n            logger.error(f\"Error occurred while creating DataLoader: {e}\")\n            return None\n\n        for _ in range(self._epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                if torch.isnan(loss):  # type: ignore\n                    raise RuntimeError(\"NaN loss encountered.\")\n                loss.backward()\n                optimizer.step()\n\n        # Predictions\n        bin_preds_internal = None\n        if torch:\n            with torch.no_grad():\n                probs = model(torch.from_numpy(X_np)).view(-1).cpu().numpy()\n            bin_preds_internal = (probs &gt;= 0.5).astype(int)\n\n        if mapping_applied:\n            preds = [inverse_mapping[float(p)] for p in bin_preds_internal]  # type: ignore\n            y_eval = np.vectorize(class_mapping.get)(Y_raw).astype(int)\n            preds_eval = bin_preds_internal\n        else:\n            preds = bin_preds_internal.tolist()  # type: ignore\n            y_eval = Y_mapped.astype(int)\n            preds_eval = bin_preds_internal\n\n        accuracy = (preds_eval == y_eval).sum() / len(y_eval)\n        print(\n            f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n        )\n        if _corpus is not None:\n            _corpus.metadata[\"nnet_predictions\"] = (\n                f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n            )\n        if mcp:\n            return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n        return preds\n\n    # Multi-class path\n    # Map original classes to indices\n    sorted_classes = sorted(unique_classes.tolist())\n    class_to_idx = {c: i for i, c in enumerate(sorted_classes)}\n    idx_to_class = {i: c for c, i in class_to_idx.items()}\n    Y_idx = np.vectorize(class_to_idx.get)(Y_raw).astype(np.int64)\n\n    model = MultiClassNet(vnum, num_classes)\n    criterion = nn.CrossEntropyLoss()  # type: ignore\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  # type: ignore\n\n    X_tensor = torch.from_numpy(X_np)  # type: ignore\n    y_tensor = torch.from_numpy(Y_idx)  # type: ignore\n\n    dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # type: ignore\n\n    for _ in range(self._epochs):\n        for batch_X, batch_y in dataloader:\n            optimizer.zero_grad()\n            logits = model(batch_X)\n            loss = criterion(logits, batch_y)\n            if torch.isnan(loss):  # type: ignore\n                raise RuntimeError(\"NaN loss encountered.\")\n            loss.backward()\n            optimizer.step()\n\n    with torch.no_grad():  # type: ignore\n        logits_full = model(torch.from_numpy(X_np))  # type: ignore\n        pred_indices = torch.argmax(logits_full, dim=1).cpu().numpy()  # type: ignore\n\n    preds = [idx_to_class[i] for i in pred_indices]\n    accuracy = (pred_indices == Y_idx).sum() / len(Y_idx)\n    print(\n        f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n    )\n    if _corpus is not None:\n        _corpus.metadata[\"nnet_predictions\"] = (\n            f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n        )\n    if mcp:\n        return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n    return preds\n</code></pre>"},{"location":"modules/#ml.ML.get_pca","title":"<code>get_pca(y, n=3, mcp=False)</code>","text":"<p>Perform a manual PCA (no sklearn PCA) on the feature matrix for target y.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name (used only for data preparation).</p> required <code>n</code> <code>int</code> <p>Number of principal components to keep.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>{ 'covariance_matrix': cov_mat, 'eigenvalues': eig_vals_sorted, 'eigenvectors': eig_vecs_sorted, 'explained_variance_ratio': var_exp, 'cumulative_explained_variance_ratio': cum_var_exp, 'projection_matrix': matrix_w, 'transformed': X_pca</p> <p>}</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_pca(self, y: str, n: int = 3, mcp=False):\n    \"\"\"\n    Perform a manual PCA (no sklearn PCA) on the feature matrix for target y.\n\n    Args:\n        y (str): Target column name (used only for data preparation).\n        n (int): Number of principal components to keep.\n\n    Returns:\n        dict: {\n            'covariance_matrix': cov_mat,\n            'eigenvalues': eig_vals_sorted,\n            'eigenvectors': eig_vecs_sorted,\n            'explained_variance_ratio': var_exp,\n            'cumulative_explained_variance_ratio': cum_var_exp,\n            'projection_matrix': matrix_w,\n            'transformed': X_pca\n        }\n    \"\"\"\n    X_np, Y_raw, X, Y = self._process_xy(y=y)\n    X_std = StandardScaler().fit_transform(X_np)\n\n    cov_mat = np.cov(X_std.T)\n    eig_vals, eig_vecs = np.linalg.eigh(cov_mat)  # symmetric matrix -&gt; eigh\n\n    # Sort eigenvalues (and vectors) descending\n    idx = np.argsort(eig_vals)[::-1]\n    eig_vals_sorted = eig_vals[idx]\n    eig_vecs_sorted = eig_vecs[:, idx]\n\n    factors = X_std.shape[1]\n    n = max(1, min(n, factors))\n\n    # Explained variance ratios\n    tot = eig_vals_sorted.sum()\n    var_exp = (eig_vals_sorted / tot) * 100.0\n    cum_var_exp = np.cumsum(var_exp)\n\n    # Projection matrix (first n eigenvectors)\n    matrix_w = eig_vecs_sorted[:, :n]\n\n    # Project data\n    X_pca = X_std @ matrix_w\n\n    # Optional prints (retain original behavior)\n    print(\"Covariance matrix:\\n\", cov_mat)\n    print(\"Eigenvalues (desc):\\n\", eig_vals_sorted)\n    print(\"Explained variance (%):\\n\", var_exp[:n])\n    print(\"Cumulative explained variance (%):\\n\", cum_var_exp[:n])\n    print(\"Projection matrix (W):\\n\", matrix_w)\n    print(\"Transformed (first 5 rows):\\n\", X_pca[:5])\n\n    result = {\n        \"covariance_matrix\": cov_mat,\n        \"eigenvalues\": eig_vals_sorted,\n        \"eigenvectors\": eig_vecs_sorted,\n        \"explained_variance_ratio\": var_exp,\n        \"cumulative_explained_variance_ratio\": cum_var_exp,\n        \"projection_matrix\": matrix_w,\n        \"transformed\": X_pca,\n    }\n\n    if self._csv.corpus is not None:\n        self._csv.corpus.metadata[\"pca\"] = (\n            f\"PCA kept {n} components explaining \"\n            f\"{cum_var_exp[n-1]:.2f}% variance.\"\n        )\n    if mcp:\n        return (\n            f\"PCA kept {n} components explaining {cum_var_exp[n-1]:.2f}% variance.\"\n        )\n    return result\n</code></pre>"},{"location":"modules/#ml.ML.get_regression","title":"<code>get_regression(y, mcp=False)</code>","text":"<p>Perform linear or logistic regression based on the outcome variable type.</p> <p>If the outcome is binary, fit a logistic regression model. Otherwise, fit a linear regression model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name for the regression.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Regression results including coefficients, intercept, and metrics.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_regression(self, y: str, mcp=False):\n    \"\"\"\n    Perform linear or logistic regression based on the outcome variable type.\n\n    If the outcome is binary, fit a logistic regression model.\n    Otherwise, fit a linear regression model.\n\n    Args:\n        y (str): Target column name for the regression.\n\n    Returns:\n        dict: Regression results including coefficients, intercept, and metrics.\n    \"\"\"\n    if ML_INSTALLED is False:\n        logger.info(\n            \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n        )\n        return None\n\n    if self._csv is None:\n        raise ValueError(\n            \"CSV data is not set. Please set self.csv before calling get_regression.\"\n        )\n\n    X_np, Y_raw, X, Y = self._process_xy(y=y)\n\n    # Check if outcome is binary (logistic) or continuous (linear)\n    unique_values = np.unique(Y_raw)\n    num_unique = len(unique_values)\n\n    # Determine if binary classification or regression\n    is_binary = num_unique == 2\n\n    if is_binary:\n        # Logistic Regression\n        print(f\"\\n=== Logistic Regression for {y} ===\")\n        print(f\"Binary outcome detected with values: {unique_values}\")\n\n        model = LogisticRegression(max_iter=1000, random_state=42)\n        model.fit(X_np, Y_raw)\n\n        # Predictions\n        y_pred = model.predict(X_np)\n\n        # Accuracy\n        accuracy = accuracy_score(Y_raw, y_pred)\n        print(f\"\\nAccuracy: {accuracy*100:.2f}%\")\n\n        # Coefficients and Intercept\n        print(f\"\\nCoefficients:\")\n        for i, coef in enumerate(model.coef_[0]):\n            feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n            print(f\"  {feature_name}: {coef:.5f}\")\n\n        print(f\"\\nIntercept: {model.intercept_[0]:.5f}\")\n\n        coef_str = \"\\n\".join(\n            [\n                f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                for i, coef in enumerate(model.coef_[0])\n            ]\n        )\n\n        # Store in metadata\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"logistic_regression_accuracy\"] = (\n                f\"Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\"\n            )\n            self._csv.corpus.metadata[\"logistic_regression_coefficients\"] = (\n                f\"Coefficients:\\n{coef_str}\"\n            )\n            self._csv.corpus.metadata[\"logistic_regression_intercept\"] = (\n                f\"Intercept: {model.intercept_[0]:.5f}\"\n            )\n\n        if mcp:\n            return f\"\"\"\n            Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\n            Coefficients:\n            {coef_str}\n            Intercept: {model.intercept_[0]:.5f}\n            \"\"\"\n        return {\n            \"model_type\": \"logistic\",\n            \"accuracy\": accuracy,\n            \"coefficients\": model.coef_[0],\n            \"intercept\": model.intercept_[0],\n            \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n        }\n    else:\n        # Linear Regression\n        print(f\"\\n=== Linear Regression for {y} ===\")\n        print(f\"Continuous outcome detected with {num_unique} unique values\")\n\n        model = LinearRegression()\n        model.fit(X_np, Y_raw)\n\n        # Predictions\n        y_pred = model.predict(X_np)\n\n        # Metrics\n        mse = mean_squared_error(Y_raw, y_pred)\n        r2 = r2_score(Y_raw, y_pred)\n        print(f\"\\nMean Squared Error (MSE): {mse:.5f}\")\n        print(f\"R\u00b2 Score: {r2:.5f}\")\n\n        # Coefficients and Intercept\n        print(f\"\\nCoefficients:\")\n        for i, coef in enumerate(model.coef_):\n            feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n            print(f\"  {feature_name}: {coef:.5f}\")\n\n        print(f\"\\nIntercept: {model.intercept_:.5f}\")\n\n        coef_str = \"\\n\".join(\n            [\n                f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                for i, coef in enumerate(model.coef_)\n            ]\n        )\n\n        # Store in metadata\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"linear_regression_mse\"] = (\n                f\"Linear Regression MSE for predicting {y}: {mse:.5f}\"\n            )\n            self._csv.corpus.metadata[\"linear_regression_r2\"] = (\n                f\"Linear Regression R\u00b2 for predicting {y}: {r2:.5f}\"\n            )\n            self._csv.corpus.metadata[\"linear_regression_coefficients\"] = (\n                f\"Coefficients:\\n{coef_str}\"\n            )\n            self._csv.corpus.metadata[\"linear_regression_intercept\"] = (\n                f\"Intercept: {model.intercept_:.5f}\"\n            )\n\n        if mcp:\n            return f\"\"\"\n            Linear Regression MSE for predicting {y}: {mse:.5f}\n            R\u00b2: {r2:.5f}\n            Feature Names and Coefficients:\n            {coef_str}\n            Intercept: {model.intercept_:.5f}\n            \"\"\"\n        return {\n            \"model_type\": \"linear\",\n            \"mse\": mse,\n            \"r2\": r2,\n            \"coefficients\": model.coef_,\n            \"intercept\": model.intercept_,\n            \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n        }\n</code></pre>"},{"location":"modules/#ml.ML.svm_confusion_matrix","title":"<code>svm_confusion_matrix(y, test_size=0.25, random_state=0, mcp=False)</code>","text":"<p>Generate confusion matrix for SVM</p> <p>Returns:</p> Type Description <p>[list] -- [description]</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def svm_confusion_matrix(self, y: str, test_size=0.25, random_state=0, mcp=False):\n    \"\"\"Generate confusion matrix for SVM\n\n    Returns:\n        [list] -- [description]\n    \"\"\"\n    X_np, Y_raw, X, Y = self._process_xy(y=y)\n    Y = self._convert_to_binary(Y)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, Y, test_size=test_size, random_state=random_state\n    )\n    sc = StandardScaler()\n    # Issue #22\n    y_test = y_test.astype(\"int\")\n    y_train = y_train.astype(\"int\")\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    classifier = SVC(kernel=\"linear\", random_state=0)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    # Issue #22\n    y_pred = y_pred.astype(\"int\")\n    _confusion_matrix = confusion_matrix(y_test, y_pred)\n    print(f\"Confusion Matrix for SVM predicting {y}:\\n{_confusion_matrix}\")\n    # Output\n    # [[2 0]\n    #  [2 0]]\n    if self._csv.corpus is not None:\n        self._csv.corpus.metadata[\"svm_confusion_matrix\"] = (\n            f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n        )\n\n    if mcp:\n        return f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n\n    return _confusion_matrix\n</code></pre>"},{"location":"modules/#network.Network","title":"<code>Network</code>","text":"<p>A class to represent a network of documents and their relationships.</p> Source code in <code>src/crisp_t/network.py</code> <pre><code>class Network:\n    \"\"\"\n    A class to represent a network of documents and their relationships.\n    \"\"\"\n\n    def __init__(self, corpus: Corpus):\n        \"\"\"\n        Initialize the Network with a corpus.\n\n        :param corpus: Corpus object containing documents to be included in the network.\n        \"\"\"\n        self._corpus = corpus\n        self._cluster = Cluster(corpus)\n        self._processed_docs = self._cluster.processed_docs\n        self._graph = None\n\n    def cooccurence_network(self, window_size=2):\n        self._graph = network.build_cooccurrence_network(\n            self._processed_docs, window_size=window_size\n        )\n        return self._graph\n\n    def similarity_network(self, method=\"levenshtein\"):\n        text = Text(self._corpus)\n        docs = text.make_spacy_doc()\n        data = [sent.text.lower() for sent in docs.sents]\n        self._graph = network.build_similarity_network(data, method)\n        return self._graph\n\n    def graph_as_dict(self):\n        \"\"\"\n        Convert the graph to a dictionary representation.\n\n        :return: Dictionary representation of the graph.\n        \"\"\"\n        if self._graph is None:\n            raise ValueError(\n                \"Graph has not been created yet. Call cooccurence_network() first.\"\n            )\n        return sorted(self._graph.adjacency())[0]\n</code></pre>"},{"location":"modules/#network.Network.__init__","title":"<code>__init__(corpus)</code>","text":"<p>Initialize the Network with a corpus.</p> <p>:param corpus: Corpus object containing documents to be included in the network.</p> Source code in <code>src/crisp_t/network.py</code> <pre><code>def __init__(self, corpus: Corpus):\n    \"\"\"\n    Initialize the Network with a corpus.\n\n    :param corpus: Corpus object containing documents to be included in the network.\n    \"\"\"\n    self._corpus = corpus\n    self._cluster = Cluster(corpus)\n    self._processed_docs = self._cluster.processed_docs\n    self._graph = None\n</code></pre>"},{"location":"modules/#network.Network.graph_as_dict","title":"<code>graph_as_dict()</code>","text":"<p>Convert the graph to a dictionary representation.</p> <p>:return: Dictionary representation of the graph.</p> Source code in <code>src/crisp_t/network.py</code> <pre><code>def graph_as_dict(self):\n    \"\"\"\n    Convert the graph to a dictionary representation.\n\n    :return: Dictionary representation of the graph.\n    \"\"\"\n    if self._graph is None:\n        raise ValueError(\n            \"Graph has not been created yet. Call cooccurence_network() first.\"\n        )\n    return sorted(self._graph.adjacency())[0]\n</code></pre>"},{"location":"modules/#visualize.QRVisualize","title":"<code>QRVisualize</code>","text":"Source code in <code>src/crisp_t/visualize.py</code> <pre><code>class QRVisualize:\n\n    def __init__(\n        self, corpus: Corpus | None = None, folder_path: str | None = None\n    ) -&gt; None:\n        # Matplotlib figure components assigned lazily by plotting methods\n        self.corpus = corpus\n        self.folder_path = folder_path\n        self.fig: Figure | None = None\n        self.ax: Axes | None = None\n        self.sc: PathCollection | None = None\n        self.annot: Annotation | None = None\n        self.names: list[str] = []\n        self.c: np.ndarray | None = None\n\n    def _ensure_columns(\n        self, df: pd.DataFrame, required: Iterable[str]\n    ) -&gt; pd.DataFrame:\n        \"\"\"Ensure that the DataFrame has the required columns.\n\n        Behavior:\n        - If all required columns already exist, return df unchanged.\n        - If the DataFrame has exactly the same number of columns as required,\n          rename columns positionally to match the required names.\n        - Otherwise, raise a ValueError listing the missing columns.\n        \"\"\"\n        required = list(required)\n        # Fast path: all required columns present\n        missing = [col for col in required if col not in df.columns]\n        if not missing:\n            return df\n\n        # If shape matches, attempt a positional rename\n        if len(df.columns) == len(required):\n            df = df.copy()\n            df.columns = required\n            return df\n\n        # Otherwise, cannot satisfy required columns\n        raise ValueError(f\"Missing required columns: {missing}\")\n\n    def _finalize_plot(\n        self,\n        fig: Figure,\n        folder_path: str | None,\n        show: bool,\n    ) -&gt; Figure:\n        if not folder_path:\n            folder_path = self.folder_path\n        if folder_path:\n            output_path = Path(folder_path)\n            if output_path.parent:\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n            fig.savefig(folder_path)\n        if show:\n            plt.show(block=False)\n        else:\n            plt.close(fig)\n        return fig\n\n    def plot_frequency_distribution_of_words(\n        self,\n        df: pd.DataFrame | None = None,\n        folder_path: str | None = None,\n        text_column: str = \"Text\",\n        bins: int = 100,\n        show: bool = True,\n    ) -&gt; Tuple[Figure, Axes]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\")\n        df = self._ensure_columns(df, [text_column])\n        doc_lens = df[text_column].dropna().map(len).tolist()\n        if not doc_lens:\n            raise ValueError(\"No documents available to plot frequency distribution.\")\n\n        fig, ax = plt.subplots(figsize=(16, 7), dpi=160)\n        counts, _, _ = ax.hist(doc_lens, bins=bins, color=\"navy\")\n        counts = np.asarray(counts)\n        if counts.size:\n            ax.set_ylim(top=float(counts.max()) * 1.1)\n\n        stats = {\n            \"Mean\": round(np.mean(doc_lens), 2),\n            \"Median\": round(np.median(doc_lens), 2),\n            \"Stdev\": round(np.std(doc_lens), 2),\n            \"1%ile\": round(np.quantile(doc_lens, q=0.01), 2),\n            \"99%ile\": round(np.quantile(doc_lens, q=0.99), 2),\n        }\n        for idx, (label, value) in enumerate(stats.items()):\n            ax.text(\n                0.98,\n                0.98 - idx * 0.05,\n                f\"{label}: {value}\",\n                transform=ax.transAxes,\n                ha=\"right\",\n                va=\"top\",\n                fontsize=11,\n            )\n\n        ax.set(\n            ylabel=\"Number of Documents\",\n            xlabel=\"Document Word Count\",\n            title=\"Distribution of Document Word Counts\",\n        )\n        ax.tick_params(axis=\"both\", labelsize=12)\n        if doc_lens:\n            ax.set_xlim(left=0, right=max(doc_lens) * 1.05)\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, ax\n\n    def plot_distribution_by_topic(\n        self,\n        df: pd.DataFrame | None = None,\n        folder_path: str | None = None,\n        topic_column: str = \"Dominant_Topic\",\n        text_column: str = \"Text\",\n        bins: int = 100,\n        show: bool = True,\n    ) -&gt; Tuple[Figure, np.ndarray]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\")\n        df = self._ensure_columns(df, [topic_column, text_column])\n        unique_topics = sorted(df[topic_column].dropna().unique())\n        if not unique_topics:\n            raise ValueError(\"No topics found to plot distribution.\")\n\n        n_topics = len(unique_topics)\n        n_cols = min(3, n_topics)\n        n_rows = math.ceil(n_topics / n_cols)\n        cols = list(mcolors.TABLEAU_COLORS.values())\n\n        fig, axes = plt.subplots(\n            n_rows,\n            n_cols,\n            figsize=(6 * n_cols, 5 * n_rows),\n            dpi=160,\n            sharex=True,\n            sharey=True,\n        )\n        if isinstance(axes, np.ndarray):\n            axes_flat = axes.flatten().tolist()\n        else:\n            axes_flat = [axes]\n\n        for idx, topic in enumerate(unique_topics):\n            ax = axes_flat[idx]\n            topic_series = cast(\n                pd.Series,\n                df.loc[df[topic_column] == topic, text_column],\n            )\n            topic_docs = topic_series.dropna()\n            doc_lens = topic_docs.map(len).tolist()\n            color = cols[idx % len(cols)]\n            if doc_lens:\n                ax.hist(doc_lens, bins=bins, color=color, alpha=0.7)\n                sns.kdeplot(\n                    doc_lens,\n                    color=\"black\",\n                    fill=False,\n                    ax=ax.twinx(),\n                    warn_singular=False,\n                )\n            ax.set(xlabel=\"Document Word Count\")\n            ax.set_ylabel(\"Number of Documents\", color=color)\n            ax.set_title(f\"Topic: {topic}\", fontdict=dict(size=14, color=color))\n            ax.tick_params(axis=\"y\", labelcolor=color, color=color)\n\n        for extra_ax in axes_flat[len(unique_topics) :]:\n            extra_ax.set_visible(False)\n\n        fig.tight_layout()\n        fig.suptitle(\n            \"Distribution of Document Word Counts by Dominant Topic\",\n            fontsize=20,\n            y=1.02,\n        )\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        axes_array = np.array(axes_flat, dtype=object).reshape(n_rows, n_cols)\n        return fig, axes_array\n\n    def plot_wordcloud(\n        self,\n        topics=None,\n        folder_path: str | None = None,\n        max_words: int = 50,\n        show: bool = True,\n    ) -&gt; Tuple[Figure, np.ndarray]:\n        if not topics:\n            try:\n                topics = self.corpus.visualization[\"word_cloud\"]\n            except Exception as e:\n                raise ValueError(f\"Failed to retrieve topics from corpus: {e}\")\n        n_topics = len(topics)\n        n_cols = min(3, n_topics)\n        n_rows = math.ceil(n_topics / n_cols)\n        cols = list(mcolors.TABLEAU_COLORS.values())\n\n        fig, axes = plt.subplots(\n            n_rows,\n            n_cols,\n            figsize=(6 * n_cols, 4 * n_rows),\n            sharex=True,\n            sharey=True,\n        )\n        axes_flat = axes.flatten().tolist() if isinstance(axes, np.ndarray) else [axes]\n\n        for idx, (topic_id, words) in enumerate(topics):\n            ax = axes_flat[idx]\n            topic_words = dict(words)\n            color = cols[idx % len(cols)]\n            cloud = WordCloud(\n                stopwords=STOPWORDS,\n                background_color=\"white\",\n                width=800,\n                height=400,\n                max_words=max_words,\n                colormap=\"tab10\",\n                color_func=lambda *args, color=color, **kwargs: color,\n                prefer_horizontal=0.9,\n            )\n            cloud.generate_from_frequencies(topic_words)\n            ax.imshow(cloud)\n            ax.set_title(f\"Topic {topic_id}\", fontdict=dict(size=14))\n            ax.axis(\"off\")\n\n        for extra_ax in axes_flat[len(topics) :]:\n            extra_ax.set_visible(False)\n\n        fig.tight_layout()\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, np.array(axes_flat).reshape(n_rows, n_cols)\n\n    def plot_top_terms(\n        self,\n        df: pd.DataFrame | None = None,\n        term_column: str = \"term\",\n        frequency_column: str = \"frequency\",\n        top_n: int = 20,\n        folder_path: str | None = None,\n        ascending: bool = False,\n        show: bool = True,\n    ) -&gt; Tuple[Figure, Axes]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\")\n        if top_n &lt;= 0:\n            raise ValueError(\"top_n must be greater than zero.\")\n\n        df = self._ensure_columns(df, [term_column, frequency_column])\n        subset = df[[term_column, frequency_column]].dropna()\n        if subset.empty:\n            raise ValueError(\"No data available to plot top terms.\")\n\n        subset = subset.sort_values(frequency_column, ascending=ascending).head(top_n)\n        subset = subset.iloc[::-1]\n\n        fig, ax = plt.subplots(figsize=(10, max(4, top_n * 0.4)))\n        ax.barh(subset[term_column], subset[frequency_column], color=\"steelblue\")\n        ax.set_xlabel(\"Frequency\")\n        ax.set_ylabel(\"Term\")\n        ax.set_title(\"Top Terms by Frequency\")\n        for idx, value in enumerate(subset[frequency_column]):\n            ax.text(value, idx, f\" {value}\", va=\"center\")\n        fig.tight_layout()\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, ax\n\n    def plot_correlation_heatmap(\n        self,\n        df: pd.DataFrame | None = None,\n        columns: Sequence[str] | None = None,\n        folder_path: str | None = None,\n        cmap: str = \"coolwarm\",\n        show: bool = True,\n    ) -&gt; Tuple[Figure, Axes]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\")\n        if columns:\n            df = self._ensure_columns(df, columns)\n            data = df[list(columns)]\n        else:\n            data = df\n        if data.empty:\n            raise ValueError(\"No data available to compute correlation heatmap.\")\n\n        numeric_data = data.select_dtypes(include=[np.number])\n        if numeric_data.shape[1] &lt; 2:\n            raise ValueError(\n                \"At least two numeric columns are required for correlation heatmap.\"\n            )\n\n        corr = numeric_data.corr()\n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.heatmap(corr, ax=ax, cmap=cmap, annot=True, fmt=\".2f\", square=True)\n        ax.set_title(\"Correlation Heatmap\")\n        fig.tight_layout()\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, ax\n\n    def plot_importance(\n        self,\n        topics: Sequence[Tuple[int, Sequence[Tuple[str, float]]]],\n        processed_docs: Sequence[Sequence[str]],\n        folder_path: str | None = None,\n        show: bool = True,\n    ) -&gt; Tuple[Figure, np.ndarray]:\n        if not topics:\n            raise ValueError(\"No topics provided to plot importance.\")\n        if not processed_docs:\n            raise ValueError(\"No processed documents provided to plot importance.\")\n\n        counter = Counter(word for doc in processed_docs for word in doc)\n        rows = []\n        for topic_id, words in topics:\n            for word, weight in words:\n                rows.append(\n                    {\n                        \"word\": word,\n                        \"topic_id\": topic_id,\n                        \"importance\": weight,\n                        \"word_count\": counter.get(word, 0),\n                    }\n                )\n\n        df = pd.DataFrame(rows)\n        if df.empty:\n            raise ValueError(\"Unable to build importance DataFrame from inputs.\")\n\n        topic_ids = sorted(df[\"topic_id\"].unique())\n        n_topics = len(topic_ids)\n        n_cols = min(3, n_topics)\n        n_rows = math.ceil(n_topics / n_cols)\n        cols = list(mcolors.TABLEAU_COLORS.values())\n\n        fig, axes = plt.subplots(\n            n_rows,\n            n_cols,\n            figsize=(7 * n_cols, 5 * n_rows),\n            sharey=False,\n            dpi=160,\n        )\n        axes_flat = axes.flatten().tolist() if isinstance(axes, np.ndarray) else [axes]\n\n        for idx, topic_id in enumerate(topic_ids):\n            ax = axes_flat[idx]\n            subset = df[df[\"topic_id\"] == topic_id]\n            color = cols[idx % len(cols)]\n            ax.bar(\n                subset[\"word\"],\n                subset[\"word_count\"],\n                color=color,\n                width=0.5,\n                alpha=0.4,\n                label=\"Word Count\",\n            )\n            ax_twin = ax.twinx()\n            ax_twin.plot(\n                subset[\"word\"],\n                subset[\"importance\"],\n                color=color,\n                marker=\"o\",\n                label=\"Importance\",\n            )\n            ax.set_title(f\"Topic {topic_id}\", color=color, fontsize=14)\n            ax.set_xlabel(\"Word\")\n            ax.set_ylabel(\"Word Count\", color=color)\n            ax.tick_params(axis=\"y\", labelcolor=color)\n            ax_twin.set_ylabel(\"Importance\", color=color)\n            ax_twin.tick_params(axis=\"y\", labelcolor=color)\n            ax.set_xticklabels(subset[\"word\"], rotation=30, ha=\"right\")\n            ax.legend(loc=\"upper left\")\n            ax_twin.legend(loc=\"upper right\")\n\n        for extra_ax in axes_flat[len(topic_ids) :]:\n            extra_ax.set_visible(False)\n\n        fig.tight_layout()\n        fig.suptitle(\n            \"Word Count and Importance of Topic Keywords\",\n            fontsize=20,\n            y=1.02,\n        )\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, np.array(axes_flat).reshape(n_rows, n_cols)\n\n    def sentence_chart(self, lda_model, text, start=0, end=13, folder_path=None):\n        if lda_model is None:\n            raise ValueError(\"LDA model is not provided.\")\n        corp = text[start:end]\n        mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n        fig, axes = plt.subplots(\n            end - start, 1, figsize=(20, (end - start) * 0.95), dpi=160\n        )\n        axes[0].axis(\"off\")\n        for i, ax in enumerate(axes):\n            try:\n                if i &gt; 0:\n                    corp_cur = corp[i - 1]\n                    topic_percs, wordid_topics, _ = lda_model[corp_cur]\n                    word_dominanttopic = [\n                        (lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics\n                    ]\n                    ax.text(\n                        0.01,\n                        0.5,\n                        \"Doc \" + str(i - 1) + \": \",\n                        verticalalignment=\"center\",\n                        fontsize=16,\n                        color=\"black\",\n                        transform=ax.transAxes,\n                        fontweight=700,\n                    )\n\n                    # Draw Rectange\n                    topic_percs_sorted = sorted(\n                        topic_percs, key=lambda x: (x[1]), reverse=True\n                    )\n                    ax.add_patch(\n                        Rectangle(\n                            (0.0, 0.05),\n                            0.99,\n                            0.90,\n                            fill=None,\n                            alpha=1,\n                            color=mycolors[topic_percs_sorted[0][0]],\n                            linewidth=2,\n                        )\n                    )\n\n                    word_pos = 0.06\n                    for j, (word, topics) in enumerate(word_dominanttopic):\n                        if j &lt; 14:\n                            ax.text(\n                                word_pos,\n                                0.5,\n                                word,\n                                horizontalalignment=\"left\",\n                                verticalalignment=\"center\",\n                                fontsize=16,\n                                color=mycolors[topics],\n                                transform=ax.transAxes,\n                                fontweight=700,\n                            )\n                            word_pos += 0.009 * len(\n                                word\n                            )  # to move the word for the next iter\n                            ax.axis(\"off\")\n                    ax.text(\n                        word_pos,\n                        0.5,\n                        \". . .\",\n                        horizontalalignment=\"left\",\n                        verticalalignment=\"center\",\n                        fontsize=16,\n                        color=\"black\",\n                        transform=ax.transAxes,\n                    )\n            except Exception as e:\n                logger.error(f\"Error occurred while processing document {i - 1}: {e}\")\n                continue\n\n        plt.subplots_adjust(wspace=0, hspace=0)\n        plt.suptitle(\n            \"Sentence Topic Coloring for Documents: \"\n            + str(start)\n            + \" to \"\n            + str(end - 2),\n            fontsize=22,\n            y=0.95,\n            fontweight=700,\n        )\n        plt.tight_layout()\n        plt.show(block=False)\n        # save\n        if folder_path:\n            plt.savefig(folder_path)\n            plt.close()\n\n    def _cluster_chart(self, lda_model, text, n_topics=3, folder_path=None):\n        # Get topic weights\n        topic_weights = []\n        for i, row_list in enumerate(lda_model[text]):\n            topic_weights.append([w for i, w in row_list[0]])\n\n        # Array of topic weights\n        arr = pd.DataFrame(topic_weights).fillna(0).values\n\n        # Keep the well separated points (optional)\n        arr = arr[np.amax(arr, axis=1) &gt; 0.35]\n\n        # Dominant topic number in each doc\n        topic_num = np.argmax(arr, axis=1)\n\n        # tSNE Dimension Reduction\n        tsne_model = TSNE(\n            n_components=2, verbose=1, random_state=0, angle=0.99, init=\"pca\"\n        )\n        tsne_lda = tsne_model.fit_transform(arr)\n\n        # Plot\n        plt.figure(figsize=(16, 10), dpi=160)\n        for i in range(n_topics):\n            plt.scatter(\n                tsne_lda[topic_num == i, 0],\n                tsne_lda[topic_num == i, 1],\n                label=str(i),\n                alpha=0.5,\n            )\n        plt.title(\"t-SNE Clustering of Topics\", fontsize=22)\n        plt.xlabel(\"t-SNE Dimension 1\", fontsize=16)\n        plt.ylabel(\"t-SNE Dimension 2\", fontsize=16)\n        plt.legend(title=\"Topic Number\", loc=\"upper right\")\n        plt.show(block=False)\n        # save\n        if folder_path:\n            plt.savefig(folder_path)\n            plt.close()\n\n    def most_discussed_topics(\n        self, lda_model, dominant_topics, topic_percentages, folder_path=None\n    ):\n\n        # Distribution of Dominant Topics in Each Document\n        df = pd.DataFrame(dominant_topics, columns=[\"Document_Id\", \"Dominant_Topic\"])\n        dominant_topic_in_each_doc = df.groupby(\"Dominant_Topic\").size()\n        df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(\n            name=\"count\"\n        ).reset_index()\n\n        # Total Topic Distribution by actual weight\n        topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n        df_topic_weightage_by_doc = (\n            topic_weightage_by_doc.sum().to_frame(name=\"count\").reset_index()\n        )\n\n        # Top 3 Keywords for each Topic\n        topic_top3words = [\n            (i, topic)\n            for i, topics in lda_model.show_topics(formatted=False)\n            for j, (topic, wt) in enumerate(topics)\n            if j &lt; 3\n        ]\n\n        df_top3words_stacked = pd.DataFrame(\n            topic_top3words, columns=[\"topic_id\", \"words\"]\n        )\n        df_top3words = df_top3words_stacked.groupby(\"topic_id\").agg(\", \\n\".join)\n        df_top3words.reset_index(level=0, inplace=True)\n\n        # Plot\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n\n        # Topic Distribution by Dominant Topics\n        ax1.bar(\n            x=\"Dominant_Topic\",\n            height=\"count\",\n            data=df_dominant_topic_in_each_doc,\n            width=0.5,\n            color=\"firebrick\",\n        )\n        ax1.set_xticks(\n            range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__())\n        )\n        tick_formatter = FuncFormatter(\n            lambda x, pos: \"Topic \"\n            + str(x)\n            + \"\\n\"\n            + df_top3words.loc[df_top3words.topic_id == x, \"words\"].values[0]  # type: ignore\n        )\n        ax1.xaxis.set_major_formatter(tick_formatter)\n        ax1.set_title(\"Number of Documents by Dominant Topic\", fontdict=dict(size=10))\n        ax1.set_ylabel(\"Number of Documents\")\n        ax1.set_ylim(0, 1000)\n\n        # Topic Distribution by Topic Weights\n        ax2.bar(\n            x=\"index\",\n            height=\"count\",\n            data=df_topic_weightage_by_doc,\n            width=0.5,\n            color=\"steelblue\",\n        )\n        ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n        ax2.xaxis.set_major_formatter(tick_formatter)\n        ax2.set_title(\"Number of Documents by Topic Weightage\", fontdict=dict(size=10))\n\n        plt.show(block=False)\n\n        # save\n        if folder_path:\n            plt.savefig(folder_path)\n            plt.close()\n\n    def update_annot(self, ind):\n        if self.annot is None or self.sc is None or self.c is None:\n            raise RuntimeError(\"cluster_chart must be called before update_annot.\")\n        indices_array = np.atleast_1d(ind.get(\"ind\", []))\n        if indices_array.size == 0:\n            return\n        indices = indices_array.astype(int)\n        idx = int(indices[0])\n        offsets = np.asarray(self.sc.get_offsets())\n        pos = offsets[idx]\n        annot = self.annot\n        annot.xy = (float(pos[0]), float(pos[1]))\n        text = \"{}, {}\".format(\n            \" \".join(list(map(str, indices))),\n            \" \".join([self.names[n] for n in indices]),\n        )\n        annot.set_text(text)\n        cmap = plt.get_cmap(\"RdYlGn\")\n        norm = mcolors.Normalize(1, 4)\n        bbox = annot.get_bbox_patch()\n        if bbox is not None:\n            try:\n                color_value = float(self.c[idx])\n            except (TypeError, ValueError):\n                color_value = 1.0\n            bbox.set_facecolor(cmap(norm(color_value)))\n            bbox.set_alpha(0.4)\n\n    def hover(self, event):\n        if self.annot is None or self.sc is None or self.fig is None or self.ax is None:\n            return\n        vis = self.annot.get_visible()\n        if event.inaxes == self.ax:\n            cont, ind = self.sc.contains(event)\n            if cont:\n                self.update_annot(ind)\n                self.annot.set_visible(True)\n                self.fig.canvas.draw_idle()\n            elif vis:\n                self.annot.set_visible(False)\n                self.fig.canvas.draw_idle()\n\n    # https://stackoverflow.com/questions/7908636/how-to-add-hovering-annotations-to-a-plot\n    def cluster_chart(self, data, folder_path=None):\n        # Scatter plot for Text Cluster Prediction\n        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n        self.names = list(map(str, data[\"title\"]))\n        self.sc = plt.scatter(\n            data[\"x\"],\n            data[\"y\"],\n            c=data[\"colour\"],\n            s=36,\n            edgecolors=\"black\",\n            linewidths=0.75,\n        )\n        self.c = np.asarray(data[\"colour\"])\n        self.annot = self.ax.annotate(\n            \"\",\n            xy=(0, 0),\n            xytext=(20, 20),\n            textcoords=\"offset points\",\n            bbox=dict(boxstyle=\"round\", fc=\"w\"),\n            arrowprops=dict(arrowstyle=\"-&gt;\"),\n        )\n        self.annot.set_visible(False)\n        plt.title(\"Text Cluster Prediction\")\n        plt.axis(\"off\")  # Optional: Remove axes for a cleaner look\n        plt.colorbar(self.sc, label=\"Colour\")  # Add colorbar if needed\n        self.fig.canvas.mpl_connect(\"motion_notify_event\", self.hover)\n        plt.show(block=False)\n        # save\n        if folder_path:\n            # annotate with data['title']\n            for i, txt in enumerate(data[\"title\"]):\n                plt.annotate(\n                    txt,\n                    (data[\"x\"][i], data[\"y\"][i]),\n                    fontsize=8,\n                    ha=\"right\",\n                    va=\"bottom\",\n                )\n            plt.savefig(folder_path)\n            plt.close()\n\n    def get_lda_viz(\n        self,\n        lda_model,\n        corpus_bow,\n        dictionary,\n        folder_path: str | None = None,\n        mds: str = \"tsne\",\n        lambda_val: float = 0.6,\n        show: bool = True,\n    ) -&gt; str | None:\n        \"\"\"\n        Generate an interactive LDA visualization using pyLDAvis.\n\n        Args:\n            lda_model: The trained LDA model\n            corpus_bow: Bag of words corpus\n            dictionary: Gensim dictionary\n            folder_path: Path to save the HTML visualization\n            mds: Dimension reduction method ('tsne', 'mmds', or 'pcoa')\n            lambda_val: Lambda parameter for relevance metric (default: 0.6).\n                       Mettler et al. (2025) performed several experiments to identify\n                       the optimal value of \u03bb, which turned out to be 0.6.\n            show: Whether to display the visualization\n\n        Returns:\n            HTML string of the visualization if successful, None otherwise\n\n        Raises:\n            ImportError: If pyLDAvis is not installed\n            ValueError: If required inputs are missing\n        \"\"\"\n        if not PYLDAVIS_AVAILABLE:\n            raise ImportError(\n                \"pyLDAvis is not installed. Install it with: pip install pyLDAvis\"\n            )\n\n        if lda_model is None:\n            raise ValueError(\"LDA model is required\")\n        if corpus_bow is None:\n            raise ValueError(\"Corpus bag of words is required\")\n        if dictionary is None:\n            raise ValueError(\"Dictionary is required\")\n\n        try:\n            # Prepare the visualization data\n            vis_data = gensimvis.prepare(\n                lda_model,\n                corpus_bow,\n                dictionary,\n                mds=mds,\n                R=30,\n                lambda_step=0.01,\n                plot_opts={\"xlab\": \"PC1\", \"ylab\": \"PC2\"},\n            )\n\n            # Save to HTML file if path provided\n            if folder_path:\n                output_path = Path(folder_path)\n                if output_path.parent:\n                    output_path.parent.mkdir(parents=True, exist_ok=True)\n                pyLDAvis.save_html(vis_data, str(output_path))\n                logger.info(f\"LDA visualization saved to {output_path}\")\n\n            # Return HTML string for embedding or further use\n            html_string = pyLDAvis.prepared_data_to_html(vis_data)\n            return html_string\n\n        except Exception as e:\n            logger.error(f\"Error generating LDA visualization: {e}\")\n            raise\n\n    def draw_tdabm(\n        self,\n        corpus: Corpus | None = None,\n        folder_path: str | None = None,\n        show: bool = True,\n    ) -&gt; Figure:\n        \"\"\"\n        Draw TDABM (Topological Data Analysis Ball Mapper) visualization.\n\n        Creates a 2D graph showing landmark points as circles:\n        - Circle size is proportional to the count of points in the ball\n        - Circle color represents mean y value (red for low, purple for high)\n        - Lines connect landmark points with non-empty intersections\n\n        Based on the algorithm by Rudkin and Dlotko (2024).\n\n        Args:\n            corpus: Corpus with 'tdabm' metadata. If None, uses self.corpus\n            folder_path: Path to save the figure. If None, uses self.folder_path\n            show: Whether to display the plot\n\n        Returns:\n            Matplotlib Figure object\n        \"\"\"\n        if corpus is None:\n            corpus = self.corpus\n\n        if corpus is None:\n            raise ValueError(\"No corpus provided\")\n\n        if \"tdabm\" not in corpus.metadata:\n            raise ValueError(\n                \"Corpus metadata does not contain 'tdabm' data. Run TDABM analysis first.\"\n            )\n\n        tdabm_data = corpus.metadata[\"tdabm\"]\n        landmarks = tdabm_data[\"landmarks\"]\n\n        if not landmarks:\n            raise ValueError(\"No landmarks found in TDABM data\")\n\n        # Create figure\n        fig, ax = plt.subplots(figsize=(12, 10))\n\n        # Collect all landmark locations\n        locations = [landmark[\"location\"] for landmark in landmarks]\n        counts = [landmark[\"count\"] for landmark in landmarks]\n        mean_ys = [landmark[\"mean_y\"] for landmark in landmarks]\n        landmark_ids = [landmark[\"id\"] for landmark in landmarks]\n\n        # Perform PCA to reduce to 2 components (PC1, PC2)\n        from sklearn.decomposition import PCA\n\n        locations_array = np.array(locations)\n        if locations_array.shape[1] &lt; 2:\n            # If only 1D, pad with zeros\n            locations_array = np.pad(locations_array, ((0, 0), (0, 1)), mode=\"constant\")\n        pca = PCA(n_components=2)\n        positions = pca.fit_transform(locations_array)\n\n        # Normalize mean_y for color mapping (red=0, purple=max)\n        min_y = min(mean_ys)\n        max_y = max(mean_ys)\n\n        if max_y - min_y &gt; 0:\n            normalized_ys = [(y - min_y) / (max_y - min_y) for y in mean_ys]\n        else:\n            normalized_ys = [0.5] * len(mean_ys)\n\n        # Create color map: red (0) to green (1)\n        colors = []\n        for norm_y in normalized_ys:\n            # Interpolate from red (1,0,0) to green (0,1,0)\n            r = 1.0 - norm_y\n            g = norm_y\n            b = 0.0\n            colors.append((r, g, b))\n\n        # Draw connections first (so they appear behind circles)\n        landmark_dict = {lm[\"id\"]: idx for idx, lm in enumerate(landmarks)}\n\n        for i, landmark in enumerate(landmarks):\n            for connected_id in landmark[\"connections\"]:\n                if connected_id in landmark_dict:\n                    j = landmark_dict[connected_id]\n                    # Only draw each connection once (avoid duplicates)\n                    if i &lt; j:\n                        ax.plot(\n                            [positions[i, 0], positions[j, 0]],\n                            [positions[i, 1], positions[j, 1]],\n                            \"k-\",\n                            alpha=0.3,\n                            linewidth=1,\n                            zorder=1,\n                        )\n\n        # Normalize counts for circle sizes (scale for visibility)\n        max_count = max(counts)\n        min_count = min(counts)\n\n        if max_count &gt; min_count:\n            # Scale sizes between 100 and 2000\n            sizes = [\n                100 + 1900 * (c - min_count) / (max_count - min_count) for c in counts\n            ]\n        else:\n            sizes = [500] * len(counts)\n\n        # Draw circles for landmarks\n        scatter = ax.scatter(\n            positions[:, 0],\n            positions[:, 1],\n            s=sizes,\n            c=colors,\n            alpha=0.6,\n            edgecolors=\"black\",\n            linewidths=1.5,\n            zorder=2,\n        )\n\n        # Add count and mean_y as label inside each circle\n        for i, (pos, count, mean_y) in enumerate(zip(positions, counts, mean_ys)):\n            ax.annotate(\n                f\"{count}\\n{mean_y:.2f}\",\n                xy=pos,\n                xytext=(0, 0),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n                fontweight=\"bold\",\n                zorder=3,\n            )\n\n        # Set labels and title\n        x_vars = tdabm_data.get(\"x_variables\", [])\n        y_var = tdabm_data.get(\"y_variable\", \"y\")\n\n        # Axis labels reflect PCA components\n        ax.set_xlabel(\"PC1\", fontsize=12)\n        ax.set_ylabel(\"PC2\", fontsize=12)\n\n        ax.set_title(\n            f\"TDABM Visualization\\n\"\n            f'Y variable: {y_var}, Radius: {tdabm_data.get(\"radius\", 0.3)}\\n'\n            f\"Landmarks: {len(landmarks)}\",\n            fontsize=14,\n            fontweight=\"bold\",\n        )\n\n        # Add colorbar for mean_y (red to green)\n        sm = plt.cm.ScalarMappable(\n            cmap=mcolors.LinearSegmentedColormap.from_list(\n                \"red_green\", [\"red\", \"green\"]\n            ),\n            norm=plt.Normalize(vmin=min_y, vmax=max_y),\n        )\n        sm.set_array([])\n        cbar = plt.colorbar(sm, ax=ax)\n        cbar.set_label(f\"Mean {y_var}\", fontsize=12)\n\n        # Add legend for circle sizes\n        # Create dummy scatter plots for legend\n        legend_counts = [min_count, (min_count + max_count) / 2, max_count]\n        legend_sizes = []\n        for c in legend_counts:\n            if max_count &gt; min_count:\n                size = 100 + 1900 * (c - min_count) / (max_count - min_count)\n            else:\n                size = 500\n            legend_sizes.append(size)\n\n        legend_elements = []\n        for size, count in zip(legend_sizes, legend_counts):\n            legend_elements.append(\n                plt.scatter(\n                    [],\n                    [],\n                    s=size,\n                    c=\"gray\",\n                    alpha=0.6,\n                    edgecolors=\"black\",\n                    linewidths=1.5,\n                    label=f\"{int(count)} points\",\n                )\n            )\n\n        ax.legend(\n            handles=legend_elements,\n            title=\"Ball Size\",\n            loc=\"upper right\",\n            framealpha=0.9,\n        )\n\n        ax.grid(True, alpha=0.3)\n        ax.set_aspect(\"equal\", adjustable=\"box\")\n\n        plt.tight_layout()\n\n        return self._finalize_plot(fig, folder_path, show)\n</code></pre>"},{"location":"modules/#visualize.QRVisualize.draw_tdabm","title":"<code>draw_tdabm(corpus=None, folder_path=None, show=True)</code>","text":"<p>Draw TDABM (Topological Data Analysis Ball Mapper) visualization.</p> <p>Creates a 2D graph showing landmark points as circles: - Circle size is proportional to the count of points in the ball - Circle color represents mean y value (red for low, purple for high) - Lines connect landmark points with non-empty intersections</p> <p>Based on the algorithm by Rudkin and Dlotko (2024).</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>Corpus | None</code> <p>Corpus with 'tdabm' metadata. If None, uses self.corpus</p> <code>None</code> <code>folder_path</code> <code>str | None</code> <p>Path to save the figure. If None, uses self.folder_path</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib Figure object</p> Source code in <code>src/crisp_t/visualize.py</code> <pre><code>def draw_tdabm(\n    self,\n    corpus: Corpus | None = None,\n    folder_path: str | None = None,\n    show: bool = True,\n) -&gt; Figure:\n    \"\"\"\n    Draw TDABM (Topological Data Analysis Ball Mapper) visualization.\n\n    Creates a 2D graph showing landmark points as circles:\n    - Circle size is proportional to the count of points in the ball\n    - Circle color represents mean y value (red for low, purple for high)\n    - Lines connect landmark points with non-empty intersections\n\n    Based on the algorithm by Rudkin and Dlotko (2024).\n\n    Args:\n        corpus: Corpus with 'tdabm' metadata. If None, uses self.corpus\n        folder_path: Path to save the figure. If None, uses self.folder_path\n        show: Whether to display the plot\n\n    Returns:\n        Matplotlib Figure object\n    \"\"\"\n    if corpus is None:\n        corpus = self.corpus\n\n    if corpus is None:\n        raise ValueError(\"No corpus provided\")\n\n    if \"tdabm\" not in corpus.metadata:\n        raise ValueError(\n            \"Corpus metadata does not contain 'tdabm' data. Run TDABM analysis first.\"\n        )\n\n    tdabm_data = corpus.metadata[\"tdabm\"]\n    landmarks = tdabm_data[\"landmarks\"]\n\n    if not landmarks:\n        raise ValueError(\"No landmarks found in TDABM data\")\n\n    # Create figure\n    fig, ax = plt.subplots(figsize=(12, 10))\n\n    # Collect all landmark locations\n    locations = [landmark[\"location\"] for landmark in landmarks]\n    counts = [landmark[\"count\"] for landmark in landmarks]\n    mean_ys = [landmark[\"mean_y\"] for landmark in landmarks]\n    landmark_ids = [landmark[\"id\"] for landmark in landmarks]\n\n    # Perform PCA to reduce to 2 components (PC1, PC2)\n    from sklearn.decomposition import PCA\n\n    locations_array = np.array(locations)\n    if locations_array.shape[1] &lt; 2:\n        # If only 1D, pad with zeros\n        locations_array = np.pad(locations_array, ((0, 0), (0, 1)), mode=\"constant\")\n    pca = PCA(n_components=2)\n    positions = pca.fit_transform(locations_array)\n\n    # Normalize mean_y for color mapping (red=0, purple=max)\n    min_y = min(mean_ys)\n    max_y = max(mean_ys)\n\n    if max_y - min_y &gt; 0:\n        normalized_ys = [(y - min_y) / (max_y - min_y) for y in mean_ys]\n    else:\n        normalized_ys = [0.5] * len(mean_ys)\n\n    # Create color map: red (0) to green (1)\n    colors = []\n    for norm_y in normalized_ys:\n        # Interpolate from red (1,0,0) to green (0,1,0)\n        r = 1.0 - norm_y\n        g = norm_y\n        b = 0.0\n        colors.append((r, g, b))\n\n    # Draw connections first (so they appear behind circles)\n    landmark_dict = {lm[\"id\"]: idx for idx, lm in enumerate(landmarks)}\n\n    for i, landmark in enumerate(landmarks):\n        for connected_id in landmark[\"connections\"]:\n            if connected_id in landmark_dict:\n                j = landmark_dict[connected_id]\n                # Only draw each connection once (avoid duplicates)\n                if i &lt; j:\n                    ax.plot(\n                        [positions[i, 0], positions[j, 0]],\n                        [positions[i, 1], positions[j, 1]],\n                        \"k-\",\n                        alpha=0.3,\n                        linewidth=1,\n                        zorder=1,\n                    )\n\n    # Normalize counts for circle sizes (scale for visibility)\n    max_count = max(counts)\n    min_count = min(counts)\n\n    if max_count &gt; min_count:\n        # Scale sizes between 100 and 2000\n        sizes = [\n            100 + 1900 * (c - min_count) / (max_count - min_count) for c in counts\n        ]\n    else:\n        sizes = [500] * len(counts)\n\n    # Draw circles for landmarks\n    scatter = ax.scatter(\n        positions[:, 0],\n        positions[:, 1],\n        s=sizes,\n        c=colors,\n        alpha=0.6,\n        edgecolors=\"black\",\n        linewidths=1.5,\n        zorder=2,\n    )\n\n    # Add count and mean_y as label inside each circle\n    for i, (pos, count, mean_y) in enumerate(zip(positions, counts, mean_ys)):\n        ax.annotate(\n            f\"{count}\\n{mean_y:.2f}\",\n            xy=pos,\n            xytext=(0, 0),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"center\",\n            fontsize=8,\n            fontweight=\"bold\",\n            zorder=3,\n        )\n\n    # Set labels and title\n    x_vars = tdabm_data.get(\"x_variables\", [])\n    y_var = tdabm_data.get(\"y_variable\", \"y\")\n\n    # Axis labels reflect PCA components\n    ax.set_xlabel(\"PC1\", fontsize=12)\n    ax.set_ylabel(\"PC2\", fontsize=12)\n\n    ax.set_title(\n        f\"TDABM Visualization\\n\"\n        f'Y variable: {y_var}, Radius: {tdabm_data.get(\"radius\", 0.3)}\\n'\n        f\"Landmarks: {len(landmarks)}\",\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\n    # Add colorbar for mean_y (red to green)\n    sm = plt.cm.ScalarMappable(\n        cmap=mcolors.LinearSegmentedColormap.from_list(\n            \"red_green\", [\"red\", \"green\"]\n        ),\n        norm=plt.Normalize(vmin=min_y, vmax=max_y),\n    )\n    sm.set_array([])\n    cbar = plt.colorbar(sm, ax=ax)\n    cbar.set_label(f\"Mean {y_var}\", fontsize=12)\n\n    # Add legend for circle sizes\n    # Create dummy scatter plots for legend\n    legend_counts = [min_count, (min_count + max_count) / 2, max_count]\n    legend_sizes = []\n    for c in legend_counts:\n        if max_count &gt; min_count:\n            size = 100 + 1900 * (c - min_count) / (max_count - min_count)\n        else:\n            size = 500\n        legend_sizes.append(size)\n\n    legend_elements = []\n    for size, count in zip(legend_sizes, legend_counts):\n        legend_elements.append(\n            plt.scatter(\n                [],\n                [],\n                s=size,\n                c=\"gray\",\n                alpha=0.6,\n                edgecolors=\"black\",\n                linewidths=1.5,\n                label=f\"{int(count)} points\",\n            )\n        )\n\n    ax.legend(\n        handles=legend_elements,\n        title=\"Ball Size\",\n        loc=\"upper right\",\n        framealpha=0.9,\n    )\n\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n\n    plt.tight_layout()\n\n    return self._finalize_plot(fig, folder_path, show)\n</code></pre>"},{"location":"modules/#visualize.QRVisualize.get_lda_viz","title":"<code>get_lda_viz(lda_model, corpus_bow, dictionary, folder_path=None, mds='tsne', lambda_val=0.6, show=True)</code>","text":"<p>Generate an interactive LDA visualization using pyLDAvis.</p> <p>Parameters:</p> Name Type Description Default <code>lda_model</code> <p>The trained LDA model</p> required <code>corpus_bow</code> <p>Bag of words corpus</p> required <code>dictionary</code> <p>Gensim dictionary</p> required <code>folder_path</code> <code>str | None</code> <p>Path to save the HTML visualization</p> <code>None</code> <code>mds</code> <code>str</code> <p>Dimension reduction method ('tsne', 'mmds', or 'pcoa')</p> <code>'tsne'</code> <code>lambda_val</code> <code>float</code> <p>Lambda parameter for relevance metric (default: 0.6).        Mettler et al. (2025) performed several experiments to identify        the optimal value of \u03bb, which turned out to be 0.6.</p> <code>0.6</code> <code>show</code> <code>bool</code> <p>Whether to display the visualization</p> <code>True</code> <p>Returns:</p> Type Description <code>str | None</code> <p>HTML string of the visualization if successful, None otherwise</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pyLDAvis is not installed</p> <code>ValueError</code> <p>If required inputs are missing</p> Source code in <code>src/crisp_t/visualize.py</code> <pre><code>def get_lda_viz(\n    self,\n    lda_model,\n    corpus_bow,\n    dictionary,\n    folder_path: str | None = None,\n    mds: str = \"tsne\",\n    lambda_val: float = 0.6,\n    show: bool = True,\n) -&gt; str | None:\n    \"\"\"\n    Generate an interactive LDA visualization using pyLDAvis.\n\n    Args:\n        lda_model: The trained LDA model\n        corpus_bow: Bag of words corpus\n        dictionary: Gensim dictionary\n        folder_path: Path to save the HTML visualization\n        mds: Dimension reduction method ('tsne', 'mmds', or 'pcoa')\n        lambda_val: Lambda parameter for relevance metric (default: 0.6).\n                   Mettler et al. (2025) performed several experiments to identify\n                   the optimal value of \u03bb, which turned out to be 0.6.\n        show: Whether to display the visualization\n\n    Returns:\n        HTML string of the visualization if successful, None otherwise\n\n    Raises:\n        ImportError: If pyLDAvis is not installed\n        ValueError: If required inputs are missing\n    \"\"\"\n    if not PYLDAVIS_AVAILABLE:\n        raise ImportError(\n            \"pyLDAvis is not installed. Install it with: pip install pyLDAvis\"\n        )\n\n    if lda_model is None:\n        raise ValueError(\"LDA model is required\")\n    if corpus_bow is None:\n        raise ValueError(\"Corpus bag of words is required\")\n    if dictionary is None:\n        raise ValueError(\"Dictionary is required\")\n\n    try:\n        # Prepare the visualization data\n        vis_data = gensimvis.prepare(\n            lda_model,\n            corpus_bow,\n            dictionary,\n            mds=mds,\n            R=30,\n            lambda_step=0.01,\n            plot_opts={\"xlab\": \"PC1\", \"ylab\": \"PC2\"},\n        )\n\n        # Save to HTML file if path provided\n        if folder_path:\n            output_path = Path(folder_path)\n            if output_path.parent:\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n            pyLDAvis.save_html(vis_data, str(output_path))\n            logger.info(f\"LDA visualization saved to {output_path}\")\n\n        # Return HTML string for embedding or further use\n        html_string = pyLDAvis.prepared_data_to_html(vis_data)\n        return html_string\n\n    except Exception as e:\n        logger.error(f\"Error generating LDA visualization: {e}\")\n        raise\n</code></pre>"}]}