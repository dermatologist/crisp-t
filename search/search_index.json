{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udd0d CRISP-T (Sense-making from Text and Numbers!)","text":"<p>\u2728 Have you been co-creating reality with adaptive AI vibes?</p> <p>TL;DR CRISP-T is a qualitative research method and a toolkit to perform mixed data (text + numeric) analytics for computational triangulation and sense-making. More importantly, CRISP brings \"vibe analytics\" \"SKILLS\" for mixed data with an AI agent, adopting an adaptive epistomology.</p> <p>\ud83d\udc49 CLI Cheatsheet | Demo &amp; Examples | Documentation</p> <p> </p> <p>Give us a star \u2b50\ufe0f if you find this useful!</p>"},{"location":"#introduction","title":"\ud83d\udcd6 Introduction","text":"<p>Qualitative research focuses on collecting and analyzing textual data to explore complex phenomena. While qualitative and quantitative data are often used together, there is no standard method for combining them.</p> <p>CRISP-T integrates textual data (interview transcripts, field notes) and numeric data (surveys, demographics) into a unified corpus. It allows researchers to: - Perform INDUCTIVE analysis (finding patterns). - Triangulate findings by linking text topics to numeric trends. - Use Semantic Search to find relevant literature or code documents. - Leverage GenAI through our MCP server for agentic analysis.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\u2705 No Python required - Full CLI support.</li> <li>\u2705 Interpretivist approach - Designed for sense-making, not just data science.</li> <li>\u2705 GenAI Ready - Augments LLMs with agentic tools.</li> <li>\u2705 Open Source - GPL-3.0 License.</li> </ul>"},{"location":"#the-crisp-tools-may-also-be-useful-for","title":"The CRISP tools may also be useful for:","text":"<ul> <li>\u2b55 Automated interview coding dictionary generation.</li> <li>\u2b55 Semantically filtering journal articles for literature review.</li> <li>\u2b55 Generating visualizations for qualitative (e.g. word cloud) and quantitative (e.g. TDABM) data.</li> <li>\u2b55 Many other \u2026.</li> </ul>"},{"location":"#crisp-is-not-for","title":"CRISP is \u274c NOT for:","text":"<ul> <li>\u274c  Multimodal prediction. Use this instead!</li> <li>\u274c  Sequential mixed methods research, where qualitative and quantitative data are collected and analyzed in separate phases.</li> <li>\u274c  Convergent parallel mixed method designs where qualitative and quantitative data are collected simultaneously but analyzed separately.</li> </ul>"},{"location":"#installation","title":"\ud83d\udcbb Installation","text":"<pre><code>pip install crisp-t\n</code></pre> <p>Recommended (for Machine Learning features):</p> <pre><code>pip install crisp-t[ml]\n</code></pre> <p>Optional (for XGBoost):</p> <pre><code>pip install crisp-t[xg]\n# Mac users: brew install libomp\n</code></pre>"},{"location":"#usage","title":"\ud83d\ude80 Usage","text":"<p>CRISP-T provides three main CLI tools. See the CLI Cheatsheet for a full list of commands.</p>"},{"location":"#1-crisp-analysis","title":"1. <code>crisp</code> (Analysis)","text":"<p>The main tool for data import and analysis.</p> <ul> <li>Import Data: <code>crisp --source ./data --out ./corpus</code></li> <li>Analyze Text: <code>crisp --inp ./corpus --topics --sentiment</code></li> <li>Analyze Numbers: <code>crisp --inp ./corpus --kmeans --regression</code></li> </ul>"},{"location":"#2-crispviz-visualization","title":"2. <code>crispviz</code> (Visualization)","text":"<p>Generates charts and graphs from your analysis.</p> <ul> <li>Word Cloud: <code>crispviz --inp ./corpus --wordcloud --out ./viz</code></li> <li>Interactive Topics: <code>crispviz --inp ./corpus --ldavis --out ./viz</code></li> </ul>"},{"location":"#3-crispt-corpus-toolkit","title":"3. <code>crispt</code> (Corpus Toolkit)","text":"<p>Manages corpus structure and detailed queries.</p> <ul> <li>Semantic Search: <code>crispt --inp ./corpus --semantic \"query\" --num 5</code></li> <li>Metadata: <code>crispt --inp ./corpus --meta \"project=phase1\"</code></li> </ul> <p>\ud83d\udc49 View the Step-by-Step Demo</p>"},{"location":"#mcp-server-agentic-ai","title":"\ud83e\udd16 MCP Server (Agentic AI)","text":"<p>CRISP-T includes a Model Context Protocol (MCP) server, allowing AI agents (like Claude Desktop) to interact directly with your data.</p>"},{"location":"#configuration-claude-desktop","title":"Configuration (Claude Desktop)","text":"<p>MacOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code> Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></p> <pre><code>{\n  \"mcpServers\": {\n    \"crisp-t\": {\n      \"command\": \"&lt;python-path&gt;crisp-mcp\"\n    }\n  }\n}\n</code></pre> <p>The MCP server provides tools for semantic search, topic modeling, clustering, and more, allowing you to ask your AI assistant to \"analyze the trends in this corpus\" directly. Most of the CLI commands are available as MCP tools.</p>"},{"location":"#data-model","title":"Data model","text":""},{"location":"#documentation-references","title":"\ud83d\udcda Documentation &amp; References","text":"<ul> <li>CLI Cheatsheet</li> <li>Demo / Tutorial</li> <li>Full Documentation</li> <li>Mettler et al. (2025) - Methodological Reflection</li> </ul>"},{"location":"#contribution-contact","title":"\ud83e\udd1d Contribution &amp; Contact","text":"<ul> <li>License: GPL-3.0</li> <li>Author: Bell Eapen (UIS)| Contact</li> <li>Social: </li> </ul> <p>First presented at ICIS 2025.</p>"},{"location":"DEMO/","title":"Demo","text":""},{"location":"DEMO/#setup","title":"Setup","text":"<ul> <li>CRISP-T is a python package that can be installed using pip and used from the command line. Your system should have python 3.11 or higher and pip installed. You can download and install Python for your operating system here. Optionally, CRISP-T can be imported in python scripts or jupyter notebooks, but this is not covered in this demo. See documentation for more details.</li> <li>Install Crisp-T with <code>pip install crisp-t[ml]</code> or <code>uv pip install crisp-t[ml]</code></li> <li>(Optional) Download covid narratives data to  <code>crisp_source</code> folder in home directory or current directory using <code>crisp --covid covidstories.omeka.net --source crisp_source</code>. You may use any other source of textual data (e.g. journal articles, interview transcripts) in .txt or .pdf format in the <code>crisp_source</code> folder or the folder you specify with --source option.</li> <li>(Optional) Download Psycological Effects of COVID dataset to <code>crisp_source</code> folder. You may use any other numeric dataset in .csv format in the <code>crisp_source</code> folder or the folder you specify with --source option.</li> <li>Create a <code>crisp_input</code> folder in home directory or current directory for keeping imported data for analysis.</li> </ul>"},{"location":"DEMO/#import-data","title":"Import data","text":"<ul> <li>Run the following command to import data from <code>crisp_source</code> folder to <code>crisp_input</code> folder.</li> <li><code>--source</code> reads data from a directory (reads .txt, .pdf and a single .csv) or from a URL</li> </ul> <pre><code>crisp --source crisp_source --out crisp_input\n</code></pre> <ul> <li>Ignore warnings related to pdf files.</li> </ul>"},{"location":"DEMO/#perform-exploratory-tasks-using-nlp","title":"Perform Exploratory tasks using NLP","text":"<ul> <li>Run the following command to perform a topic modelling and assign topics(keywords) to each narrative.</li> <li>--inp crisp_input below is optional as it defaults to <code>crisp_input</code> folder.</li> </ul> <pre><code>crisp --inp crisp_input --assign --out crisp_input\n</code></pre> <ul> <li>The results will be saved in the same <code>crisp_input</code> folder, overwriting the corpus file.</li> <li>You may run several other analyses (see documentation for details) and tweak parameters as needed.</li> <li>Hints will be provided in the terminal.</li> </ul> <p>From now on, we will use <code>crisp_input</code> folder as input folder unless specified otherwise as that is the default.</p>"},{"location":"DEMO/#explore-results","title":"Explore results","text":"<pre><code>crisp --print \"documents 10\"\n</code></pre> <ul> <li>Notice that we have omitted --inp as it defaults to <code>crisp_input</code> folder. If you want to use a different folder, use --inp to specify it. The --out option helps to save intermediate results in a different folder.</li> <li> <p>The above command prints first 10 documents in the corpus.</p> </li> <li> <p>Next, let us see the metadata assigned to each document.</p> </li> </ul> <pre><code>crisp --print \"documents metadata\"\n</code></pre> <ul> <li>Notice keywords/topics assigned to each narrative.</li> <li>You will notice interviewee and interviewer keywords. These are assigned based on the presence of these words in the narratives and may not be useful.</li> <li>You may remove these keywords by using --ignore with assign and check the results again.</li> </ul> <pre><code>crisp --clear --assign --ignore interviewee,interviewer --out crisp_input\ncrisp --print \"documents metadata\"\n</code></pre> <ul> <li>--clear option clears the cache before running the analysis. \u26a0\ufe0f While analysing multiple datasets, use <code>crisp --clear</code> option to clear cache before switching datasets. \u26a0\ufe0f</li> <li>Now you will see that these keywords are removed from the results.</li> <li>It prints the first 5 documents by default.</li> </ul> <pre><code>crisp --print \"metadata clusters\"\n</code></pre> <ul> <li>Prints the clusters assigned to each document based on keywords.</li> <li>There are many other options to explore the results. See documentation for details.</li> <li>Let us choose narratives that contain 'mask' keyword and show the concepts/topics in these narratives.</li> </ul> <pre><code>crisp --inp crisp_input --clear --filters keywords=mask --topics\n</code></pre> <ul> <li>The above results will not be saved as --out is not specified.</li> <li>Notice time, people as topics in this subset of narratives.</li> <li>If --filters is used, only the filtered documents are used for the analysis. When using filters you should explicitly specify --inp and --out options with different folders to avoid overwriting the input data.</li> </ul>"},{"location":"DEMO/#quantitative-exploratory-analysis","title":"Quantitative exploratory analysis","text":"<ul> <li>Let us see do a kmeans clustering of the csv dataset of covid data.</li> </ul> <pre><code>crisp --include relaxed,self_time,sleep_bal,time_dp,travel_time,home_env --kmeans\n</code></pre> <ul> <li>Notice 3 clusters with different centroids. (number of clusters can be changed with --num option).</li> </ul>"},{"location":"DEMO/#confirmation","title":"Confirmation","text":"<ul> <li>Let us add a relationship between numb:self_time and text:work in the corpus for future confirmation with LLMs.</li> </ul> <pre><code>crispt --add-rel \"text:work|numb:self_time|correlates\"\n</code></pre> <ul> <li>Let us do a regression analysis to see how <code>relaxed</code> is affected by other variables.</li> </ul> <pre><code>crisp --include relaxed,self_time,sleep_bal,time_dp,travel_time,home_env --regression --outcome relaxed\n</code></pre> <ul> <li>self_time has a positive correlation with relaxed.</li> <li>What about a decision tree analysis?</li> </ul> <pre><code>crisp --include relaxed,self_time,sleep_bal,time_dp,travel_time,home_env --cls --outcome relaxed\n</code></pre> <ul> <li>Relaxed is converted to binary variable internally for classification.</li> <li>Ideally, you should do the binary conversion externally based on domain knowledge.</li> <li>Notice that self_time is the most important variable in predicting relaxed.</li> </ul>"},{"location":"DEMO/#topological-data-analysis-rudkin-s-dlotko-p-2024","title":"Topological Data Analysis Rudkin, S., &amp; Dlotko, P. (2024)","text":"<ul> <li>Let us do a TDA analysis to see the shape of the data.</li> <li>parameters to --tdabm are specified as follows: outcome:varables:radius</li> </ul> <pre><code>\ncrispt --tdabm relaxed:self_time,sleep_bal,time_dp,travel_time:0.6 --out crisp_input\n\n</code></pre> <ul> <li>Let us visualize the TDA network.</li> </ul> <pre><code>crispviz --tdabm --out viz_out/\n</code></pre>"},{"location":"DEMO/#sense-making-by-triangulation","title":"Sense-making by triangulation","text":""},{"location":"DEMO/#now-let-us-try-out-a-csv-dataset-with-text-and-numeric-data","title":"Now let us try out a csv dataset with text and numeric data.","text":"<ul> <li>Download SMS Smishing Collection Data Set from Kaggle and convert the text file to csv adding the headers id, CLASS and SMS. Convert CLASS to numeric 0 and 1 for ham and smish respectively and add id as serial numbers.</li> <li>Place the csv file in a new <code>crisp_source</code> folder.</li> <li>Import the csv file to <code>crisp_input</code> folder using the following command.</li> </ul> <pre><code>crisp --source crisp_source/ --unstructured SMS\n</code></pre> <ul> <li>Notice that the text column SMS is specified with --unstructured option. This creates CRISP documents from the text column.</li> <li>Now assign topics to the documents. Note that this also assigns clusters.</li> </ul> <pre><code>crisp --assign\n</code></pre> <ul> <li>Now print the results to examine.</li> </ul> <pre><code>crisp --print \"metadata clusters\"\n</code></pre> <ul> <li>Let us choose the cluster 1 and see the SMS classes in this cluster. (0=ham, 1=smish)</li> </ul> <pre><code>crisp --filters cluster=1 --print \"dataframe stats\"\n</code></pre> <ul> <li>Next, let us check if the SMS texts converge towards predicting the CLASS (ham/ smish) variable with LSTM model.</li> </ul> <pre><code>crisp --lstm --outcome CLASS\n</code></pre>"},{"location":"DEMO/#mcp-server-for-agentic-ai-optional-but-llms-may-be-better-at-sense-making","title":"MCP Server for agentic AI. (Optional, but LLMs may be better at sense-making!)","text":""},{"location":"DEMO/#try-out-the-mcp-server-with-the-following-command-llms-will-offer-course-corrections-and-suggestions","title":"Try out the MCP server with the following command. (LLMs will offer course corrections and suggestions)","text":"<ul> <li>load corpus from /Users/your-user-id/crisp_input</li> <li>use available tools</li> <li>What are the columns in df?</li> <li>Do a regression using time_bp,time_dp,travel_time,self_time with relaxed as outcome</li> <li>Interpret the results</li> <li>Is self_time or related concepts occur frequently in documents?</li> <li>can you ignore \"interviewer,interviewee\" and assign topics again? Yes.</li> <li>What are the topics in documents with keyword \"work\"?</li> </ul>"},{"location":"DEMO/#visualization","title":"Visualization","text":""},{"location":"DEMO/#lets-visualize-the-clusters-in-2d-space-using-pca","title":"Let's visualize the clusters in 2D space using PCA.","text":"<pre><code>crispviz --ldavis --out viz_out/\n</code></pre> <ul> <li>The visualization will be saved in <code>viz_out</code> folder. Open the html file in a browser to explore.</li> </ul>"},{"location":"DEMO/#lets-generate-a-word-cloud-of-keywords-in-the-corpus","title":"Let's generate a word cloud of keywords in the corpus.","text":"<pre><code>crispviz --wordcloud --out viz_out/\n</code></pre> <ul> <li>The word cloud will be saved in <code>viz_out</code> folder.</li> </ul>"},{"location":"DEMO/#more-examples-comprehensive-cli-usage","title":"More examples \u2014 comprehensive CLI usage","text":"<p>The following grouped examples show common and advanced usage patterns for the three CLIs: <code>crisp</code>, <code>crispt</code>, and <code>crispviz</code>. These are practical, copy-pasteable command lines that demonstrate option combinations and formats discussed in this demo and cheatsheet.</p>"},{"location":"DEMO/#a-data-import-basic-workflow-crisp","title":"A. Data import &amp; basic workflow (<code>crisp</code>)","text":""},{"location":"DEMO/#import-a-folder-with-text-files-and-a-csv-specify-unstructured-text-column","title":"Import a folder with text files and a CSV; specify unstructured text column","text":"<pre><code>crisp --source ./raw_data --unstructured \"comments\" --out ./crisp_input\n</code></pre>"},{"location":"DEMO/#import-but-limit-text-files-and-csv-rows-when-ingesting-large-sources","title":"Import but limit text files and CSV rows when ingesting large sources","text":"<pre><code>crisp --source ./raw_data --out ./crisp_input --num 10 --rec 500\n</code></pre>"},{"location":"DEMO/#import-csv-placed-in-the-source-folder-ignore-specific-stopwordscolumns","title":"Import CSV placed in the source folder; ignore specific stopwords/columns","text":"<pre><code>crisp --source ./survey --unstructured \"comments\" --ignore \"interviewer,interviewee\" --out ./survey_corpus\n</code></pre>"},{"location":"DEMO/#b-filtering-and-linking-crisp-crispt-examples","title":"B. Filtering and linking (<code>crisp</code> + <code>crispt</code>) \u2014 examples","text":""},{"location":"DEMO/#exact-match-filters-both-and-separators-supported","title":"Exact-match filters (both <code>=</code> and <code>:</code> separators supported)","text":"<pre><code>crisp --inp ./crisp_input --filters category=Health --topics\ncrisp --inp ./crisp_input --filters category:Health --topics\n</code></pre>"},{"location":"DEMO/#special-link-filters-textdf-and-dftext","title":"Special link filters (text\u2192df and df\u2192text)","text":"<pre><code># Filter dataframe rows that are linked from documents via embeddings\ncrispt --inp ./crisp_input --filters embedding:text --out ./linked_by_embedding\n\n# Filter documents that are linked from dataframe rows via temporal links\ncrispt --inp ./crisp_input --filters temporal:df --out ./linked_docs\n</code></pre>"},{"location":"DEMO/#legacy-shorthand-mappings-both-map-to-embeddingtext-or-temporaltext","title":"Legacy shorthand mappings \u2014 both map to <code>embedding:text</code> or <code>temporal:text</code>","text":"<pre><code>crispt --inp ./crisp_input --filters =embedding\ncrispt --inp ./crisp_input --filters :temporal\n</code></pre>"},{"location":"DEMO/#id-linkage-filter-to-a-single-id-or-sync-remaining-docsrows-with-blank-value","title":"ID linkage: filter to a single ID, or sync remaining docs\u2194rows with blank value","text":"<pre><code># Filter to specific id\ncrisp --inp ./crisp_input --filters id=12345 --nlp\n\n# Sync documents and dataframe rows by ID after other filters\ncrisp --inp ./crisp_input --filters id: --out ./synced_output\n</code></pre>"},{"location":"DEMO/#c-text-analysis-quick-examples-crisp","title":"C. Text analysis quick examples (<code>crisp</code>)","text":""},{"location":"DEMO/#topic-modeling-and-then-assign-topics-to-documents","title":"Topic modeling and then assign topics to documents","text":"<pre><code>crisp --inp ./crisp_input --topics --assign --out ./crisp_input_analyzed\n</code></pre>"},{"location":"DEMO/#run-sentiment-and-summary-together","title":"Run sentiment and summary together","text":"<pre><code>crisp --inp ./crisp_input --sentiment --summary --num 5\n</code></pre>"},{"location":"DEMO/#run-all-nlp-analyses-coding-dictionary-topics-categories-summary-sentiment","title":"Run all NLP analyses (coding dictionary, topics, categories, summary, sentiment)","text":"<pre><code>crisp --inp ./crisp_input --nlp\n</code></pre>"},{"location":"DEMO/#d-machine-learning-cross-modal-examples-crisp","title":"D. Machine learning &amp; cross-modal examples (<code>crisp</code>)","text":""},{"location":"DEMO/#run-k-means-clustering-on-numeric-csv-columns","title":"Run k-means clustering on numeric CSV columns","text":"<pre><code>crisp --inp ./survey_corpus --kmeans --num 4 --include age,income,score\n</code></pre>"},{"location":"DEMO/#classification-svm-decision-tree-using-a-dataframe-outcome-column","title":"Classification (SVM + Decision Tree) using a DataFrame outcome column","text":"<pre><code>crisp --inp ./survey_corpus --cls --outcome satisfaction_binary --include a,b,c --aggregation majority\n</code></pre>"},{"location":"DEMO/#neural-net-requires-crisp-tml","title":"Neural net (requires <code>crisp-t[ml]</code>)","text":"<pre><code>crisp --inp ./survey_corpus --nnet --outcome target_col --include feat1,feat2\n</code></pre>"},{"location":"DEMO/#lstm-using-text-documents-aligned-by-id-column-in-csv","title":"LSTM using text documents aligned by <code>id</code> column in CSV","text":"<pre><code>crisp --inp ./survey_corpus --lstm --outcome CLASS\n</code></pre>"},{"location":"DEMO/#e-corpus-management-inspection-crispt-examples","title":"E. Corpus management &amp; inspection (<code>crispt</code>) \u2014 examples","text":""},{"location":"DEMO/#create-a-new-corpus-and-add-documents","title":"Create a new corpus and add documents","text":"<pre><code>crispt --id my_corpus --name \"Study A\" --doc \"1|Intro|This is the first document\" --out ./my_corpus\n</code></pre>"},{"location":"DEMO/#add-metadata-and-a-relationship","title":"Add metadata and a relationship","text":"<pre><code>crispt --inp ./my_corpus --meta \"source=field\" --add-rel \"text:work|numb:self_time|correlates\" --out ./my_corpus\n</code></pre>"},{"location":"DEMO/#remove-a-document-and-clear-relationships","title":"Remove a document and clear relationships","text":"<pre><code>crispt --inp ./my_corpus --remove-doc 1 --clear-rel --out ./my_corpus\n</code></pre>"},{"location":"DEMO/#inspect-dataset-columns-row-counts-or-specific-rows","title":"Inspect dataset columns, row counts, or specific rows","text":"<pre><code>crispt --inp ./my_corpus --df-cols\ncrispt --inp ./my_corpus --df-row-count\ncrispt --inp ./my_corpus --df-row 12\n</code></pre>"},{"location":"DEMO/#print-usage-two-supported-formats","title":"Print usage: two supported formats","text":"<pre><code># Multi-flag form\ncrispt --inp ./my_corpus --print documents --print 10\n\n# Single-string form\ncrispt --inp ./my_corpus --print \"dataframe metadata\"\n</code></pre>"},{"location":"DEMO/#f-semantic-embedding-features-crispt-examples","title":"F. Semantic &amp; embedding features (<code>crispt</code>) \u2014 examples","text":""},{"location":"DEMO/#semantic-search-for-similar-documents-requires-embedding-backend","title":"Semantic search for similar documents (requires embedding backend)","text":"<pre><code>crispt --inp ./my_corpus --semantic \"patient anxiety\" --num 8 --rec 0.45\n</code></pre>"},{"location":"DEMO/#find-documents-similar-to-a-list-of-document-ids","title":"Find documents similar to a list of document IDs","text":"<pre><code>crispt --inp ./my_corpus --similar-docs \"1,2,3\" --num 5\n</code></pre>"},{"location":"DEMO/#semantic-chunks-search-within-specific-document-chunks-use-with-doc-id","title":"Semantic-chunks: search within specific document chunks (use with --doc-id)","text":"<pre><code>crispt --inp ./my_corpus --doc-id 5 --semantic-chunks \"query phrase\" --rec 0.6\n</code></pre>"},{"location":"DEMO/#embedding-linking-and-stats","title":"Embedding linking and stats","text":"<pre><code>crispt --inp ./my_corpus --embedding-link \"cosine:3:0.7\" --embedding-stats --out ./emb_links\ncrispt --inp ./emb_links --filters embedding:df --out ./docs_linked_to_rows\n</code></pre>"},{"location":"DEMO/#g-temporal-utilities-crispt-examples","title":"G. Temporal utilities (<code>crispt</code>) \u2014 examples","text":""},{"location":"DEMO/#link-by-time-nearest-window-with-seconds-or-sequence","title":"Link by time (nearest, window with seconds, or sequence)","text":"<pre><code>crispt --inp ./my_corpus --temporal-link \"nearest:timestamp\"\ncrispt --inp ./my_corpus --temporal-link \"window:timestamp:300\"  # \u00b1300 seconds\n</code></pre>"},{"location":"DEMO/#temporal-summaries-sentiment-trends-and-topics-over-periods","title":"Temporal summaries, sentiment trends, and topics over periods","text":"<pre><code>crispt --inp ./my_corpus --temporal-summary W\ncrispt --inp ./my_corpus --temporal-sentiment W:mean\ncrispt --inp ./my_corpus --temporal-topics W:5\n</code></pre>"},{"location":"DEMO/#h-visualization-examples-crispviz","title":"H. Visualization examples (<code>crispviz</code>)","text":""},{"location":"DEMO/#word-frequency-topic-wordcloud-lda-interactive-visualization","title":"Word frequency + topic wordcloud + LDA interactive visualization","text":"<pre><code>crispviz --inp ./crisp_input_analyzed --out viz_out --freq --wordcloud --ldavis\n</code></pre>"},{"location":"DEMO/#top-terms-with-custom-top-n-and-bins","title":"Top terms with custom top-n and bins","text":"<pre><code>crispviz --inp ./crisp_input --out viz_out --top-terms --top-n 30 --bins 80\n</code></pre>"},{"location":"DEMO/#correlation-heatmap-with-selected-numeric-columns","title":"Correlation heatmap with selected numeric columns","text":"<pre><code>crispviz --inp ./survey_corpus --out viz_out --corr-heatmap --corr-columns \"age,income,score\"\n</code></pre>"},{"location":"DEMO/#graph-visualization-filtered-by-node-types-and-a-different-layout","title":"Graph visualization filtered by node types and a different layout","text":"<pre><code>crispviz --inp ./my_corpus --out viz_out --graph --graph-nodes document,keyword --graph-layout circular\n</code></pre>"},{"location":"DEMO/#i-small-tips-parameter-semantics","title":"I. Small tips &amp; parameter semantics","text":"<ul> <li><code>--rec</code> for <code>crispt</code> semantic commands can be a similarity threshold (float, default 0.4), while <code>--rec</code> for some <code>crisp</code> commands is used as an integer count \u2014 check the command context.</li> <li><code>--num</code> defaults differ by context (e.g., <code>crispt</code> search default is 5; <code>crisp</code> analysis default is 3).</li> <li><code>--aggregation</code> accepts <code>majority|mean|first|mode</code> and controls how multiple documents map to one numeric row are aggregated for ML tasks.</li> </ul>"},{"location":"DEMO/#j-full-run-example-import-analyze-visualize","title":"J. Full-run example (import \u2192 analyze \u2192 visualize)","text":"<pre><code># 1) Import\ncrisp --source ./raw_data --unstructured \"comments\" --out ./crisp_input\n\n# 2) Run NLP + sentiment + save\ncrisp --inp ./crisp_input --topics --assign --sentiment --out ./crisp_input_analyzed\n\n# 3) Link by embedding and run regression on linked set\ncrispt --inp ./crisp_input_analyzed --embedding-link \"cosine:1:0.7\" --out ./linked\ncrisp --inp ./linked --outcome satisfaction_score --regression --out ./final_results\n\n# 4) Create visualizations\ncrispviz --inp ./final_results --out viz_out --ldavis --wordcloud --corr-heatmap\n</code></pre>"},{"location":"GRAPH/","title":"GRAPH","text":"<p>Example: Graph Generation Workflow</p> <pre><code># Step 1: Load corpus with documents that have keywords\ncrispt --inp crisp_input --graph --out crisp_input\n\n# Step 2: Visualize the graph (all node types)\ncrispviz --inp crisp_input --out visualizations --graph\n\n# Step 3: Visualize only documents and keywords\ncrispviz --inp crisp_input --out visualizations --graph --graph-nodes document,keyword\n\n# Step 4: Try different graph layouts\ncrispviz --inp crisp_input --out visualizations --graph --graph-layout circular\n</code></pre> <p>About <code>--graph-nodes</code>:</p> <p>The <code>--graph-nodes</code> option allows you to filter which node types are included in the graph visualization. For example, to show only documents and keywords, use:</p> <pre><code>crispviz --inp crisp_input --out visualizations --graph --graph-nodes document,keyword\n</code></pre> <p>Valid node types: <code>document</code>, <code>keyword</code>, <code>cluster</code>, <code>metadata</code>. If omitted or set to <code>all</code>, all node types are included. Edges are only shown if both endpoints are present in the filtered node set.</p> <p>The graph visualization shows: - Documents (red nodes): Your corpus documents - Keywords (teal nodes): Keywords extracted from documents - Clusters (light green nodes): Document clusters (if clustering analysis was performed) - Metadata (yellow nodes): Metadata from DataFrame (if present with aligning ID field)</p> <p>Note: If documents don't have keywords assigned, run keyword assignment first using text analysis features before generating the graph.</p>"},{"location":"MCP_SERVER/","title":"CRISP-T MCP Server Guide","text":""},{"location":"MCP_SERVER/#overview","title":"Overview","text":"<p>The CRISP-T MCP (Model Context Protocol) server exposes all CRISP-T functionality through a standardized interface that can be used by AI assistants and other MCP-compatible clients.</p>"},{"location":"MCP_SERVER/#installation","title":"Installation","text":"<p>Install CRISP-T with MCP support:</p> <pre><code>pip install crisp-t\n</code></pre> <p>For machine learning features:</p> <pre><code>pip install crisp-t[ml]\n</code></pre>"},{"location":"MCP_SERVER/#starting-the-server","title":"Starting the Server","text":"<p>The MCP server runs via stdio and can be started with:</p> <pre><code>crisp-mcp\n</code></pre> <p>Or using Python directly:</p> <pre><code>python -m crisp_t.mcp\n</code></pre>"},{"location":"MCP_SERVER/#client-configuration","title":"Client Configuration","text":""},{"location":"MCP_SERVER/#claude-desktop","title":"Claude Desktop","text":"<p>Add the following to your Claude Desktop configuration file:</p> <p>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></p> <p>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code></p> <pre><code>{\n  \"mcpServers\": {\n    \"crisp-t\": {\n      \"command\": \"crisp-mcp\"\n    }\n  }\n}\n</code></pre> <p>After adding the configuration, restart Claude Desktop. The CRISP-T tools will be available in the MCP menu.</p>"},{"location":"MCP_SERVER/#other-mcp-clients","title":"Other MCP Clients","text":"<p>Configure your MCP client to run the <code>crisp-mcp</code> command via stdio. Consult your client's documentation for specific configuration instructions.</p>"},{"location":"MCP_SERVER/#available-tools","title":"Available Tools","text":""},{"location":"MCP_SERVER/#corpus-management","title":"Corpus Management","text":""},{"location":"MCP_SERVER/#load_corpus","title":"<code>load_corpus</code>","text":"<p>Load a corpus from a folder or source directory.</p> <p>Arguments: - <code>inp</code> (optional): Path to folder containing corpus.json - <code>source</code> (optional): Source directory or URL to read data from - <code>text_columns</code> (optional): Comma-separated text column names for CSV data - <code>ignore_words</code> (optional): Comma-separated words to ignore during analysis</p> <p>Example:</p> <pre><code>{\n  \"inp\": \"/path/to/corpus_folder\"\n}\n</code></pre>"},{"location":"MCP_SERVER/#save_corpus","title":"<code>save_corpus</code>","text":"<p>Save the current corpus to a folder.</p> <p>Arguments: - <code>out</code> (required): Output folder path</p>"},{"location":"MCP_SERVER/#add_document","title":"<code>add_document</code>","text":"<p>Add a new document to the corpus.</p> <p>Arguments: - <code>doc_id</code> (required): Unique document ID - <code>text</code> (required): Document text content - <code>name</code> (optional): Document name</p>"},{"location":"MCP_SERVER/#remove_document","title":"<code>remove_document</code>","text":"<p>Remove a document by ID.</p> <p>Arguments: - <code>doc_id</code> (required): Document ID to remove</p>"},{"location":"MCP_SERVER/#get_document","title":"<code>get_document</code>","text":"<p>Get document details by ID.</p> <p>Arguments: - <code>doc_id</code> (required): Document ID</p>"},{"location":"MCP_SERVER/#list_documents","title":"<code>list_documents</code>","text":"<p>List all document IDs in the corpus.</p>"},{"location":"MCP_SERVER/#add_relationship","title":"<code>add_relationship</code>","text":"<p>Add a relationship between text keywords and numeric columns.</p> <p>Arguments: - <code>first</code> (required): First entity (e.g., \"text:healthcare\") - <code>second</code> (required): Second entity (e.g., \"num:age_group\") - <code>relation</code> (required): Relationship type (e.g., \"correlates\")</p> <p>Example:</p> <pre><code>{\n  \"first\": \"text:satisfaction\",\n  \"second\": \"num:rating_score\",\n  \"relation\": \"predicts\"\n}\n</code></pre>"},{"location":"MCP_SERVER/#get_relationships","title":"<code>get_relationships</code>","text":"<p>Get all relationships in the corpus.</p>"},{"location":"MCP_SERVER/#get_relationships_for_keyword","title":"<code>get_relationships_for_keyword</code>","text":"<p>Get relationships involving a specific keyword.</p> <p>Arguments: - <code>keyword</code> (required): Keyword to search for</p>"},{"location":"MCP_SERVER/#nlptext-analysis","title":"NLP/Text Analysis","text":""},{"location":"MCP_SERVER/#generate_coding_dictionary","title":"<code>generate_coding_dictionary</code>","text":"<p>Generate a qualitative coding dictionary with categories, properties, and dimensions.</p> <p>Arguments: - <code>num</code> (optional): Number of categories to extract (default: 3) - <code>top_n</code> (optional): Top N items per category (default: 3)</p> <p>Returns: - Categories (verbs representing main actions/themes) - Properties (nouns associated with categories) - Dimensions (adjectives/adverbs describing properties)</p>"},{"location":"MCP_SERVER/#topic_modeling","title":"<code>topic_modeling</code>","text":"<p>Perform LDA topic modeling to discover latent topics.</p> <p>Arguments: - <code>num_topics</code> (optional): Number of topics to generate (default: 3) - <code>num_words</code> (optional): Number of words per topic (default: 5)</p> <p>Returns: Topics with keywords and weights showing word importance within each topic.</p>"},{"location":"MCP_SERVER/#assign_topics","title":"<code>assign_topics</code>","text":"<p>Assign documents to their dominant topics.</p> <p>Arguments: - <code>num_topics</code> (optional): Number of topics (should match topic_modeling, default: 3)</p> <p>Returns: Document-topic assignments with contribution percentages. These assignments create keyword labels that can be used to filter or categorize documents.</p>"},{"location":"MCP_SERVER/#extract_categories","title":"<code>extract_categories</code>","text":"<p>Extract common categories/concepts from the corpus.</p> <p>Arguments: - <code>num</code> (optional): Number of categories (default: 10)</p>"},{"location":"MCP_SERVER/#generate_summary","title":"<code>generate_summary</code>","text":"<p>Generate an extractive text summary of the corpus.</p> <p>Arguments: - <code>weight</code> (optional): Summary length parameter (default: 10)</p>"},{"location":"MCP_SERVER/#sentiment_analysis","title":"<code>sentiment_analysis</code>","text":"<p>Perform VADER sentiment analysis.</p> <p>Arguments: - <code>documents</code> (optional): Analyze at document level (default: false) - <code>verbose</code> (optional): Verbose output (default: true)</p> <p>Returns: Sentiment scores (negative, neutral, positive, compound).</p>"},{"location":"MCP_SERVER/#semantic-search-requires-chromadb","title":"Semantic Search (requires chromadb)","text":""},{"location":"MCP_SERVER/#semantic_search","title":"<code>semantic_search</code>","text":"<p>Find documents similar to a query using semantic similarity.</p> <p>Arguments: - <code>query</code> (required): Search query text - <code>n_results</code> (optional): Number of similar documents to return (default: 5)</p> <p>Returns: List of similar documents with their IDs and names, ranked by semantic similarity.</p> <p>Example:</p> <pre><code>{\n  \"query\": \"machine learning and AI\",\n  \"n_results\": 5\n}\n</code></pre>"},{"location":"MCP_SERVER/#find_similar_documents","title":"<code>find_similar_documents</code>","text":"<p>Find documents similar to a given set of reference documents based on semantic similarity. This tool is particularly useful for literature reviews and qualitative research where you want to find additional documents that are similar to a set of known relevant documents. It can also be used to identify documents with similar themes, topics, or content for grouping and analysis purposes.</p> <p>Arguments: - <code>document_ids</code> (required): A single document ID or comma-separated list of document IDs to use as reference - <code>n_results</code> (optional): Number of similar documents to return (default: 5) - <code>threshold</code> (optional): Minimum similarity threshold 0-1 (default: 0.7). Only documents above this threshold are returned</p> <p>Returns: List of document IDs similar to the reference documents, excluding the reference documents themselves.</p> <p>Example:</p> <pre><code>{\n  \"document_ids\": \"doc1,doc5,doc12\",\n  \"n_results\": 10,\n  \"threshold\": 0.7\n}\n</code></pre> <p>Use Cases: - Literature reviews: Find papers similar to known relevant papers - Qualitative research: Identify documents with similar themes - Content grouping: Group similar documents for analysis - Document recommendation: Suggest related documents to researchers</p>"},{"location":"MCP_SERVER/#semantic_chunk_search","title":"<code>semantic_chunk_search</code>","text":"<p>Perform semantic search on chunks of a specific document. This tool is useful for coding and annotating documents by identifying relevant sections that match specific concepts or themes.</p> <p>Arguments: - <code>query</code> (required): Search query text (concept or set of concepts) - <code>doc_id</code> (required): Document ID to search within - <code>threshold</code> (optional): Minimum similarity threshold 0-1 (default: 0.5). Only chunks above this threshold are returned - <code>n_results</code> (optional): Maximum number of chunks to retrieve before filtering (default: 10)</p> <p>Returns: List of matching text chunks from the specified document that can be used for qualitative analysis or document annotation.</p> <p>Example:</p> <pre><code>{\n  \"query\": \"patient satisfaction\",\n  \"doc_id\": \"interview_01\",\n  \"threshold\": 0.6,\n  \"n_results\": 10\n}\n</code></pre> <p>Use Cases: - Coding qualitative interview transcripts for specific themes - Identifying sections of documents relevant to research questions - Annotating documents with concept labels - Finding evidence for theoretical constructs within texts</p>"},{"location":"MCP_SERVER/#export_metadata_df","title":"<code>export_metadata_df</code>","text":"<p>Export ChromaDB collection metadata as a pandas DataFrame.</p> <p>Arguments: - <code>metadata_keys</code> (optional): Comma-separated list of metadata keys to include</p> <p>Returns: DataFrame with document metadata that can be merged with numeric data for mixed-methods analysis.</p>"},{"location":"MCP_SERVER/#tdabm-topological-data-analysis-ball-mapper","title":"TDABM (Topological Data Analysis Ball Mapper)","text":""},{"location":"MCP_SERVER/#tdabm_analysis","title":"<code>tdabm_analysis</code>","text":"<p>Perform Topological Data Analysis Ball Mapper (TDABM) analysis to uncover hidden, global patterns in complex, noisy, or high-dimensional data.</p> <p>Based on the algorithm by Rudkin and Dlotko (2024), TDABM creates a point cloud from multidimensional data and covers it with overlapping balls, revealing topological structure and relationships between variables.</p> <p>Arguments: - <code>y_variable</code> (required): Name of the continuous Y variable to analyze - <code>x_variables</code> (required): Comma-separated list of ordinal/numeric X variable names - <code>radius</code> (optional): Radius for ball coverage (default: 0.3). Smaller values create more detailed mappings.</p> <p>Example:</p> <pre><code>{\n  \"y_variable\": \"satisfaction\",\n  \"x_variables\": \"age,income,education\",\n  \"radius\": 0.3\n}\n</code></pre> <p>Returns: Analysis results in JSON format including landmark points, their locations, connections, and mean Y values. Results are stored in corpus metadata['tdabm'].</p> <p>Use Cases: - Discovering hidden patterns in multidimensional data - Visualizing relationships between multiple variables - Identifying clusters and connections in complex datasets - Performing model-free exploratory data analysis - Understanding global structure in high-dimensional data</p> <p>Note: After running TDABM analysis, use <code>save_corpus</code> to persist results, then visualize with <code>crispviz --tdabm</code>.</p> <p>Reference: Rudkin, S., &amp; Dlotko, P. (2024). Topological Data Analysis Ball Mapper for multidimensional data visualization. Paper reference to be added - algorithm implementation based on the TDABM methodology described by the authors.</p>"},{"location":"MCP_SERVER/#dataframecsv-operations","title":"DataFrame/CSV Operations","text":""},{"location":"MCP_SERVER/#get_df_columns","title":"<code>get_df_columns</code>","text":"<p>Get all column names from the DataFrame.</p>"},{"location":"MCP_SERVER/#get_df_row_count","title":"<code>get_df_row_count</code>","text":"<p>Get the number of rows in the DataFrame.</p>"},{"location":"MCP_SERVER/#get_df_row","title":"<code>get_df_row</code>","text":"<p>Get a specific row by index.</p> <p>Arguments: - <code>index</code> (required): Row index</p>"},{"location":"MCP_SERVER/#machine-learning-requires-crisp-tml","title":"Machine Learning (requires crisp-t[ml])","text":""},{"location":"MCP_SERVER/#kmeans_clustering","title":"<code>kmeans_clustering</code>","text":"<p>Perform K-Means clustering on numeric data.</p> <p>Arguments: - <code>num_clusters</code> (optional): Number of clusters (default: 3) - <code>outcome</code> (optional): Outcome variable to exclude</p> <p>Returns: Cluster assignments and membership information.</p>"},{"location":"MCP_SERVER/#decision_tree_classification","title":"<code>decision_tree_classification</code>","text":"<p>Train a decision tree classifier and return variable importance.</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable - <code>top_n</code> (optional): Top N important features (default: 10)</p> <p>Returns: - Confusion matrix - Feature importance rankings (shows which variables are most predictive)</p>"},{"location":"MCP_SERVER/#svm_classification","title":"<code>svm_classification</code>","text":"<p>Perform SVM classification.</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable</p> <p>Returns: Confusion matrix showing classification performance.</p>"},{"location":"MCP_SERVER/#neural_network_classification","title":"<code>neural_network_classification</code>","text":"<p>Train a neural network classifier.</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable</p> <p>Returns: Predictions and accuracy metrics.</p>"},{"location":"MCP_SERVER/#regression_analysis","title":"<code>regression_analysis</code>","text":"<p>Perform linear or logistic regression (auto-detects based on outcome type).</p> <p>Arguments: - <code>outcome</code> (required): Target/outcome variable</p> <p>Returns: - Model type (linear or logistic) - Coefficients for each factor (showing strength and direction of relationships) - Intercept - Performance metrics (R\u00b2, accuracy, etc.)</p>"},{"location":"MCP_SERVER/#pca_analysis","title":"<code>pca_analysis</code>","text":"<p>Perform Principal Component Analysis.</p> <p>Arguments: - <code>outcome</code> (required): Variable to exclude from PCA - <code>n_components</code> (optional): Number of components (default: 3)</p>"},{"location":"MCP_SERVER/#association_rules","title":"<code>association_rules</code>","text":"<p>Generate association rules using Apriori algorithm.</p> <p>Arguments: - <code>outcome</code> (required): Variable to exclude - <code>min_support</code> (optional): Minimum support 1-99 (default: 50) - <code>min_threshold</code> (optional): Minimum threshold 1-99 (default: 50)</p>"},{"location":"MCP_SERVER/#knn_search","title":"<code>knn_search</code>","text":"<p>Find K-nearest neighbors for a specific record.</p> <p>Arguments: - <code>outcome</code> (required): Target variable - <code>n</code> (optional): Number of neighbors (default: 3) - <code>record</code> (optional): Record index, 1-based (default: 1)</p>"},{"location":"MCP_SERVER/#resources","title":"Resources","text":"<p>The server exposes corpus documents as resources that can be read:</p> <ul> <li><code>corpus://document/{id}</code> - Access document text content by ID</li> </ul>"},{"location":"MCP_SERVER/#prompts","title":"Prompts","text":""},{"location":"MCP_SERVER/#analysis_workflow","title":"<code>analysis_workflow</code>","text":"<p>Complete step-by-step guide for conducting a CRISP-T analysis based on INSTRUCTIONS.md.</p> <p>This prompt provides: - Data preparation steps - Descriptive analysis workflow - Advanced pattern discovery techniques - Predictive modeling approaches - Validation and triangulation strategies</p>"},{"location":"MCP_SERVER/#triangulation_guide","title":"<code>triangulation_guide</code>","text":"<p>Guide for triangulating qualitative and quantitative findings.</p> <p>This prompt explains: - How to link topic keywords with numeric variables - Strategies for comparing patterns across data types - Using relationships to document connections - Best practices for validation</p>"},{"location":"MCP_SERVER/#example-workflow","title":"Example Workflow","text":"<p>Here's a typical analysis workflow using the MCP server:</p> <ol> <li> <p>Load data <code>load_corpus(inp=\"/path/to/corpus\")</code></p> </li> <li> <p>Explore the data <code>list_documents()    get_df_columns()    get_df_row_count()</code></p> </li> <li> <p>Perform text analysis <code>generate_coding_dictionary(num=10, top_n=5)    topic_modeling(num_topics=5, num_words=10)    assign_topics(num_topics=5)    sentiment_analysis(documents=true)</code></p> </li> <li> <p>Perform numeric analysis <code>regression_analysis(outcome=\"satisfaction_score\")    decision_tree_classification(outcome=\"readmission\", top_n=10)</code></p> </li> <li> <p>Link findings <code>add_relationship(      first=\"text:healthcare_access\",      second=\"num:insurance_status\",      relation=\"correlates\"    )</code></p> </li> <li> <p>Save results <code>save_corpus(out=\"/path/to/output\")</code></p> </li> </ol>"},{"location":"MCP_SERVER/#key-features","title":"Key Features","text":""},{"location":"MCP_SERVER/#topic-modeling-creates-keywords","title":"Topic Modeling Creates Keywords","text":"<p>When you use <code>topic_modeling</code> and <code>assign_topics</code>, the tool assigns topic keywords to documents. These keywords can then be used to: - Filter documents by theme - Create relationships with numeric columns - Categorize documents for further analysis</p>"},{"location":"MCP_SERVER/#regression-shows-coefficients","title":"Regression Shows Coefficients","text":"<p>The <code>regression_analysis</code> tool returns coefficients for each predictor variable, showing: - Strength of relationship (larger absolute value = stronger) - Direction of relationship (positive or negative) - Statistical significance</p>"},{"location":"MCP_SERVER/#decision-trees-show-importance","title":"Decision Trees Show Importance","text":"<p>The <code>decision_tree_classification</code> tool returns variable importance rankings, indicating which features are most predictive of the outcome.</p>"},{"location":"MCP_SERVER/#relationships-link-data-types","title":"Relationships Link Data Types","text":"<p>Use <code>add_relationship</code> to document connections between: - Text findings (keywords from topics) - Numeric variables (DataFrame columns) - Theoretical constructs</p> <p>Example: Link \"healthcare_quality\" keyword to \"patient_satisfaction\" column with relation \"predicts\".</p>"},{"location":"MCP_SERVER/#tips-for-effective-use","title":"Tips for Effective Use","text":"<ol> <li> <p>Always load corpus first: Most tools require a loaded corpus to function.</p> </li> <li> <p>Use prompts for guidance: Request the <code>analysis_workflow</code> or <code>triangulation_guide</code> prompts for step-by-step instructions.</p> </li> <li> <p>Save frequently: Use <code>save_corpus</code> to preserve analysis metadata and relationships.</p> </li> <li> <p>Document relationships: Use <code>add_relationship</code> to link textual findings with numeric variables based on your theoretical framework.</p> </li> <li> <p>Iterate and refine: Analysis is iterative. Load your saved corpus and continue refining based on new insights.</p> </li> </ol>"},{"location":"MCP_SERVER/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MCP_SERVER/#no-corpus-loaded-error","title":"\"No corpus loaded\" error","text":"<p>Make sure to call <code>load_corpus</code> before using other tools.</p>"},{"location":"MCP_SERVER/#ml-dependencies-not-available-error","title":"\"ML dependencies not available\" error","text":"<p>Install the ML extras:</p> <pre><code>pip install crisp-t[ml]\n</code></pre>"},{"location":"MCP_SERVER/#server-not-appearing-in-claude-desktop","title":"Server not appearing in Claude Desktop","text":"<ol> <li>Check that the configuration file is in the correct location</li> <li>Verify the JSON syntax is valid</li> <li>Restart Claude Desktop after adding the configuration</li> <li>Check that <code>crisp-mcp</code> command is in your PATH</li> </ol>"},{"location":"MCP_SERVER/#additional-resources","title":"Additional Resources","text":"<ul> <li>CRISP-T Documentation</li> <li>INSTRUCTION.md - Detailed function reference</li> <li>Example Workflow</li> <li>GitHub Repository</li> </ul>"},{"location":"SEMANTIC_SEARCH/","title":"Semantic Search Examples","text":"<p>This document demonstrates using the semantic search features in CRISP-T.</p>"},{"location":"SEMANTIC_SEARCH/#command-line-examples","title":"Command Line Examples","text":""},{"location":"SEMANTIC_SEARCH/#example-1-basic-semantic-search","title":"Example 1: Basic Semantic Search","text":"<p>Search for documents similar to a query:</p> <pre><code>crispt \\\n  --id medical_corpus \\\n  --name \"Medical Research\" \\\n  --doc \"1|AI in Healthcare|Machine learning and artificial intelligence applications in medical diagnosis\" \\\n  --doc \"2|NLP Research|Natural language processing techniques for clinical text analysis\" \\\n  --doc \"3|Patient Care|Improving patient outcomes through evidence-based medicine\" \\\n  --doc \"4|Drug Discovery|Using computational methods for pharmaceutical research\" \\\n  --semantic \"artificial intelligence medical\" \\\n  --num 2 \\\n  --print\n</code></pre> <p>This will find the 2 most similar documents to the query \"artificial intelligence medical\".</p>"},{"location":"SEMANTIC_SEARCH/#example-2-find-similar-documents-literature-review","title":"Example 2: Find Similar Documents (Literature Review)","text":"<p>Find documents similar to a reference document (useful for literature reviews):</p> <pre><code>crispt \\\n  --id research_corpus \\\n  --name \"Research Papers\" \\\n  --doc \"1|Paper A|Deep learning for medical image analysis\" \\\n  --doc \"2|Paper B|Transfer learning in computer vision\" \\\n  --doc \"3|Paper C|Natural language understanding models\" \\\n  --doc \"4|Paper D|Convolutional neural networks for diagnosis\" \\\n  --similar-docs \"1\" \\\n  --num 2 \\\n  --rec 0.7 \\\n  --print\n</code></pre> <p>This finds 2 documents most similar to document \"1\", with a similarity threshold of 0.7.</p> <p>You can also search based on multiple reference documents:</p> <pre><code>crispt \\\n  --inp ./corpus_folder \\\n  --similar-docs \"1,2,5\" \\\n  --num 5 \\\n  --rec 0.6\n</code></pre> <p>This finds documents similar to documents 1, 2, and 5 combined, returning up to 5 results with similarity above 0.6.</p>"},{"location":"SEMANTIC_SEARCH/#example-3-export-metadata-to-dataframe","title":"Example 3: Export Metadata to DataFrame","text":"<p>Create a corpus and export document metadata as a DataFrame:</p> <pre><code>crispt \\\n  --id research_papers \\\n  --name \"Research Papers\" \\\n  --doc \"1|Paper A|Deep learning for image classification\" \\\n  --doc \"2|Paper B|Transfer learning in computer vision\" \\\n  --doc \"3|Paper C|Natural language understanding models\" \\\n  --metadata-df \\\n  --df-cols \\\n  --out ./output\n</code></pre> <p>This creates a DataFrame with document IDs and metadata, which can be saved for further analysis.</p>"},{"location":"SEMANTIC_SEARCH/#example-3-filter-and-export","title":"Example 3: Filter and Export","text":"<p>Combine semantic search with DataFrame export:</p> <pre><code>crispt \\\n  --inp ./corpus_folder \\\n  --semantic \"machine learning healthcare\" \\\n  --num 10 \\\n  --metadata-df \\\n  --metadata-keys \"topic,year,author\" \\\n  --out ./filtered_corpus\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#python-api-examples","title":"Python API Examples","text":""},{"location":"SEMANTIC_SEARCH/#example-1-basic-semantic-search_1","title":"Example 1: Basic Semantic Search","text":"<pre><code>from crisp_t.model import Corpus, Document\nfrom crisp_t.semantic import Semantic\n\n# Create corpus\ndocs = [\n    Document(\n        id='1',\n        name='AI Doc',\n        text='Machine learning and artificial intelligence',\n        metadata={'topic': 'tech', 'year': '2024'}\n    ),\n    Document(\n        id='2',\n        name='NLP Doc',\n        text='Natural language processing',\n        metadata={'topic': 'nlp', 'year': '2023'}\n    ),\n]\ncorpus = Corpus(id='test', name='Test', description='Test', documents=docs)\n\n# Perform semantic search\nsemantic = Semantic(corpus)\nresult = semantic.get_similar('artificial intelligence', n_results=1)\n\nprint(f\"Found {len(result.documents)} documents\")\nfor doc in result.documents:\n    print(f\"- {doc.name}: {doc.text[:50]}...\")\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#example-2-find-similar-documents","title":"Example 2: Find Similar Documents","text":"<pre><code>from crisp_t.model import Corpus, Document\nfrom crisp_t.semantic import Semantic\n\n# Create corpus\ndocs = [\n    Document(id='1', text='Machine learning and AI'),\n    Document(id='2', text='Natural language processing'),\n    Document(id='3', text='Healthcare data analysis'),\n    Document(id='4', text='Deep learning neural networks'),\n]\ncorpus = Corpus(id='research', documents=docs)\n\n# Find documents similar to document 1\nsemantic = Semantic(corpus)\nsimilar_ids = semantic.get_similar_documents(\n    document_ids='1',\n    n_results=2,\n    threshold=0.7\n)\n\nprint(f\"Similar documents: {similar_ids}\")\n# Output: Similar documents: ['4', '2']\n\n# Find documents similar to multiple reference docs\nsimilar_ids = semantic.get_similar_documents(\n    document_ids='1,2',\n    n_results=2,\n    threshold=0.6\n)\nprint(f\"Similar to 1 and 2: {similar_ids}\")\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#example-3-export-metadata","title":"Example 3: Export Metadata","text":"<pre><code>from crisp_t.model import Corpus, Document\nfrom crisp_t.semantic import Semantic\n\n# Create corpus with metadata\ndocs = [\n    Document(\n        id='doc1',\n        text='Healthcare AI research',\n        metadata={'category': 'medical', 'score': 0.95}\n    ),\n    Document(\n        id='doc2',\n        text='Computer vision applications',\n        metadata={'category': 'vision', 'score': 0.87}\n    ),\n]\ncorpus = Corpus(id='research', documents=docs)\n\n# Export metadata to DataFrame\nsemantic = Semantic(corpus)\nresult = semantic.get_df(metadata_keys=['category', 'score'])\n\nprint(result.df)\n# Output:\n#      id category  score\n# 0  doc1  medical   0.95\n# 1  doc2   vision   0.87\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#example-3-save-and-restore-collection","title":"Example 3: Save and Restore Collection","text":"<pre><code>from crisp_t.semantic import Semantic\n\n# Create and save\nsemantic = Semantic(corpus)\nsemantic.save_collection('./my_collection')\n\n# Later, restore\nnew_semantic = Semantic(new_corpus)\nnew_semantic.restore_collection('./my_collection')\nresult = new_semantic.get_similar('query text', n_results=5)\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#mcp-tool-examples","title":"MCP Tool Examples","text":"<p>When using CRISP-T's MCP server with an AI assistant:</p>"},{"location":"SEMANTIC_SEARCH/#semantic-search","title":"Semantic Search","text":"<pre><code>Use the semantic_search tool with:\n- query: \"machine learning healthcare\"\n- n_results: 5\n\nThis will find the 5 most similar documents and update the current corpus.\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#find-similar-documents-literature-review","title":"Find Similar Documents (Literature Review)","text":"<pre><code>Use the find_similar_documents tool with:\n- document_ids: \"doc1,doc2\"\n- n_results: 5\n- threshold: 0.7\n\nThis will find documents similar to doc1 and doc2, returning up to 5 results \nwith similarity above 0.7. This is particularly useful for literature reviews \nwhere you want to find additional relevant papers similar to known good examples.\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#export-metadata","title":"Export Metadata","text":"<pre><code>Use the export_metadata_df tool with:\n- metadata_keys: \"topic,year,author\"\n\nThis will create a DataFrame with the specified metadata fields.\n</code></pre>"},{"location":"SEMANTIC_SEARCH/#notes","title":"Notes","text":"<ul> <li>By default, ChromaDB uses ONNX MiniLM-L6-V2 embeddings which require a download on first use</li> <li>If network is unavailable, the system will automatically fall back to simple bag-of-words embeddings</li> <li>For production use, consider pre-downloading the embedding model or using a persistent ChromaDB instance</li> <li>Semantic search works best with longer, descriptive text documents</li> <li>The simple embeddings fallback is suitable for testing and offline use, but production should use the default embeddings for better quality</li> </ul>"},{"location":"SEMANTIC_SEARCH/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>Semantic search can be combined with other CRISP-T features:</p> <ol> <li>Topic Modeling + Semantic Search: First use topic modeling to assign keywords, then use semantic search to find similar documents based on those topics</li> <li>Coding Dictionary + Metadata Export: Extract coding dictionaries, then export metadata to analyze patterns across documents</li> <li>Triangulation: Use semantic search to group similar qualitative responses, then correlate with quantitative data using relationships</li> </ol>"},{"location":"TDABM/","title":"TDABM","text":""},{"location":"TDABM/#tdabm-topological-data-analysis-ball-mapper","title":"TDABM (Topological Data Analysis Ball Mapper)","text":"<p>CRISP-T implements the Topological Data Analysis Ball Mapper (TDABM) algorithm based on Rudkin and Dlotko (2024). TDABM provides a model-free method to visualize multidimensional data and uncover hidden, global patterns in complex, noisy, or high-dimensional datasets.</p>"},{"location":"TDABM/#how-tdabm-works","title":"How TDABM Works","text":"<ol> <li>Point Cloud Creation: Data is transformed into a point cloud where each axis represents one of the selected variables (X variables).</li> <li>Ball Covering: The algorithm randomly selects landmark points and creates balls of a specified radius around them, covering all data points.</li> <li>Connection Mapping: Landmark points with non-empty intersections are connected, revealing the topological structure of the data.</li> <li>Visualization: The result is visualized as a 2D graph where:</li> <li>Circle size represents the number of points in each ball</li> <li>Circle color represents the mean value of the outcome variable (Y), ranging from red (low) to purple (high)</li> <li>Lines connect overlapping balls, showing the data's topological structure</li> </ol>"},{"location":"TDABM/#using-tdabm","title":"Using TDABM","text":"<pre><code># Perform TDABM analysis\ncrispt --inp corpus_dir --tdabm satisfaction:age,income,education:0.3 --out corpus_dir\n\n# Visualize TDABM results\ncrispviz --inp corpus_dir --tdabm --out visualizations\n</code></pre>"},{"location":"TDABM/#when-to-use-tdabm","title":"When to Use TDABM","text":"<ul> <li>Discovering hidden patterns in multidimensional data</li> <li>Visualizing relationships between multiple variables</li> <li>Identifying clusters and connections in complex datasets</li> <li>Performing model-free exploratory data analysis</li> <li>Understanding global structure in high-dimensional data</li> </ul>"},{"location":"TDABM/#reference","title":"Reference","text":"<p>Rudkin, S., &amp; Dlotko, P. (2024). Topological Data Analysis Ball Mapper for multidimensional data visualization. Paper reference to be added - algorithm implementation based on the TDABM methodology described by the authors.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Feature/fix skill 2 #75 (dermatologist)</li> <li>Feature/fix mcp 1 #74 (dermatologist)</li> <li>Time embedding Linkage #70 (dermatologist)</li> <li>Bump pyasn1 from 0.6.1 to 0.6.2 #64 (dependabot[bot])</li> <li>Enhance CLI UX with colored output and improved messaging for non-technical users #61 (Copilot)</li> <li>Add --import flag synonym for --source and enhance help with data preparation steps #56 (Copilot)</li> <li>Bump peter-evans/create-pull-request from 7 to 8 #55 (dependabot[bot])</li> <li>Bump nvuillam/github-dependents-info from 1.6.3 to 2.0.2 #54 (dependabot[bot])</li> <li>Add getting started video link to README #53 (dermatologist)</li> <li>Optimize slow code patterns in text processing and data loading #52 (Copilot)</li> <li>Bump actions/checkout from 5 to 6 #51 (dependabot[bot])</li> <li>Add graph creation and visualization for corpus relationships #50 (Copilot)</li> <li>Feature/fix filter 1 #48 (dermatologist)</li> <li>Feature/parallel 1 #47 (dermatologist)</li> <li>Add subcommand support, unquoted syntax, and color formatting to --print option #46 (Copilot)</li> <li>Update README and add TDABM documentation for multidimensional data v\u2026 #44 (dermatologist)</li> <li>Implement TDABM \\(Topological Data Analysis Ball Mapper\\) for multidimensional data visualization #43 (Copilot)</li> <li>Update deps #41 (dermatologist)</li> <li>Replace sample CSV with dynamically created temporary CSV for data pr\u2026 #40 (dermatologist)</li> <li>Add LDA Visualization with pyLDAvis, Coherence and Perplexity Metrics #39 (Copilot)</li> <li>Add tqdm progress bars for long-running functions #37 (Copilot)</li> <li>Add LSTM text classification method to test convergence between text and outcome variables #35 (Copilot)</li> <li>Add semantic search based on document list for literature reviews #33 (Copilot)</li> <li>Add semantic chunk search for document coding and annotation #31 (Copilot)</li> <li>Bump astral-sh/setup-uv from 6 to 7 #29 (dependabot[bot])</li> <li>Add semantic search with ChromaDB and metadata export functionality #28 (Copilot)</li> <li>Add AGENTS.md file with comprehensive instructions for LLM agents #26 (Copilot)</li> <li>Update pytest.yml to support multiple OS and extend timeout; revise D\u2026 #25 (dermatologist)</li> <li>Feature/fix ml output 1 #24 (dermatologist)</li> <li>Feature/demo 1 #23 (dermatologist)</li> <li>Feature/copilot mcp fix 1 #22 (dermatologist)</li> <li>Add Linear and Logistic Regression Support to ML Module #19 (Copilot)</li> <li>Feature/vibe 1 #17 (dermatologist)</li> <li>Feature/utils 1 #16 (dermatologist)</li> <li>Feature/document meta 1 #15 (dermatologist)</li> <li>Feature/cli 1 #14 (dermatologist)</li> <li>Implement comprehensive CLI interface and documentation for CRISP-T framework #13 (Copilot)</li> <li>Feature/visualize 1 #11 (dermatologist)</li> <li>Feature/model 2 #10 (dermatologist)</li> <li>Update GitHub Actions to use actions/checkout@v5 across all workflows #9 (dermatologist)</li> <li>Feature/uv 1 #8 (dermatologist)</li> <li>Feature/readme 1 #4 (dermatologist)</li> <li>Bump astral-sh/setup-uv from 5 to 6 #2 (dependabot[bot])</li> </ul>"},{"location":"changelog/#v210-2026-01-30","title":"v2.1.0 (2026-01-30)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v200-2026-01-29","title":"v2.0.0 (2026-01-29)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v123-2026-01-09","title":"v1.2.3 (2026-01-09)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Feature request: Improve the CLI scripts using click. #60</li> <li>Feature request: Improve the cli with Typer. #57</li> </ul>"},{"location":"changelog/#v122-2025-12-23","title":"v1.2.2 (2025-12-23)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v121-2025-12-01","title":"v1.2.1 (2025-12-01)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v120-2025-11-08","title":"v1.2.0 (2025-11-08)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Add a graph creation and visualization feature. #49</li> </ul>"},{"location":"changelog/#v111-2025-10-28","title":"v1.1.1 (2025-10-28)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v110-2025-10-28","title":"v1.1.0 (2025-10-28)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v100-2025-10-26","title":"v1.0.0 (2025-10-26)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Improve the --print command with more options and better display. #45</li> </ul>"},{"location":"changelog/#v090-2025-10-25","title":"v0.9.0 (2025-10-25)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Implement Topological Data Analysis Ball Mapper \\(TDABM\\) #42</li> </ul>"},{"location":"changelog/#v080-2025-10-22","title":"v0.8.0 (2025-10-22)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: LDA output Visualization for text mining #38</li> </ul>"},{"location":"changelog/#v070-2025-10-21","title":"v0.7.0 (2025-10-21)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Add a progress bar for long running functions. #36</li> </ul>"},{"location":"changelog/#v060-2025-10-19","title":"v0.6.0 (2025-10-19)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Add LSTM method to ml.py. #34</li> </ul>"},{"location":"changelog/#v050-2025-10-18","title":"v0.5.0 (2025-10-18)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Feature request: Semantic Search based on a list of documents #32</li> </ul>"},{"location":"changelog/#v040-2025-10-15","title":"v0.4.0 (2025-10-15)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Feature request: Semantic Search for a concept within a document #30</li> </ul>"},{"location":"changelog/#v030-2025-10-12","title":"v0.3.0 (2025-10-12)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature request: Semantic Search with chromadb and metadata export as dataframe #27</li> </ul>"},{"location":"changelog/#v020-2025-10-10","title":"v0.2.0 (2025-10-10)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v010-2025-10-10","title":"v0.1.0 (2025-10-10)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Feature Request: Create an MCP server #20</li> <li>Feature request: Add Linear and Logistic regression #18</li> <li>Feature: Command line interface and Documentation. #12</li> </ul> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"cheatsheet/","title":"CRISP-T CLI Cheatsheet","text":"<p>This cheatsheet provides a comprehensive reference for all Command Line Interface (CLI) commands available in CRISP-T.</p>"},{"location":"cheatsheet/#command-overview","title":"\ud83d\udccb Command Overview","text":"<p>CRISP-T provides three main CLI tools: 1. <code>crisp</code>: Main tool for data import, analysis (text &amp; numeric), and machine learning. 2. <code>crispt</code>: Corpus management tool for detailed structural manipulation, metadata editing, and semantic search. 3. <code>crispviz</code>: Visualization tool for generating charts, graphs, and word clouds.</p>"},{"location":"cheatsheet/#data-import-commands-required-first-step","title":"\ud83d\udce5 Data Import Commands (Required first step)","text":"<p>Used to bring data into the CRISP-T environment.</p> Command Description Example <code>crisp --source &lt;path&gt;</code> Import data from a directory (PDFs, TXT, CSV) or URL. <code>crisp --source ./raw_data --out ./crisp_input</code> <code>crisp --sources &lt;path&gt; --sources &lt;path2&gt;</code> Import from multiple sources. <code>crisp --sources ./data1 --sources ./data2</code> <code>crisp --unstructured &lt;col&gt;</code> Specify CSV columns containing free text to treat as documents. <code>crisp --source ./data --unstructured \"comments\"</code> <p>Common Options: *   <code>--out &lt;path&gt;</code>: Directory to save the imported corpus (required). Recommended value <code>crisp_input</code>. *   <code>--num &lt;int&gt;</code>: Limit number of text files to import (when used with <code>--source</code>). *   <code>--rec &lt;int&gt;</code>: Limit number of CSV rows to import (when used with <code>--source</code>). *   <code>--ignore &lt;text&gt;</code>: Comma-separated list of words/columns to exclude during import.</p>"},{"location":"cheatsheet/#data-analysis-commands","title":"\ud83d\udd0d Data Analysis Commands","text":"<p>All commands below are run with the <code>--inp &lt;corpus-path&gt;</code> argument to specify the input corpus. <code>--inp</code> defaults to <code>crisp_input</code>.</p>"},{"location":"cheatsheet/#text-analysis-nlp","title":"Text Analysis (NLP)","text":"Action Flag Description Topic Modeling <code>--topics</code> Run Latent Dirichlet Allocation (LDA) to find topics. Assign Topics <code>--assign</code> Assign the most relevant topic to each document. Sentiment <code>--sentiment</code> VADER sentiment analysis (pos, neg, neu, compound). Summarize <code>--summary</code> Generate an extractive summary of the corpus. Categories <code>--cat</code> Extract common categories/themes. Coding Dictionary <code>--codedict</code> Generate a qualitative coding dictionary. All NLP <code>--nlp</code> Run all above NLP analyses at once. <ul> <li><code>--visualize</code>: Prepare data for visualization; used with <code>--assign</code> (topic assignments) and with <code>crispviz</code>.</li> </ul>"},{"location":"cheatsheet/#numeric-statistical-analysis","title":"Numeric &amp; Statistical Analysis","text":"Action Flag Description Clustering <code>--kmeans</code> K-Means clustering of numeric data. Regression <code>--regression</code> Linear or Logistic regression (auto-detected). Classification <code>--cls</code> Decision Tree and SVM classification. Neural Net <code>--nnet</code> Simple Neural Network classifier. LSTM <code>--lstm</code> Long Short-Term Memory network for text sequences. Association Rules <code>--cart</code> Apriori algorithm for association rules. PCA <code>--pca</code> Principal Component Analysis dimensionality reduction. Nearest Neighbors <code>--knn</code> K-Nearest Neighbors search. All ML <code>--ml</code> Run all above ML analyses (requires <code>crisp-t[ml]</code>). <p>Common Analysis Options: *   <code>--num &lt;n&gt;</code>: Parameter for analysis (e.g., number of clusters, topics, or summary sentences). Default: 3. *   <code>--rec &lt;n&gt;</code>: Number of results/rows to display. Default: 3. *   <code>--filters &lt;key=val&gt;</code>: Filter data before analysis (e.g., <code>category=A</code>). *   <code>--outcome &lt;col&gt;</code>: Target variable for ML/Regression. *   <code>--include &lt;cols&gt;</code>: specific columns to include in analysis. *   <code>--aggregation &lt;strategy&gt;</code>: Aggregation strategy when multiple documents link to one row. Choices: <code>majority</code>, <code>mean</code>, <code>first</code>, <code>mode</code>. (Used with cross-modal analysis. More details).</p>"},{"location":"cheatsheet/#visualization-commands-crispviz","title":"\ud83d\udcca Visualization Commands (<code>crispviz</code>)","text":"<p>Run these commands after performing the relevant analysis with <code>crisp</code> or <code>crispt</code>.</p> Visualization Flag Prerequisite Word Frequency <code>--freq</code> None Top Terms <code>--top-terms</code> None Correlation Heatmap <code>--corr-heatmap</code> CSV data present Topic Word Cloud <code>--wordcloud</code> <code>crisp --topics</code> LDA Interactive <code>--ldavis</code> <code>crisp --topics</code> (Saves as HTML) Topic Distribution <code>--by-topic</code> <code>crisp --topics</code> TDABM Network <code>--tdabm</code> <code>crispt --tdabm ...</code> Knowledge Graph <code>--graph</code> <code>crispt --graph</code> <p>Common Options: *   <code>--out &lt;dir&gt;</code>: Required. Directory to save images (PNG/HTML). *   <code>--inp &lt;dir&gt;</code>: Input corpus directory. *   <code>--topics-num &lt;n&gt;</code>: Number of topics to assume for visualization (default: 8). *   <code>--bins &lt;n&gt;</code>: Number of bins for histograms (default: 100). *   <code>--top-n &lt;n&gt;</code>: Number of top terms to display in <code>--top-terms</code> (default: 20). *   <code>--corr-columns &lt;col1,col2&gt;</code>: Comma-separated numeric columns to use for <code>--corr-heatmap</code>; otherwise columns are auto-selected. *   <code>--graph-nodes &lt;types&gt;</code>: Comma-separated node types to include in graph viz: <code>document</code>, <code>keyword</code>, <code>cluster</code>, <code>metadata</code> (use <code>all</code> for every type). *   <code>--graph-layout &lt;name&gt;</code>: Layout algorithm for graph viz: <code>spring</code> (default), <code>circular</code>, <code>kamada_kawai</code>, or <code>spectral</code>.</p>"},{"location":"cheatsheet/#corpus-manipulation-crispt","title":"\ud83d\udee0 Corpus Manipulation (<code>crispt</code>)","text":"<p>Advanced tools for managing the corpus structure.</p>"},{"location":"cheatsheet/#search-query","title":"Search &amp; Query","text":"<ul> <li><code>--semantic \"query\"</code>: Semantic search for documents.</li> <li><code>--semantic-chunks \"query\"</code>: Search within specific document chunks (needs <code>--doc-id</code>).</li> <li><code>--similar-docs \"id1,id2\"</code>: Find documents similar to the provided IDs.</li> <li><code>--feature-search \"query\"</code>: Search for features/variables in the dataframe.</li> </ul> <p>Inspect &amp; Utilities: *   <code>--print</code>: Display the full corpus structure in a formatted view. *   <code>--df-cols</code>: List all DataFrame column names. *   <code>--df-row-count</code>: Show the number of rows in the DataFrame. *   <code>--df-row &lt;index&gt;</code>: Display a specific DataFrame row by index. *   <code>--doc-ids</code>: List all document IDs in the corpus. *   <code>--doc-id &lt;id&gt;</code>: Show details for a specific document by ID. *   <code>--remove-doc &lt;id&gt;</code>: Remove a document from the corpus by ID.</p> <p>Print Options: *   Two formats are supported for <code>--print</code>:     - Multiple flags: pass several <code>--print</code> flags, e.g. <code>--print documents --print 10</code>.     - Single string: pass the full command as one quoted argument, e.g. <code>--print \"documents 10\"</code> or <code>--print \"dataframe metadata\"</code>.</p> <p>Examples:</p> <pre><code># Print first 10 documents (two-flag form)\ncrispt --inp ./corpus --print documents --print 10\n\n# Print DataFrame metadata columns (single-string form)\ncrispt --inp ./corpus --print \"dataframe metadata\"\n</code></pre>"},{"location":"cheatsheet/#metadata-structure","title":"Metadata &amp; Structure","text":"<ul> <li><code>--id &lt;name&gt;</code>: Create new corpus with ID.</li> <li><code>--doc \"id|text\"</code>: Add a document manually.</li> <li><code>--meta \"key=val\"</code>: Add metadata to corpus.</li> <li><code>--add-rel \"A|B|rel\"</code>: Define relationship between data points.</li> <li><code>--temporal-link &lt;method&gt;</code>: Link docs to data by time (<code>nearest</code>, <code>window</code>, etc.).</li> <li><code>--clear-rel</code>: Remove all relationships from the corpus metadata.</li> <li><code>--relationships</code>: Print all relationships defined in the corpus.</li> <li><code>--relationships-for-keyword &lt;kw&gt;</code>: Print relationships involving a specific keyword.</li> <li><code>--metadata-df</code>: Export semantic-search collection metadata as a DataFrame.</li> <li><code>--metadata-keys &lt;keys&gt;</code>: Comma-separated metadata keys to include when exporting.</li> </ul>"},{"location":"cheatsheet/#advanced-analysis","title":"Advanced Analysis","text":"<ul> <li><code>--tdabm \"y:x1,x2:ranking\"</code>: Topological Data Analysis / Agent Based Modeling.</li> <li><code>--graph</code>: Generate graph network from keywords/metadata.</li> </ul> <p>Temporal &amp; Embedding Tools: *   <code>--temporal-summary &lt;period&gt;</code>: Generate temporal summary (<code>D</code>, <code>W</code>, <code>M</code>, <code>Y</code>). *   <code>--temporal-sentiment &lt;period:aggregation&gt;</code>: Analyze sentiment trends (e.g., <code>W:mean</code>). *   <code>--temporal-topics &lt;period:top_n&gt;</code>: Extract topics over time (e.g., <code>W:5</code>). *   <code>--temporal-subgraphs &lt;period&gt;</code>: Create time-sliced subgraphs (e.g., <code>W</code>). *   <code>--embedding-link &lt;metric:top_k:threshold&gt;</code>: Link by embedding similarity (e.g., <code>cosine:1:0.7</code>). *   <code>--embedding-stats</code>: Display statistics about embedding-based links. *   <code>--embedding-viz &lt;method:output_path&gt;</code>: Visualize embedding space (<code>tsne</code>, <code>pca</code>, <code>umap</code>).</p>"},{"location":"cheatsheet/#data-linking-filtering","title":"\ud83d\udd17 Data Linking &amp; Filtering","text":"<p>Linking Text to Numbers: *   Keyword Linking: Default. Matches text keywords to dataframe columns. *   ID Linking: Use <code>--linkage id</code> to match Document ID to a DataFrame column. *   Temporal Linking: <code>crispt --temporal-link \"window:timestamp:300\"</code> links by time proximity (in seconds). More details *   Embedding Linking: <code>crispt --embedding-link \"cosine:1:0.7\"</code> More details</p> <p>Filtering: *   <code>--filters key=value</code> or <code>--filters key:value</code>: Standard exact match filter. Both <code>=</code> and <code>:</code> are accepted as separators for key/value filters. *   Special link filters: <code>--filters embedding:text</code>, <code>--filters embedding:df</code>, <code>--filters temporal:text</code>, <code>--filters temporal:df</code> (you can also use <code>=</code> instead of <code>:</code>). These select by linked rows or documents using embedding or temporal links. *   Legacy shorthand mappings: <code>--filters =embedding</code> or <code>--filters :embedding</code> are mapped to <code>embedding:text</code>; <code>--filters =temporal</code> or <code>--filters :temporal</code> are mapped to <code>temporal:text</code>. *   ID linkage: <code>--filters id=&lt;value&gt;</code> filters to a specific document/row by ID. Using <code>--filters id:</code> or <code>--filters id=</code> with no value syncs remaining documents and dataframe rows by matching <code>id</code> values after other filters are applied. *   <code>--temporal-filter \"start:end\"</code>: Filter by time range (ISO format).</p>"},{"location":"cheatsheet/#common-workflows","title":"\ud83d\udca1 Common Workflows","text":""},{"location":"cheatsheet/#1-basic-qualitative-analysis","title":"1. Basic Qualitative Analysis","text":"<pre><code># Import\ncrisp --source ./interviews --out ./corpus\n# Analyze Topics &amp; Sentiment\ncrisp --inp ./corpus --topics --sentiment --out ./corpus_analyzed\n# Visualize\ncrispviz --inp ./corpus_analyzed --wordcloud --ldavis --out ./viz\n</code></pre>"},{"location":"cheatsheet/#2-mixed-methods-text-csv","title":"2. Mixed Methods (Text + CSV)","text":"<pre><code># Import CSV with text column\ncrisp --source ./survey_data --unstructured \"comments\" --out ./survey_corpus\n# Cluster numeric data &amp; analyze text options\ncrisp --inp ./survey_corpus --kmeans --num 5 --topics --num 5\n</code></pre>"},{"location":"cheatsheet/#3-semantic-search","title":"3. Semantic Search","text":"<pre><code># Find documents about specific topic\ncrispt --inp ./corpus --semantic \"patient anxiety\" --num 10\n</code></pre>"},{"location":"cheatsheet/#4-cross-modal-analysis-text-csv","title":"4. Cross-Modal Analysis (Text + CSV)","text":"<pre><code># Import CSV that contains free-text comments and numeric outcomes\ncrisp --source ./survey_data --unstructured \"comments\" --out ./survey_corpus\n\n# Link documents to rows by ID, then run classification using linked text as features\ncrisp --inp ./survey_corpus --linkage id --outcome satisfaction_score --cls --aggregation majority\n\n# Use embedding linking to aggregate document embeddings to rows then run regression\ncrispt --inp ./survey_corpus --filters embedding:text --out ./linked &amp;&amp; \\\ncrisp --inp ./linked --outcome satisfaction_score --regression\n</code></pre>"},{"location":"cheatsheet/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<ul> <li>\"No input data provided\": Ensure you used <code>--source</code> (to import) or <code>--inp</code> (to load existing).</li> <li>ML dependencies missing: Run <code>pip install crisp-t[ml]</code>.</li> <li>Visualization errors: Ensure you ran the prerequisite analysis step first (e.g., running <code>--topics</code> before <code>--wordcloud</code>).</li> <li>Caching issues: Use <code>--clear</code> to force a refresh if you change datasets or experience weird results.</li> </ul>"},{"location":"contributing/","title":"Contributing to <code>crisp-t</code>","text":""},{"location":"contributing/#please-note","title":"Please note:","text":"<ul> <li>(Optional) We adopt Git Flow. Most feature branches are pushed to the repository and deleted when merged to develop branch.</li> <li>(Important): Submit pull requests to the develop branch or feature/ branches</li> <li>Use GitHub Issues for feature requests and bug reports. Include as much information as possible while reporting bugs.</li> </ul>"},{"location":"contributing/#contributing-step-by-step","title":"Contributing (Step-by-step)","text":"<ol> <li> <p>Fork the repo and clone it to your local computer, and set up the upstream remote:</p> <pre><code>git clone https://github.com/dermatologist/crisp-t.git\ncd crisp-t\ngit remote add upstream https://github.com/dermatologist/crisp-t.git\n</code></pre> </li> <li> <p>Checkout out a new local branch based on your master and update it to the latest (BRANCH-123 is the branch name, You can name it whatever you want. Try to give it a meaningful name. If you are fixing an issue, please include the issue #).</p> <pre><code>git checkout -b BRANCH-123 develop\ngit clean -df\ngit pull --rebase upstream develop\n</code></pre> </li> </ol> <p>Please keep your code clean. If you find another bug, you want to fix while being in a new branch, please fix it in a separated branch instead.</p> <ol> <li> <p>Push the branch to your fork. Treat it as a backup.</p> <pre><code>git push origin BRANCH-123\n</code></pre> </li> <li> <p>Code</p> </li> <li> <p>Adhere to common conventions you see in the existing code.</p> </li> <li> <p>Include tests as much as possible, and ensure they pass.</p> </li> <li> <p>Commit to your branch</p> <pre><code> git commit -m \"BRANCH-123: Put change summary here (can be a ticket title)\"\n</code></pre> </li> </ol> <p>NEVER leave the commit message blank! Provide a detailed, clear, and complete description of your commit!</p> <ol> <li> <p>Update your branch to the latest code.</p> <pre><code>git pull --rebase upstream develop\n</code></pre> </li> <li> <p>Important If you have made many commits, please squash them into atomic units of work. (Most Git GUIs such as sourcetree and smartgit offer a squash option)</p> <pre><code>git checkout develop\ngit pull --rebase upstream develop\ngit merge --squash BRANCH-123\ngit commit -m \"fix: 123\"\n</code></pre> </li> </ol> <p>Push changes to your fork:</p> <pre><code>    git push\n</code></pre> <ol> <li>Issue a Pull Request</li> </ol> <p>In order to make a pull request:   * Click \"Pull Request\".   * Choose the develop branch   * Click 'Create pull request'   * Fill in some details about your potential patch including a meaningful title.   * Click \"Create pull request\".</p> <p>Thanks for that -- we'll get to your pull request ASAP. We love pull requests!</p>"},{"location":"contributing/#feedback","title":"Feedback","text":"<p>If you need to contact me, see my contact details on my profile page.</p>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#cli.main","title":"<code>main(verbose, covid, inp, out, csv, num, rec, unstructured, filters, codedict, topics, assign, cat, summary, sentiment, sentence, nlp, nnet, cls, knn, kmeans, cart, pca, regression, lstm, ml, visualize, ignore, include, outcome, linkage, aggregation, source, sources, print_args, clear)</code>","text":"<p>CRISP-T: Cross Industry Standard Process for Triangulation.</p> <p>A comprehensive framework for analyzing textual and numerical data using advanced NLP, machine learning, and statistical techniques. Designed for researchers and practitioners working with mixed-methods data.</p> <p>\b \ud83d\udcda GETTING STARTED - DATA PREPARATION:</p> <p>Step 1: Create a source directory (e.g., crisp_source) in your workspace</p> Add your data files to this directory: <p>\u2022 Text files: Interview transcripts, field notes (.txt or .pdf format) \u2022 Numeric data: One CSV file with quantitative data</p> Import your data to create a corpus: <p>crisp --source crisp_source --out crisp_input</p> Run analyses on your imported corpus: <p>crisp --inp crisp_input [analysis options]</p> <p>\b \ud83d\udca1 TIPS: \u2022 For CSV files with free-text columns, use --unstructured  \u2022 Use --help to see all available analysis options \u2022 Use --clear when switching between different datasets \u2022 Results can be saved at any stage using --out <p>\b \ud83d\udcd6 For detailed examples, see: docs/DEMO.md \ud83d\udcd6 For complete documentation, visit: https://github.com/dermatologist/crisp-t/wiki</p> Source code in <code>src/crisp_t/cli.py</code> <pre><code>@click.command()\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Show detailed progress and debugging information.\",\n)\n@click.option(\n    \"--covid\",\n    \"-cf\",\n    default=\"\",\n    help=\"Download COVID-19 narratives from the specified website.\",\n)\n@click.option(\n    \"--inp\",\n    \"-i\",\n    help=\"Load an existing corpus from a folder containing corpus.json and corpus_df.csv files.\",\n)\n@click.option(\n    \"--out\",\n    \"-o\",\n    help=\"Save the corpus to a folder. The corpus will be saved as corpus.json and corpus_df.csv.\",\n)\n@click.option(\n    \"--csv\",\n    default=\"\",\n    help=\"[Deprecated] CSV file name. Please place CSV in --source folder instead.\",\n)\n@click.option(\n    \"--num\",\n    \"-n\",\n    default=3,\n    help=\"Numerical parameter for analysis (e.g., number of clusters, topics, or epochs). When used with --source, limits the maximum number of text/PDF files to import. Default: 3.\",\n)\n@click.option(\n    \"--rec\",\n    \"-r\",\n    default=3,\n    help=\"Number of top results to display or record index for specific operations. When used with --source, limits the maximum number of CSV rows to import. Default: 3.\",\n)\n@click.option(\n    \"--unstructured\",\n    \"-t\",\n    multiple=True,\n    help=\"Specify CSV columns containing free-form text (e.g., open-ended survey responses). Can be used multiple times.\",\n)\n@click.option(\n    \"--filters\",\n    \"-f\",\n    multiple=True,\n    help=\"Filter documents/rows by metadata or links. Format: key=value (regular) or embedding:text/embedding:df/temporal:text/temporal:df (link filters). Can be used multiple times.\",\n)\n@click.option(\n    \"--codedict\",\n    is_flag=True,\n    help=\"Generate a qualitative coding dictionary from your text data.\",\n)\n@click.option(\n    \"--topics\",\n    is_flag=True,\n    help=\"Perform topic modeling using Latent Dirichlet Allocation (LDA).\",\n)\n@click.option(\n    \"--assign\", is_flag=True, help=\"Assign each document to its most relevant topic.\"\n)\n@click.option(\n    \"--cat\",\n    is_flag=True,\n    help=\"Extract and list common categories or themes from the corpus.\",\n)\n@click.option(\n    \"--summary\",\n    is_flag=True,\n    help=\"Generate an extractive summary of the text data.\",\n)\n@click.option(\n    \"--sentiment\",\n    is_flag=True,\n    help=\"Analyze sentiment in the text using VADER sentiment analysis.\",\n)\n@click.option(\n    \"--sentence\",\n    is_flag=True,\n    default=False,\n    help=\"Generate sentence-level analysis (use with --sentiment for document-level scores).\",\n)\n@click.option(\n    \"--nlp\",\n    is_flag=True,\n    help=\"Run all available natural language processing analyses (coding dictionary, topics, categories, summary, sentiment).\",\n)\n@click.option(\n    \"--ml\",\n    is_flag=True,\n    help=\"Run all available machine learning analyses (requires crisp-t[ml] installation).\",\n)\n@click.option(\n    \"--nnet\", is_flag=True, help=\"Train and evaluate a neural network classifier.\"\n)\n@click.option(\n    \"--cls\",\n    is_flag=True,\n    help=\"Perform classification using Support Vector Machine (SVM) and Decision Tree algorithms.\",\n)\n@click.option(\n    \"--knn\",\n    is_flag=True,\n    help=\"Perform K-Nearest Neighbors search to find similar records.\",\n)\n@click.option(\n    \"--kmeans\",\n    is_flag=True,\n    help=\"Perform K-Means clustering to group similar records.\",\n)\n@click.option(\n    \"--cart\",\n    is_flag=True,\n    help=\"Generate association rules using the Apriori algorithm (CART).\",\n)\n@click.option(\n    \"--pca\",\n    is_flag=True,\n    help=\"Perform Principal Component Analysis (PCA) for dimensionality reduction.\",\n)\n@click.option(\n    \"--regression\",\n    is_flag=True,\n    help=\"Perform regression analysis (linear or logistic, automatically detected).\",\n)\n@click.option(\n    \"--lstm\",\n    is_flag=True,\n    help=\"Train an LSTM (Long Short-Term Memory) neural network on text data to predict outcomes.\",\n)\n@click.option(\n    \"--visualize\",\n    is_flag=True,\n    help=\"Generate visualizations for words, topics, and word clouds.\",\n)\n@click.option(\n    \"--ignore\",\n    default=\"\",\n    help=\"Comma-separated list of words or columns to exclude from analysis.\",\n)\n@click.option(\n    \"--include\",\n    default=\"\",\n    help=\"Comma-separated list of columns to include in the analysis.\",\n)\n@click.option(\n    \"--outcome\",\n    default=\"\",\n    help=\"Specify the target variable (outcome) for machine learning tasks. Can be a DataFrame column name OR a text metadata field name (when --linkage is specified).\",\n)\n@click.option(\n    \"--linkage\",\n    type=click.Choice([\"id\", \"embedding\", \"temporal\", \"keyword\"], case_sensitive=False),\n    default=None,\n    help=\"Linkage method to use when outcome is a text metadata field. Choices: id, embedding, temporal, keyword.\",\n)\n@click.option(\n    \"--aggregation\",\n    type=click.Choice([\"majority\", \"mean\", \"first\", \"mode\"], case_sensitive=False),\n    default=\"majority\",\n    help=\"Aggregation strategy when multiple documents link to one row. Default: majority (for classification) or mean (for regression).\",\n)\n@click.option(\n    \"--source\",\n    \"--import\",\n    \"-s\",\n    help=\"Source directory or URL containing your data files (.txt, .pdf, and .csv).\",\n)\n@click.option(\n    \"--print\",\n    \"-p\",\n    \"print_args\",\n    multiple=True,\n    help=\"Display corpus information. Examples: --print documents --print 10, or --print 'documents 10'\",\n)\n@click.option(\n    \"--sources\",\n    multiple=True,\n    help=\"Load data from multiple sources (directories or URLs). Can be used multiple times.\",\n)\n@click.option(\n    \"--clear\",\n    is_flag=True,\n    help=\"Clear the cache before running analysis. Use when switching between datasets.\",\n)\ndef main(\n    verbose,\n    covid,\n    inp,\n    out,\n    csv,\n    num,\n    rec,\n    unstructured,\n    filters,\n    codedict,\n    topics,\n    assign,\n    cat,\n    summary,\n    sentiment,\n    sentence,\n    nlp,\n    nnet,\n    cls,\n    knn,\n    kmeans,\n    cart,\n    pca,\n    regression,\n    lstm,\n    ml,\n    visualize,\n    ignore,\n    include,\n    outcome,\n    linkage,\n    aggregation,\n    source,\n    sources,\n    print_args,\n    clear,\n):\n    \"\"\"CRISP-T: Cross Industry Standard Process for Triangulation.\n\n    A comprehensive framework for analyzing textual and numerical data using\n    advanced NLP, machine learning, and statistical techniques. Designed for\n    researchers and practitioners working with mixed-methods data.\n\n    \\b\n    \ud83d\udcda GETTING STARTED - DATA PREPARATION:\n\n    Step 1: Create a source directory (e.g., crisp_source) in your workspace\n\n    Step 2: Add your data files to this directory:\n       \u2022 Text files: Interview transcripts, field notes (.txt or .pdf format)\n       \u2022 Numeric data: One CSV file with quantitative data\n\n    Step 3: Import your data to create a corpus:\n       crisp --source crisp_source --out crisp_input\n\n    Step 4: Run analyses on your imported corpus:\n       crisp --inp crisp_input [analysis options]\n\n    \\b\n    \ud83d\udca1 TIPS:\n    \u2022 For CSV files with free-text columns, use --unstructured &lt;column_name&gt;\n    \u2022 Use --help to see all available analysis options\n    \u2022 Use --clear when switching between different datasets\n    \u2022 Results can be saved at any stage using --out\n\n    \\b\n    \ud83d\udcd6 For detailed examples, see: docs/DEMO.md\n    \ud83d\udcd6 For complete documentation, visit: https://github.com/dermatologist/crisp-t/wiki\n    \"\"\"\n\n    if verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n        click.echo(click.style(\"\u2713 Verbose mode enabled\", fg=\"cyan\"))\n\n    # Display banner with colors\n    click.echo(click.style(\"\\n\" + \"=\" * 60, fg=\"blue\", bold=True))\n    click.echo(\n        click.style(\"CRISP-T\", fg=\"green\", bold=True)\n        + click.style(\" - Qualitative Research Analysis Framework\", fg=\"white\")\n    )\n    click.echo(click.style(f\"Version: {__version__}\", fg=\"cyan\"))\n    click.echo(click.style(\"=\" * 60 + \"\\n\", fg=\"blue\", bold=True))\n\n    # Initialize components\n    read_data = ReadData()\n    corpus = None\n    text_analyzer = None\n    csv_analyzer = None\n    ml_analyzer = None\n\n    if clear:\n        clear_cache()\n\n    try:\n        # Handle COVID data download\n        if covid:\n            if not source:\n                raise click.ClickException(\n                    format_error(\n                        \"--source (output folder) is required when using --covid.\"\n                    )\n                )\n            click.echo(click.style(\"\\n\ud83d\udce5 Downloading COVID narratives...\", fg=\"yellow\"))\n            click.echo(f\"   From: {click.style(covid, fg='cyan')}\")\n            click.echo(f\"   To: {click.style(source, fg='cyan')}\")\n            try:\n                from .utils import QRUtils\n\n                QRUtils.read_covid_narratives(source, covid)\n                click.echo(\n                    format_success(\n                        f\"Successfully downloaded COVID narratives to {source}\"\n                    )\n                )\n            except Exception as e:\n                raise click.ClickException(format_error(f\"Download failed: {e}\")) from e\n\n        # Build corpus using helpers (source preferred over inp)\n        # if not source or inp, use default folders or env vars\n        try:\n            text_cols = \",\".join(unstructured) if unstructured else \"\"\n            # When using --source, num and rec are used for import limits\n            max_text_files = num if source and num &gt; 3 else None\n            max_csv_rows = rec if source and rec &gt; 3 else None\n            corpus = initialize_corpus(\n                source=source,\n                inp=inp,\n                comma_separated_text_columns=text_cols,\n                comma_separated_ignore_words=(ignore if ignore else None),\n                max_text_files=max_text_files,\n                max_csv_rows=max_csv_rows,\n            )\n            # If filters were provided with ':' while using --source, emit guidance message\n            if (\n                source\n                and filters\n                and any(\":\" in flt and \"=\" not in flt for flt in filters)\n            ):\n                click.echo(format_info(\"Filters are not supported when using --source\"))\n        except click.ClickException:\n            raise\n        except Exception as e:\n            click.echo(\n                format_error(f\"Error initializing corpus: {e}\"),\n                err=True,\n            )\n            logger.exception(f\"Failed to initialize corpus: {e}\")\n            return\n\n        # Handle multiple sources (unchanged behavior, but no filters applied here)\n        if sources and not corpus:\n            click.echo(\n                click.style(\"\\n\ud83d\udce5 Loading data from multiple sources...\", fg=\"yellow\")\n            )\n            loaded_any = False\n            # When using --sources, num and rec are used for import limits\n            max_text_files = num if sources and num &gt; 3 else None\n            max_csv_rows = rec if sources and rec &gt; 3 else None\n            for src in sources:\n                click.echo(f\"   \u2022 Reading from: {click.style(src, fg='cyan')}\")\n                try:\n                    read_data.read_source(\n                        src,\n                        comma_separated_ignore_words=ignore if ignore else None,\n                        max_text_files=max_text_files,\n                        max_csv_rows=max_csv_rows,\n                    )\n                    loaded_any = True\n                    click.echo(format_success(\"Loaded successfully\", indent=5))\n                except Exception as e:\n                    logger.exception(f\"Failed to read source {src}: {e}\")\n                    raise click.ClickException(\n                        format_error(f\"Failed to load source: {e}\")\n                    ) from e\n\n            if loaded_any:\n                corpus = read_data.create_corpus(\n                    name=\"Corpus from multiple sources\",\n                    description=f\"Data loaded from {len(sources)} sources\",\n                )\n                click.echo(\n                    click.style(\n                        f\"\\n\u2713 Successfully loaded {len(corpus.documents)} document(s) from {len(sources)} source(s)\",\n                        fg=\"green\",\n                        bold=True,\n                    )\n                )\n                # Filters are not applied for --sources in bulk mode\n\n        # Initialize analyzers with unified filter logic\n        if corpus:\n            try:\n                text_cols = \",\".join(unstructured) if unstructured else \"\"\n                text_analyzer, csv_analyzer = get_analyzers(\n                    corpus,\n                    comma_separated_unstructured_text_columns=text_cols,\n                    comma_separated_ignore_columns=(ignore if ignore else \"\"),\n                    filters=filters,\n                )\n            except Exception as e:\n                click.echo(\n                    click.style(\n                        \"\u274c Error initializing analyzers: \", fg=\"red\", bold=True\n                    )\n                    + str(e),\n                    err=True,\n                )\n                logger.exception(f\"Failed to initialize analyzers: {e}\")\n                return\n\n        # Load CSV data (deprecated)\n        if csv:\n            click.echo(\n                click.style(\"\u26a0\ufe0f  Warning: \", fg=\"yellow\", bold=True)\n                + \"--csv option has been deprecated. Put csv file in --source folder instead.\"\n            )\n\n        # Initialize ML analyzer if available and ML functions are requested\n        if (\n            ML_AVAILABLE\n            and (\n                nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml\n            )\n            and csv_analyzer\n        ):\n            if include:\n                # Ensure outcome variable is included in the filter if specified\n                if outcome and outcome not in include:\n                    include = include + \",\" + outcome\n                csv_analyzer.comma_separated_include_columns(include)\n            ml_analyzer = ML(csv=csv_analyzer)  # type: ignore\n        else:\n            if (\n                nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml\n            ) and not ML_AVAILABLE:\n                click.echo(\n                    click.style(\n                        \"\u26a0\ufe0f  Machine Learning features require additional dependencies.\",\n                        fg=\"yellow\",\n                    )\n                )\n                click.echo(\n                    click.style(\"   Install with: \", fg=\"white\")\n                    + click.style(\"pip install crisp-t[ml]\", fg=\"cyan\", bold=True)\n                )\n            if (\n                nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml\n            ) and not csv_analyzer:\n                click.echo(\n                    click.style(\"\u26a0\ufe0f  ML analysis requires CSV data. \", fg=\"yellow\")\n                    + \"Use --csv to provide a data file or include CSV in --source folder.\"\n                )\n\n        # Ensure we have data to work with\n        if not corpus and not csv_analyzer:\n            click.echo(\n                click.style(\"\\n\u26a0\ufe0f  No input data provided.\", fg=\"yellow\", bold=True)\n            )\n            click.echo(click.style(\"\\n\ud83d\udca1 Quick Start Guide:\", fg=\"cyan\", bold=True))\n            click.echo(\"   1. Place your data files in a folder (e.g., crisp_source)\")\n            click.echo(\"   2. Import the data:\")\n            click.echo(\n                click.style(\n                    \"      crisp --source crisp_source --out crisp_input\", fg=\"green\"\n                )\n            )\n            click.echo(\"   3. Run analyses:\")\n            click.echo(\n                click.style(\n                    \"      crisp --inp crisp_input --topics --sentiment\", fg=\"green\"\n                )\n            )\n            click.echo(\n                \"\\n   Run \"\n                + click.style(\"crisp --help\", fg=\"cyan\")\n                + \" for all options\\n\"\n            )\n            return\n\n        # Text Analysis Operations\n        if text_analyzer:\n            if nlp or codedict:\n                print_section_header(\n                    \"CODING DICTIONARY GENERATION\", emoji=\"\ud83d\udcd6\", color=\"blue\"\n                )\n                click.echo(\n                    click.style(\"\\nWhat is a Coding Dictionary?\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\n                    \"   A structured representation of your qualitative data organized into:\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 CATEGORY:\", fg=\"yellow\")\n                    + \" Main actions or themes (common verbs)\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 PROPERTY:\", fg=\"yellow\")\n                    + \" Concepts associated with each category (common nouns)\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 DIMENSION:\", fg=\"yellow\")\n                    + \" Characteristics of each property (adjectives, adverbs)\"\n                )\n                click.echo(\n                    click.style(\"\\n\ud83d\udca1 Tips for Better Results:\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--ignore', fg='green')} with common words you want to exclude\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--filters', fg='green')} to focus on specific document subsets\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--num', fg='green')} to control the number of categories shown\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--rec', fg='green')} to control items displayed per section\\n\"\n                )\n                try:\n                    text_analyzer.make_spacy_doc()\n                    coding_dict = text_analyzer.print_coding_dictionary(\n                        num=num, top_n=rec\n                    )\n                    if out:\n                        _save_output(coding_dict, out, \"coding_dictionary\")\n                        click.echo(\n                            click.style(\n                                \"\\n\u2713 Coding dictionary saved successfully\", fg=\"green\"\n                            )\n                        )\n                except Exception as e:\n                    click.echo(\n                        click.style(\n                            \"\\n\u274c Error generating coding dictionary: \",\n                            fg=\"red\",\n                            bold=True,\n                        )\n                        + str(e)\n                    )\n\n            if nlp or topics:\n                print_section_header(\"TOPIC MODELING (LDA)\", emoji=\"\ud83c\udfaf\", color=\"blue\")\n                click.echo(\n                    click.style(\"\\nWhat is Topic Modeling?\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\"   Discovers hidden thematic structure in your text data.\")\n                click.echo(\n                    \"   Each topic is represented as a weighted combination of words.\"\n                )\n                click.echo(click.style(\"\\n\ud83d\udcca Output Format:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    '   Topic 0: 0.116*\"category\" + 0.093*\"comparison\" + 0.070*\"incident\" + ...'\n                )\n                click.echo(\"   (Higher weights = more important words for that topic)\")\n                click.echo(click.style(\"\\n\ud83d\udca1 Tips:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   \u2022 Use {click.style('--num', fg='green')} to set the number of topics (default: 3)\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--rec', fg='green')} to control words shown per topic\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--filters', fg='green')} to analyze specific document subsets\\n\"\n                )\n                try:\n                    cluster_analyzer = Cluster(corpus=corpus)\n                    cluster_analyzer.build_lda_model(topics=num)\n                    topics_result = cluster_analyzer.print_topics(num_words=rec)\n                    click.echo(\n                        click.style(\n                            f\"\\n\u2713 Generated {len(topics_result)} topics with weights shown above\",\n                            fg=\"green\",\n                        )\n                    )\n                    if out:\n                        _save_output(topics_result, out, \"topics\")\n                        click.echo(format_success(\"Topics saved successfully\"))\n                except Exception as e:\n                    click.echo(format_error(f\"Error generating topics: {e}\"))\n\n            if nlp or assign:\n                print_section_header(\n                    \"DOCUMENT-TOPIC ASSIGNMENTS\", emoji=\"\ud83d\udccc\", color=\"blue\"\n                )\n                click.echo(click.style(\"\\nWhat does this do?\", fg=\"cyan\", bold=True))\n                click.echo(\n                    \"   Assigns each document to its most relevant topic based on content similarity.\"\n                )\n                click.echo(\"   Shows the contribution percentage for each assignment.\")\n                click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   Use {click.style('--visualize', fg='green')} to prepare data for visualization\\n\"\n                )\n                try:\n                    if \"cluster_analyzer\" not in locals():\n                        cluster_analyzer = Cluster(corpus=corpus)\n                        cluster_analyzer.build_lda_model(topics=num)\n                    assignments = cluster_analyzer.format_topics_sentences(\n                        visualize=visualize\n                    )\n                    document_assignments = cluster_analyzer.print_clusters()\n                    click.echo(\n                        click.style(\n                            f\"\\n\u2713 Assigned {len(assignments)} documents to topics\",\n                            fg=\"green\",\n                        )\n                    )\n                    if out:\n                        _save_output(assignments, out, \"topic_assignments\")\n                        click.echo(format_success(\"Assignments saved successfully\"))\n                except Exception as e:\n                    click.echo(format_error(f\"Error assigning topics: {e}\"))\n\n            if nlp or cat:\n                print_section_header(\"CATEGORY ANALYSIS\", emoji=\"\ud83c\udff7\ufe0f\", color=\"blue\")\n                click.echo(\n                    click.style(\"\\nWhat is Category Analysis?\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\"   Extracts common concepts and themes from your corpus.\")\n                click.echo(\n                    \"   Results shown as a 'bag of terms' with importance weights.\"\n                )\n                click.echo(click.style(\"\\n\ud83d\udca1 Tips:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   \u2022 Use {click.style('--num', fg='green')} to adjust the number of categories\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--filters', fg='green')} to focus on specific documents\\n\"\n                )\n                try:\n                    text_analyzer.make_spacy_doc()\n                    categories = text_analyzer.print_categories(num=num)\n                    if out:\n                        _save_output(categories, out, \"categories\")\n                        click.echo(format_success(\"Categories saved successfully\"))\n                except Exception as e:\n                    click.echo(\n                        click.style(\n                            \"\\n\u274c Error generating categories: \", fg=\"red\", bold=True\n                        )\n                        + str(e)\n                    )\n\n            if nlp or summary:\n                print_section_header(\"TEXT SUMMARIZATION\", emoji=\"\ud83d\udcdd\", color=\"blue\")\n                click.echo(\n                    click.style(\"\\nWhat is Text Summarization?\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\n                    \"   Generates an extractive summary by selecting the most important\"\n                )\n                click.echo(\"   sentences that represent the main points of your text.\")\n                click.echo(click.style(\"\\n\ud83d\udca1 Tips:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   \u2022 Use {click.style('--num', fg='green')} to control summary length (number of sentences)\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--filters', fg='green')} to summarize specific document subsets\\n\"\n                )\n                try:\n                    text_analyzer.make_spacy_doc()\n                    summary_result = text_analyzer.generate_summary(weight=num)\n                    click.echo(summary_result)\n                    if out:\n                        _save_output(summary_result, out, \"summary\")\n                        click.echo(format_success(\"Summary saved successfully\"))\n                except Exception as e:\n                    click.echo(format_error(f\"Error generating summary: {e}\"))\n\n            if nlp or sentiment:\n                print_section_header(\n                    \"SENTIMENT ANALYSIS (VADER)\", emoji=\"\ud83d\ude0a\", color=\"blue\"\n                )\n                click.echo(\n                    click.style(\"\\nWhat is Sentiment Analysis?\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\"   Analyzes the emotional tone of your text using VADER\")\n                click.echo(\"   (Valence Aware Dictionary and sEntiment Reasoner).\")\n                click.echo(click.style(\"\\n\ud83d\udcca Output Scores:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    click.style(\"   \u2022 neg:\", fg=\"red\")\n                    + \" Negative sentiment (0.0 to 1.0)\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 neu:\", fg=\"yellow\")\n                    + \" Neutral sentiment (0.0 to 1.0)\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 pos:\", fg=\"green\")\n                    + \" Positive sentiment (0.0 to 1.0)\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 compound:\", fg=\"cyan\")\n                    + \" Overall sentiment (-1.0 to +1.0)\"\n                )\n                click.echo(click.style(\"\\n\ud83d\udca1 Tips:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   \u2022 Use {click.style('--sentence', fg='green')} for document-level scores\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--filters', fg='green')} to analyze specific documents\\n\"\n                )\n                try:\n                    sentiment_analyzer = Sentiment(corpus=corpus)  # type: ignore\n                    sentiment_results = sentiment_analyzer.get_sentiment(\n                        documents=sentence, verbose=verbose\n                    )\n                    click.echo(sentiment_results)\n                    if out:\n                        _save_output(sentiment_results, out, \"sentiment\")\n                        click.echo(\n                            format_success(\"Sentiment analysis saved successfully\")\n                        )\n                except Exception as e:\n                    click.echo(\n                        format_error(f\"Error generating sentiment analysis: {e}\")\n                    )\n\n        # Machine Learning Operations\n        if ml_analyzer and ML_AVAILABLE:\n            target_col = outcome\n\n            if kmeans or ml:\n                print_section_header(\"K-MEANS CLUSTERING\", emoji=\"\ud83d\udd0d\", color=\"magenta\")\n                click.echo(\n                    click.style(\"\\nWhat is K-Means Clustering?\", fg=\"cyan\", bold=True)\n                )\n                click.echo(\n                    \"   Groups similar records together based on numeric features.\"\n                )\n                click.echo(\n                    \"   Automatically removes non-numeric columns and missing values.\"\n                )\n                click.echo(click.style(\"\\n\u26a0\ufe0f  Important:\", fg=\"yellow\", bold=True))\n                click.echo(\n                    \"   Data preprocessing may affect compatibility with other ML analyses.\"\n                )\n                click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   Use {click.style('--num', fg='green')} to set the number of clusters (default: 3)\\n\"\n                )\n                csv_analyzer.retain_numeric_columns_only()\n                csv_analyzer.drop_na()\n                _ml_analyzer = ML(csv=csv_analyzer)\n                clusters, members = _ml_analyzer.get_kmeans(\n                    number_of_clusters=num, verbose=verbose\n                )\n                _ml_analyzer.profile(members, number_of_clusters=num)\n                click.echo(\n                    click.style(\n                        f\"\\n\u2713 Clustering complete: {num} clusters generated\", fg=\"green\"\n                    )\n                )\n                if out:\n                    _save_output(\n                        {\"clusters\": clusters, \"members\": members}, out, \"kmeans\"\n                    )\n                    click.echo(format_success(\"Results saved successfully\"))\n\n            if (cls or ml) and target_col:\n                print_section_header(\n                    \"CLASSIFICATION MODELS\", emoji=\"\ud83c\udfaf\", color=\"magenta\"\n                )\n                click.echo(\n                    click.style(\n                        \"\\nWhat are Classification Models?\", fg=\"cyan\", bold=True\n                    )\n                )\n                click.echo(\n                    \"   Predict categorical outcomes using machine learning algorithms:\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 SVM:\", fg=\"yellow\")\n                    + \" Support Vector Machine with confusion matrix\"\n                )\n                click.echo(\n                    click.style(\"   \u2022 Decision Tree:\", fg=\"yellow\")\n                    + \" Tree-based classifier with feature importance\"\n                )\n                click.echo(click.style(\"\\n\ud83d\udca1 Tips:\", fg=\"cyan\", bold=True))\n                click.echo(\n                    f\"   \u2022 Use {click.style('--outcome', fg='green')} to specify your target variable\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--rec', fg='green')} to control top features displayed\"\n                )\n                click.echo(\n                    f\"   \u2022 Use {click.style('--include', fg='green')} to select specific features\\n\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        click.style(\"\u274c Error: \", fg=\"red\", bold=True)\n                        + \"--outcome is required for classification tasks\"\n                    )\n                click.echo(\n                    click.style(\"\\n\u25b8 Running SVM Classification...\", fg=\"yellow\")\n                )\n                try:\n                    confusion_matrix = ml_analyzer.svm_confusion_matrix(\n                        y=target_col,\n                        test_size=0.25,\n                        linkage_method=linkage,\n                        aggregation=aggregation,\n                    )\n                    click.echo(\n                        ml_analyzer.format_confusion_matrix_to_human_readable(\n                            confusion_matrix\n                        )\n                    )\n                    click.echo(format_success(\"SVM classification complete\", indent=2))\n                    if out:\n                        _save_output(confusion_matrix, out, \"svm_results\")\n                except Exception as e:\n                    click.echo(format_error(f\"Error in SVM: {e}\", indent=2))\n                click.echo(\n                    click.style(\n                        \"\\n\u25b8 Running Decision Tree Classification...\", fg=\"yellow\"\n                    )\n                )\n                try:\n                    cm, importance = ml_analyzer.get_decision_tree_classes(\n                        y=target_col,\n                        top_n=rec,\n                        linkage_method=linkage,\n                        aggregation=aggregation,\n                    )\n                    click.echo(\n                        \"\\n\" + click.style(\"Feature Importance:\", fg=\"cyan\", bold=True)\n                    )\n                    click.echo(\n                        ml_analyzer.format_confusion_matrix_to_human_readable(cm)\n                    )\n                    click.echo(\n                        format_success(\n                            \"Decision tree classification complete\", indent=2\n                        )\n                    )\n                    if out:\n                        _save_output(cm, out, \"decision_tree_results\")\n                except Exception as e:\n                    click.echo(format_error(f\"Error in Decision Tree: {e}\", indent=2))\n\n            if nnet or ml:\n                click.echo(\"\\n=== Neural Network Classification Accuracy ===\")\n                click.echo(\n                    \"\"\"\n                            Neural Network classifier with accuracy output.\n                Hint:   Use --outcome to specify the target variable for classification.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for neural network tasks\"\n                    )\n                try:\n                    predictions = ml_analyzer.get_nnet_predictions(\n                        y=target_col, linkage_method=linkage, aggregation=aggregation\n                    )\n                    if out:\n                        _save_output(predictions, out, \"nnet_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing Neural Network classification: {e}\")\n\n            if knn or ml:\n                click.echo(\"\\n=== K-Nearest Neighbors ===\")\n                click.echo(\n                    \"\"\"\n                           K-Nearest Neighbors search results.\n                Hint:   Use --outcome to specify the target variable for KNN search.\n                        Use --rec to specify the record number to search from (1-based index).\n                        Use --num to specify the number of nearest neighbors to retrieve.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for KNN search tasks\"\n                    )\n                if rec &lt; 1:\n                    raise click.ClickException(\n                        \"--rec must be a positive integer (1-based index)\"\n                    )\n                try:\n                    knn_results = ml_analyzer.knn_search(\n                        y=target_col,\n                        n=num,\n                        r=rec,\n                        linkage_method=linkage,\n                        aggregation=aggregation,\n                    )\n                    if out:\n                        _save_output(knn_results, out, \"knn_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing K-Nearest Neighbors search: {e}\")\n\n            if cart or ml:\n                click.echo(\"\\n=== Association Rules (CART) ===\")\n                click.echo(\n                    \"\"\"\n                           Association Rules using the Apriori algorithm.\n                Hint:   Use --outcome to specify the target variable to remove from features.\n                        Use --num to specify the minimum support (between 1 and 99).\n                        Use --rec to specify the minimum threshold for the rules (between 1 and 99).\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for association rules tasks\"\n                    )\n                if not (1 &lt;= num &lt;= 99):\n                    raise click.ClickException(\n                        \"--num must be between 1 and 99 for min_support\"\n                    )\n                if not (1 &lt;= rec &lt;= 99):\n                    raise click.ClickException(\n                        \"--rec must be between 1 and 99 for min_threshold\"\n                    )\n                _min_support = float(num / 100)\n                _min_threshold = float(rec / 100)\n                click.echo(\n                    f\"Using min_support={_min_support:.2f} and min_threshold={_min_threshold:.2f}\"\n                )\n                try:\n                    apriori_results = ml_analyzer.get_apriori(\n                        y=target_col,\n                        min_support=_min_support,\n                        min_threshold=_min_threshold,\n                    )\n                    click.echo(apriori_results)\n                    if out:\n                        _save_output(apriori_results, out, \"association_rules\")\n                except Exception as e:\n                    click.echo(f\"Error generating association rules: {e}\")\n\n            if (pca or ml) and target_col:\n                click.echo(\"\\n=== Principal Component Analysis ===\")\n                click.echo(\n                    \"\"\"\n                           Principal Component Analysis (PCA) results.\n                Hint:   Use --outcome to specify the target variable to remove from features.\n                        Use --num to specify the number of principal components to generate.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                try:\n                    pca_results = ml_analyzer.get_pca(\n                        y=target_col,\n                        n=num,\n                        linkage_method=linkage,\n                        aggregation=aggregation,\n                    )\n                    if out:\n                        _save_output(pca_results, out, \"pca_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing Principal Component Analysis: {e}\")\n\n            if (regression or ml) and target_col:\n                click.echo(\"\\n=== Regression Analysis ===\")\n                click.echo(\n                    \"\"\"\n                           Regression Analysis (Linear or Logistic Regression).\n                           Automatically detects binary outcomes for logistic regression.\n                           Otherwise uses linear regression for continuous outcomes.\n                Hint:   Use --outcome to specify the target variable for regression.\n                        Use --include to specify columns to include in the analysis (comma separated).\n                \"\"\"\n                )\n                try:\n                    regression_results = ml_analyzer.get_regression(\n                        y=target_col, linkage_method=linkage, aggregation=aggregation\n                    )\n                    if out:\n                        _save_output(regression_results, out, \"regression_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing regression analysis: {e}\")\n\n            if lstm or ml:\n                click.echo(\"\\n=== LSTM Text Classification ===\")\n                click.echo(\n                    \"\"\"\n                           LSTM (Long Short-Term Memory) model for text-based prediction.\n                           Tests if text documents converge towards predicting the outcome variable.\n                           Requires both text documents and an 'id' column to align texts with outcome.\n                Hint:   Use --outcome to specify the target variable for LSTM prediction.\n                        The outcome should be binary (two classes).\n                        Ensure documents have IDs matching the 'id' column in your data.\n                \"\"\"\n                )\n                if not target_col:\n                    raise click.ClickException(\n                        \"--outcome is required for LSTM prediction tasks\"\n                    )\n                try:\n                    lstm_results = ml_analyzer.get_lstm_predictions(y=target_col)\n                    if out:\n                        _save_output(lstm_results, out, \"lstm_results\")\n                except Exception as e:\n                    click.echo(f\"Error performing LSTM prediction: {e}\")\n\n        elif (\n            nnet or cls or knn or kmeans or cart or pca or regression or lstm or ml\n        ) and not ML_AVAILABLE:\n            click.echo(\n                click.style(\n                    \"\\n\u26a0\ufe0f  Machine Learning features are not installed.\",\n                    fg=\"yellow\",\n                    bold=True,\n                )\n            )\n            click.echo(\n                click.style(\"   Install with: \", fg=\"white\")\n                + click.style(\"pip install crisp-t[ml]\", fg=\"cyan\", bold=True)\n            )\n\n        # Save corpus and csv if output path is specified\n        if out and corpus:\n            if filters and inp and out and inp == out:\n                raise click.ClickException(\n                    click.style(\"\u274c Error: \", fg=\"red\", bold=True)\n                    + \"--out cannot be the same as --inp when using --filters. \"\n                    + \"Please specify a different output folder to avoid overwriting input data.\"\n                )\n            if filters and ((not inp) or (not out)):\n                raise click.ClickException(\n                    format_error(\n                        \"Both --inp and --out must be specified when using --filters.\"\n                    )\n                )\n            output_path = pathlib.Path(out)\n            # Allow both directory and a file path '.../corpus.json'\n            if output_path.suffix:\n                # Ensure parent exists\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n                save_base = output_path\n            else:\n                output_path.mkdir(parents=True, exist_ok=True)\n                save_base = output_path / \"corpus.json\"\n            read_data.write_corpus_to_json(str(save_base), corpus=corpus)\n            click.echo(\n                format_success(\n                    f\"Corpus saved to: {click.style(str(save_base), fg='cyan')}\"\n                )\n            )\n\n        if print_args and corpus:\n            print_section_header(\"CORPUS DETAILS\", emoji=\"\ud83d\udcca\", color=\"blue\")\n            # Join the print arguments into a single string\n            print_command = \" \".join(print_args) if print_args else None\n            if print_command:\n                click.echo(corpus.pretty_print(show=print_command))\n\n        click.echo(click.style(\"\\n\" + \"=\" * 60, fg=\"green\", bold=True))\n        click.echo(format_success(\"Analysis Complete!\"))\n        click.echo(click.style(\"=\" * 60 + \"\\n\", fg=\"green\", bold=True))\n\n    except click.ClickException:\n        # Let Click handle and set non-zero exit code\n        raise\n    except Exception as e:\n        # Convert unexpected exceptions to ClickException for non-zero exit code\n        if verbose:\n            import traceback\n\n            traceback.print_exc()\n        raise click.ClickException(str(e)) from e\n</code></pre> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p> <p>Copyright (C) 2025 Bell Eapen</p> <p>This file is part of crisp-t.</p> <p>crisp-t is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>crisp-t is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with crisp-t.  If not, see https://www.gnu.org/licenses/.</p>"},{"location":"modules/#read_data.ReadData","title":"<code>ReadData</code>","text":"Source code in <code>src/crisp_t/read_data.py</code> <pre><code>class ReadData:\n\n    def __init__(self, corpus: Corpus | None = None, source=None):\n        self._corpus = corpus\n        self._source = source\n        self._documents = []\n        self._df = pd.DataFrame()\n\n    @property\n    def corpus(self):\n        \"\"\"\n        Get the corpus.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        self._corpus.documents = self._documents\n        self._corpus.df = self._df\n        return self._corpus\n\n    @property\n    def documents(self):\n        \"\"\"\n        Get the documents.\n        \"\"\"\n        if not self._documents:\n            raise ValueError(\"No documents found. Please read data first.\")\n        return self._documents\n\n    @property\n    def df(self):\n        \"\"\"\n        Get the dataframe.\n        \"\"\"\n        if self._df is None:\n            raise ValueError(\"No dataframe found. Please read data first.\")\n        return self._df\n\n    @corpus.setter\n    def corpus(self, value):\n        \"\"\"\n        Set the corpus.\n        \"\"\"\n        if not isinstance(value, Corpus):\n            raise ValueError(\"Value must be a Corpus object.\")\n        self._corpus = value\n\n    @documents.setter\n    def documents(self, value):\n        \"\"\"\n        Set the documents.\n        \"\"\"\n        if not isinstance(value, list):\n            raise ValueError(\"Value must be a list of Document objects.\")\n        for document in value:\n            if not isinstance(document, Document):\n                raise ValueError(\"Value must be a list of Document objects.\")\n        self._documents = value\n\n    @df.setter\n    def df(self, value):\n        \"\"\"\n        Set the dataframe.\n        \"\"\"\n        if not isinstance(value, pd.DataFrame):\n            raise ValueError(\"Value must be a pandas DataFrame.\")\n        self._df = value\n\n    def pretty_print(self):\n        \"\"\"\n        Pretty print the corpus.\n        \"\"\"\n        if not self._corpus:\n            self.create_corpus()\n        if self._corpus:\n            print(\n                self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"})\n            )\n            logger.info(\n                \"Corpus: %s\",\n                self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"}),\n            )\n        else:\n            logger.error(\"No corpus available to pretty print.\")\n\n    # TODO: Enforce only one corpus (Singleton pattern)\n    def create_corpus(self, name=None, description=None):\n        \"\"\"\n        Create a corpus from the documents and dataframe.\n        \"\"\"\n        if not self._documents:\n            raise ValueError(\"No documents found. Please read data first.\")\n        if self._corpus:\n            self._corpus.documents = self._documents\n            self._corpus.df = self._df\n        else:\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n            corpus_id = f\"corpus_{timestamp}\"\n            self._corpus = Corpus(\n                documents=self._documents,\n                df=self._df,\n                visualization={},\n                metadata={},\n                id=corpus_id,\n                score=0.0,\n                name=name,\n                description=description,\n            )\n        return self._corpus\n\n    def get_documents_from_corpus(self):\n        \"\"\"\n        Get the documents from the corpus.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        return self._corpus.documents\n\n    def get_document_by_id(self, doc_id):\n        \"\"\"\n        Get a document from the corpus by its ID. Uses parallel search for large corpora.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        documents = self._corpus.documents\n        if len(documents) &lt; 10:\n            for document in tqdm(documents, desc=\"Searching documents\", disable=True):\n                if document.id == doc_id:\n                    return document\n        else:\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(lambda doc: doc.id == doc_id, document): i\n                    for i, document in enumerate(documents)\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Searching documents (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        i = futures[future]\n                        found = future.result()\n                        pbar.update(1)\n                        if found:\n                            return documents[i]\n        raise ValueError(\"Document not found: %s\" % doc_id)\n\n    def write_corpus_to_json(self, file_path=\"\", corpus=None):\n        \"\"\"\n        Write the corpus to a json file.\n\n        Accepts either a directory path or an explicit file path ending with\n        'corpus.json'. In both cases, a sibling 'corpus_df.csv' will be written\n        next to the json if a DataFrame is available.\n        \"\"\"\n        from pathlib import Path\n\n        path = Path(file_path)\n        # Determine targets\n        if path.suffix:  # treat as explicit file path\n            file_name = path\n            df_name = path.with_name(\"corpus_df.csv\")\n        else:\n            file_name = path / \"corpus.json\"\n            df_name = path / \"corpus_df.csv\"\n\n        corp = corpus if corpus is not None else self._corpus\n        if not corp:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        file_name.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_name, \"w\") as f:\n            json.dump(corp.model_dump(exclude={\"df\", \"visualization\"}), f, indent=4)\n        if (\n            corp.df is not None\n            and isinstance(corp.df, pd.DataFrame)\n            and not corp.df.empty\n        ):\n            corp.df.to_csv(df_name, index=False)\n        logger.info(\"Corpus written to %s\", file_name)\n\n    # @lru_cache(maxsize=3)\n    def read_corpus_from_json(self, file_path=\"\", comma_separated_ignore_words=\"\"):\n        \"\"\"\n        Read the corpus from a json file. Parallelizes ignore word removal for large corpora.\n        \"\"\"\n        from pathlib import Path\n\n        file_path = Path(file_path)\n        file_name = file_path / \"corpus.json\"\n        df_name = file_path / \"corpus_df.csv\"\n        if self._source:\n            file_name = Path(self._source) / file_name\n        if not file_name.exists():\n            raise ValueError(f\"File not found: {file_name}\")\n        with open(file_name) as f:\n            data = json.load(f)\n            self._corpus = Corpus.model_validate(data)\n            logger.info(f\"Corpus read from {file_name}\")\n        if df_name.exists():\n            self._corpus.df = pd.read_csv(df_name)\n        else:\n            self._corpus.df = None\n        # Remove ignore words from self._corpus.documents text\n        documents = self._corpus.documents\n\n        # Pre-compile regex patterns once for efficiency instead of inside loops\n        compiled_patterns = []\n        if comma_separated_ignore_words:\n            for word in comma_separated_ignore_words.split(\",\"):\n                pattern = re.compile(r\"\\b\" + word.strip() + r\"\\b\", flags=re.IGNORECASE)\n                compiled_patterns.append(pattern)\n\n        if len(documents) &lt; 10:\n            processed_docs = []\n            for document in tqdm(documents, desc=\"Processing documents\", disable=True):\n                for pattern in compiled_patterns:\n                    document.text = pattern.sub(\"\", document.text)\n                processed_docs.append(document)\n        else:\n\n            def process_doc(document):\n                for pattern in compiled_patterns:\n                    document.text = pattern.sub(\"\", document.text)\n                return document\n\n            processed_docs = []\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(process_doc, document): document\n                    for document in documents\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Processing documents (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        processed_docs.append(future.result())\n                        pbar.update(1)\n        self._corpus.documents = processed_docs\n        return self._corpus\n\n    # @lru_cache(maxsize=3)\n    def read_csv_to_corpus(\n        self,\n        file_name,\n        comma_separated_ignore_words=None,\n        comma_separated_text_columns=\"\",\n        id_column=\"\",\n        timestamp_column=\"\",\n        max_rows=None,\n    ):\n        \"\"\"\n        Read the corpus from a csv file. Parallelizes document creation for large CSVs.\n        Handles invalid UTF-8 byte sequences by skipping bad lines.\n\n        Args:\n            file_name: Path to CSV file\n            comma_separated_ignore_words: Stop words to ignore\n            comma_separated_text_columns: Text columns to extract\n            id_column: Column to use as document ID\n            timestamp_column: Column containing timestamps (auto-detected if not specified)\n            max_rows: Maximum number of rows to read from CSV\n        \"\"\"\n        from pathlib import Path\n\n        file_name = Path(file_name)\n        if not file_name.exists():\n            raise ValueError(f\"File not found: {file_name}\")\n\n        # Read CSV with optional row limit, handling encoding errors\n        if max_rows is not None:\n            df = pd.read_csv(\n                file_name, nrows=max_rows, encoding=\"utf-8\", on_bad_lines=\"skip\"\n            )\n            logger.info(f\"Limited CSV reading to first {max_rows} rows\")\n        else:\n            df = pd.read_csv(file_name, encoding=\"utf-8\", on_bad_lines=\"skip\")\n        original_df = df.copy()\n\n        # Auto-detect timestamp column if not specified\n        if not timestamp_column:\n            timestamp_candidates = [\n                \"timestamp\",\n                \"datetime\",\n                \"time\",\n                \"date\",\n                \"created_at\",\n                \"updated_at\",\n            ]\n            for candidate in timestamp_candidates:\n                if candidate in df.columns:\n                    timestamp_column = candidate\n                    logger.info(f\"Auto-detected timestamp column: {timestamp_column}\")\n                    break\n\n        # Parse timestamp column to datetime if it exists\n        if timestamp_column and timestamp_column in df.columns:\n            try:\n                df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n                logger.info(f\"Parsed timestamp column '{timestamp_column}' to datetime\")\n            except Exception as e:\n                logger.warning(\n                    f\"Could not parse timestamp column '{timestamp_column}': {e}\"\n                )\n\n        if comma_separated_text_columns:\n            text_columns = comma_separated_text_columns.split(\",\")\n        else:\n            text_columns = []\n        # remove text columns from the dataframe\n        for column in text_columns:\n            if column in df.columns:\n                df.drop(column, axis=1, inplace=True)\n        # Set self._df to the numeric part after dropping text columns\n        self._df = df.copy()\n        rows = list(original_df.iterrows())\n\n        # Pre-compile regex patterns once for efficiency instead of inside loops\n        compiled_patterns = []\n        if comma_separated_ignore_words:\n            for word in comma_separated_ignore_words.split(\",\"):\n                pattern = re.compile(r\"\\b\" + word.strip() + r\"\\b\", flags=re.IGNORECASE)\n                compiled_patterns.append(pattern)\n\n        def create_document(args):\n            index, row = args\n            # Use list and join for efficient string concatenation, handle None values\n            text_parts = [\n                (\n                    str(row[column])\n                    if row[column] is not None\n                    and not (\n                        isinstance(row[column], float) and row[column] != row[column]\n                    )\n                    else \"\"\n                )\n                for column in text_columns\n            ]\n            read_from_file = \" \".join(text_parts)\n            # Apply pre-compiled patterns\n            for pattern in compiled_patterns:\n                read_from_file = pattern.sub(\"\", read_from_file)\n\n            # Extract timestamp if available\n            doc_timestamp = None\n            if timestamp_column and timestamp_column in original_df.columns:\n                ts_value = row[timestamp_column]\n                if ts_value is not None and not pd.isna(ts_value):\n                    # Convert to ISO 8601 string\n                    try:\n                        if isinstance(ts_value, pd.Timestamp):\n                            doc_timestamp = ts_value.isoformat()\n                        else:\n                            doc_timestamp = pd.to_datetime(ts_value).isoformat()\n                    except Exception as exc:\n                        # Intentionally ignore timestamp parsing errors and fall back to no timestamp,\n                        # but log them for diagnostic purposes.\n                        logging.debug(\n                            \"Failed to parse timestamp value %r in column %r: %s\",\n                            ts_value,\n                            timestamp_column,\n                            exc,\n                        )\n\n            _document = Document(\n                text=read_from_file,\n                timestamp=doc_timestamp,\n                metadata={\n                    \"source\": str(file_name),\n                    \"file_name\": str(file_name),\n                    \"row\": index,\n                    \"id\": (\n                        row[id_column]\n                        if (id_column != \"\" and id_column in original_df.columns)\n                        else index\n                    ),\n                },\n                id=str(index),\n                score=0.0,\n                name=\"\",\n                description=\"\",\n            )\n            return read_from_file, _document\n\n        if len(rows) &lt; 10:\n            results = [\n                create_document(args)\n                for args in tqdm(rows, desc=\"Reading CSV rows\", disable=True)\n            ]\n        else:\n\n            results = []\n            # import multiprocessing\n\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(create_document, args): args for args in rows\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Reading CSV rows (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        results.append(future.result())\n                        pbar.update(1)\n\n        if len(results) &lt; 10:\n            for read_from_file, _document in tqdm(\n                results, desc=\"Finalizing corpus\", disable=True\n            ):\n                self._documents.append(_document)\n        else:\n\n            # import multiprocessing\n\n            n_cores = multiprocessing.cpu_count()\n            with tqdm(\n                results,\n                total=len(results),\n                desc=f\"Finalizing corpus (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for read_from_file, _document in pbar:\n                    self._documents.append(_document)\n        logger.info(f\"Corpus read from {file_name}\")\n        self.create_corpus()\n        return self._corpus\n\n    def read_source(\n        self,\n        source,\n        comma_separated_ignore_words=None,\n        comma_separated_text_columns=\"\",\n        max_text_files=None,\n        max_csv_rows=None,\n    ):\n        _CSV_EXISTS = False\n\n        # Pre-compile regex patterns once for efficiency instead of inside loops\n        compiled_patterns = []\n        if comma_separated_ignore_words:\n            for word in comma_separated_ignore_words.split(\",\"):\n                pattern = re.compile(r\"\\b\" + word.strip() + r\"\\b\", flags=re.IGNORECASE)\n                compiled_patterns.append(pattern)\n\n        def apply_ignore_patterns(text):\n            \"\"\"Apply pre-compiled ignore patterns to text.\"\"\"\n            for pattern in compiled_patterns:\n                text = pattern.sub(\"\", text)\n            return text\n\n        # if source is a url\n        if source.startswith(\"http://\") or source.startswith(\"https://\"):\n            response = requests.get(source)\n            if response.status_code == 200:\n                read_from_file = response.text\n                read_from_file = apply_ignore_patterns(read_from_file)\n                # Extract timestamp from content\n                doc_timestamp = extract_timestamp_from_text(read_from_file)\n                # self._content removed\n                _document = Document(\n                    text=read_from_file,\n                    timestamp=doc_timestamp,\n                    metadata={\"source\": source},\n                    id=source,\n                    score=0.0,\n                    name=\"\",\n                    description=\"\",\n                )\n                self._documents.append(_document)\n        elif os.path.exists(source):\n            source_path = Path(source)\n            self._source = source\n            logger.info(f\"Reading data from folder: {source}\")\n            file_list = os.listdir(source)\n\n            # Separate files by type for limiting\n            text_files = [f for f in file_list if f.endswith(\".txt\")]\n            pdf_files = [f for f in file_list if f.endswith(\".pdf\")]\n            csv_files = [f for f in file_list if f.endswith(\".csv\")]\n\n            # Apply limits to text and PDF files\n            text_pdf_count = 0\n            text_pdf_limit = (\n                max_text_files if max_text_files is not None else float(\"inf\")\n            )\n\n            if max_text_files is not None:\n                logger.info(\n                    f\"Limiting import to maximum {max_text_files} text/PDF files\"\n                )\n\n            # Process text files\n            for file_name in tqdm(\n                text_files, desc=\"Reading text files\", disable=len(text_files) &lt; 10\n            ):\n                if text_pdf_count &gt;= text_pdf_limit:\n                    break\n\n                file_path = source_path / file_name\n                with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n                    read_from_file = f.read()\n                    read_from_file = apply_ignore_patterns(read_from_file)\n                    # Extract timestamp from content\n                    doc_timestamp = extract_timestamp_from_text(read_from_file)\n                    # Extract tag from filename if it contains - or _\n                    tag = extract_tag_from_filename(file_name)\n                    # self._content removed\n                    metadata = {\n                        \"source\": str(file_path),\n                        \"file_name\": file_name,\n                    }\n                    if tag:\n                        metadata[\"tag\"] = tag\n                    _document = Document(\n                        text=read_from_file,\n                        timestamp=doc_timestamp,\n                        metadata=metadata,\n                        id=file_name,\n                        score=0.0,\n                        name=\"\",\n                        description=\"\",\n                    )\n                    self._documents.append(_document)\n                    text_pdf_count += 1\n\n            # Process PDF files\n            for file_name in tqdm(\n                pdf_files, desc=\"Reading PDF files\", disable=len(pdf_files) &lt; 10\n            ):\n                if text_pdf_count &gt;= text_pdf_limit:\n                    break\n\n                file_path = source_path / file_name\n                with open(file_path, \"rb\") as f:\n                    reader = PdfReader(f)\n                    # Use list and join for efficient string concatenation\n                    page_texts = []\n                    for page in tqdm(\n                        reader.pages,\n                        desc=f\"Reading PDF {file_name}\",\n                        leave=False,\n                        disable=len(reader.pages) &lt; 10,\n                    ):\n                        page_text = page.extract_text()\n                        # Decode and re-encode to remove invalid UTF-8 sequences\n                        if isinstance(page_text, str):\n                            page_text = page_text.encode(\n                                \"utf-8\", errors=\"ignore\"\n                            ).decode(\"utf-8\")\n                        page_texts.append(page_text)\n                    read_from_file = \"\".join(page_texts)\n                    read_from_file = apply_ignore_patterns(read_from_file)\n                    # Extract timestamp from content\n                    doc_timestamp = extract_timestamp_from_text(read_from_file)\n                    # Extract tag from filename if it contains - or _\n                    tag = extract_tag_from_filename(file_name)\n                    # self._content removed\n                    metadata = {\n                        \"source\": str(file_path),\n                        \"file_name\": file_name,\n                    }\n                    if tag:\n                        metadata[\"tag\"] = tag\n                    _document = Document(\n                        text=read_from_file,\n                        timestamp=doc_timestamp,\n                        metadata=metadata,\n                        id=file_name,\n                        score=0.0,\n                        name=\"\",\n                        description=\"\",\n                    )\n                    self._documents.append(_document)\n                    text_pdf_count += 1\n\n            # Process CSV files\n            for file_name in csv_files:\n                file_path = source_path / file_name\n                if comma_separated_text_columns == \"\":\n                    logger.info(f\"Reading CSV file: {file_path}\")\n                    self._df = Csv().read_csv(file_path)\n                    # Apply row limit if specified\n                    if max_csv_rows is not None and len(self._df) &gt; max_csv_rows:\n                        logger.info(f\"Limiting CSV to first {max_csv_rows} rows\")\n                        self._df = self._df.head(max_csv_rows)\n                    logger.info(f\"CSV file read with shape: {self._df.shape}\")\n                    _CSV_EXISTS = True\n                if comma_separated_text_columns != \"\":\n                    logger.info(f\"Reading CSV file to corpus: {file_path}\")\n                    self.read_csv_to_corpus(\n                        file_path,\n                        comma_separated_ignore_words,\n                        comma_separated_text_columns,\n                        max_rows=max_csv_rows,\n                    )\n                    logger.info(\n                        f\"CSV file read to corpus with documents: {len(self._documents)}\"\n                    )\n                    _CSV_EXISTS = True\n            if not _CSV_EXISTS:\n                # create a simple csv with columns: id, number, text\n                # and fill it with random data\n                _csv = \"\"\"\nid,number,response\n1,100,Sample text one\n2,200,Sample text two\n3,300,Sample text three\n4,400,Sample text four\n\"\"\"\n                # write the csv to a temp file\n                with tempfile.NamedTemporaryFile(\n                    mode=\"w+\", delete=False, suffix=\".csv\"\n                ) as temp_csv:\n                    temp_csv.write(_csv)\n                    temp_csv_path = temp_csv.name\n                logger.info(f\"No CSV found. Created temp CSV file: {temp_csv_path}\")\n                self._df = Csv().read_csv(temp_csv_path)\n                logger.info(f\"CSV file read with shape: {self._df.shape}\")\n                # remove the temp file\n                os.remove(temp_csv_path)\n\n        else:\n            raise ValueError(f\"Source not found: {source}\")\n\n    def corpus_as_dataframe(self):\n        \"\"\"\n        Convert the corpus to a pandas dataframe. Parallelizes for large corpora.\n        \"\"\"\n        if not self._corpus:\n            raise ValueError(\"No corpus found. Please create a corpus first.\")\n        documents = self._corpus.documents\n        if len(documents) &lt; 10:\n            data = [\n                document.model_dump()\n                for document in tqdm(\n                    documents, desc=\"Converting to dataframe\", disable=True\n                )\n            ]\n        else:\n            data = []\n\n            def dump_doc(document):\n                return document.model_dump()\n\n            n_cores = multiprocessing.cpu_count()\n            with ThreadPoolExecutor() as executor:\n                futures = {\n                    executor.submit(dump_doc, document): document\n                    for document in documents\n                }\n                with tqdm(\n                    total=len(futures),\n                    desc=f\"Converting to dataframe (parallel, {n_cores} cores)\",\n                ) as pbar:\n                    for future in as_completed(futures):\n                        data.append(future.result())\n                        pbar.update(1)\n        df = pd.DataFrame(data)\n        return df\n</code></pre>"},{"location":"modules/#read_data.ReadData.corpus","title":"<code>corpus</code>  <code>property</code> <code>writable</code>","text":"<p>Get the corpus.</p>"},{"location":"modules/#read_data.ReadData.df","title":"<code>df</code>  <code>property</code> <code>writable</code>","text":"<p>Get the dataframe.</p>"},{"location":"modules/#read_data.ReadData.documents","title":"<code>documents</code>  <code>property</code> <code>writable</code>","text":"<p>Get the documents.</p>"},{"location":"modules/#read_data.ReadData.corpus_as_dataframe","title":"<code>corpus_as_dataframe()</code>","text":"<p>Convert the corpus to a pandas dataframe. Parallelizes for large corpora.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def corpus_as_dataframe(self):\n    \"\"\"\n    Convert the corpus to a pandas dataframe. Parallelizes for large corpora.\n    \"\"\"\n    if not self._corpus:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    documents = self._corpus.documents\n    if len(documents) &lt; 10:\n        data = [\n            document.model_dump()\n            for document in tqdm(\n                documents, desc=\"Converting to dataframe\", disable=True\n            )\n        ]\n    else:\n        data = []\n\n        def dump_doc(document):\n            return document.model_dump()\n\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(dump_doc, document): document\n                for document in documents\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Converting to dataframe (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    data.append(future.result())\n                    pbar.update(1)\n    df = pd.DataFrame(data)\n    return df\n</code></pre>"},{"location":"modules/#read_data.ReadData.create_corpus","title":"<code>create_corpus(name=None, description=None)</code>","text":"<p>Create a corpus from the documents and dataframe.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def create_corpus(self, name=None, description=None):\n    \"\"\"\n    Create a corpus from the documents and dataframe.\n    \"\"\"\n    if not self._documents:\n        raise ValueError(\"No documents found. Please read data first.\")\n    if self._corpus:\n        self._corpus.documents = self._documents\n        self._corpus.df = self._df\n    else:\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n        corpus_id = f\"corpus_{timestamp}\"\n        self._corpus = Corpus(\n            documents=self._documents,\n            df=self._df,\n            visualization={},\n            metadata={},\n            id=corpus_id,\n            score=0.0,\n            name=name,\n            description=description,\n        )\n    return self._corpus\n</code></pre>"},{"location":"modules/#read_data.ReadData.get_document_by_id","title":"<code>get_document_by_id(doc_id)</code>","text":"<p>Get a document from the corpus by its ID. Uses parallel search for large corpora.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def get_document_by_id(self, doc_id):\n    \"\"\"\n    Get a document from the corpus by its ID. Uses parallel search for large corpora.\n    \"\"\"\n    if not self._corpus:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    documents = self._corpus.documents\n    if len(documents) &lt; 10:\n        for document in tqdm(documents, desc=\"Searching documents\", disable=True):\n            if document.id == doc_id:\n                return document\n    else:\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(lambda doc: doc.id == doc_id, document): i\n                for i, document in enumerate(documents)\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Searching documents (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    i = futures[future]\n                    found = future.result()\n                    pbar.update(1)\n                    if found:\n                        return documents[i]\n    raise ValueError(\"Document not found: %s\" % doc_id)\n</code></pre>"},{"location":"modules/#read_data.ReadData.get_documents_from_corpus","title":"<code>get_documents_from_corpus()</code>","text":"<p>Get the documents from the corpus.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def get_documents_from_corpus(self):\n    \"\"\"\n    Get the documents from the corpus.\n    \"\"\"\n    if not self._corpus:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    return self._corpus.documents\n</code></pre>"},{"location":"modules/#read_data.ReadData.pretty_print","title":"<code>pretty_print()</code>","text":"<p>Pretty print the corpus.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def pretty_print(self):\n    \"\"\"\n    Pretty print the corpus.\n    \"\"\"\n    if not self._corpus:\n        self.create_corpus()\n    if self._corpus:\n        print(\n            self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"})\n        )\n        logger.info(\n            \"Corpus: %s\",\n            self._corpus.model_dump_json(indent=4, exclude={\"df\", \"visualization\"}),\n        )\n    else:\n        logger.error(\"No corpus available to pretty print.\")\n</code></pre>"},{"location":"modules/#read_data.ReadData.read_corpus_from_json","title":"<code>read_corpus_from_json(file_path='', comma_separated_ignore_words='')</code>","text":"<p>Read the corpus from a json file. Parallelizes ignore word removal for large corpora.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def read_corpus_from_json(self, file_path=\"\", comma_separated_ignore_words=\"\"):\n    \"\"\"\n    Read the corpus from a json file. Parallelizes ignore word removal for large corpora.\n    \"\"\"\n    from pathlib import Path\n\n    file_path = Path(file_path)\n    file_name = file_path / \"corpus.json\"\n    df_name = file_path / \"corpus_df.csv\"\n    if self._source:\n        file_name = Path(self._source) / file_name\n    if not file_name.exists():\n        raise ValueError(f\"File not found: {file_name}\")\n    with open(file_name) as f:\n        data = json.load(f)\n        self._corpus = Corpus.model_validate(data)\n        logger.info(f\"Corpus read from {file_name}\")\n    if df_name.exists():\n        self._corpus.df = pd.read_csv(df_name)\n    else:\n        self._corpus.df = None\n    # Remove ignore words from self._corpus.documents text\n    documents = self._corpus.documents\n\n    # Pre-compile regex patterns once for efficiency instead of inside loops\n    compiled_patterns = []\n    if comma_separated_ignore_words:\n        for word in comma_separated_ignore_words.split(\",\"):\n            pattern = re.compile(r\"\\b\" + word.strip() + r\"\\b\", flags=re.IGNORECASE)\n            compiled_patterns.append(pattern)\n\n    if len(documents) &lt; 10:\n        processed_docs = []\n        for document in tqdm(documents, desc=\"Processing documents\", disable=True):\n            for pattern in compiled_patterns:\n                document.text = pattern.sub(\"\", document.text)\n            processed_docs.append(document)\n    else:\n\n        def process_doc(document):\n            for pattern in compiled_patterns:\n                document.text = pattern.sub(\"\", document.text)\n            return document\n\n        processed_docs = []\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(process_doc, document): document\n                for document in documents\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Processing documents (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    processed_docs.append(future.result())\n                    pbar.update(1)\n    self._corpus.documents = processed_docs\n    return self._corpus\n</code></pre>"},{"location":"modules/#read_data.ReadData.read_csv_to_corpus","title":"<code>read_csv_to_corpus(file_name, comma_separated_ignore_words=None, comma_separated_text_columns='', id_column='', timestamp_column='', max_rows=None)</code>","text":"<p>Read the corpus from a csv file. Parallelizes document creation for large CSVs. Handles invalid UTF-8 byte sequences by skipping bad lines.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>Path to CSV file</p> required <code>comma_separated_ignore_words</code> <p>Stop words to ignore</p> <code>None</code> <code>comma_separated_text_columns</code> <p>Text columns to extract</p> <code>''</code> <code>id_column</code> <p>Column to use as document ID</p> <code>''</code> <code>timestamp_column</code> <p>Column containing timestamps (auto-detected if not specified)</p> <code>''</code> <code>max_rows</code> <p>Maximum number of rows to read from CSV</p> <code>None</code> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def read_csv_to_corpus(\n    self,\n    file_name,\n    comma_separated_ignore_words=None,\n    comma_separated_text_columns=\"\",\n    id_column=\"\",\n    timestamp_column=\"\",\n    max_rows=None,\n):\n    \"\"\"\n    Read the corpus from a csv file. Parallelizes document creation for large CSVs.\n    Handles invalid UTF-8 byte sequences by skipping bad lines.\n\n    Args:\n        file_name: Path to CSV file\n        comma_separated_ignore_words: Stop words to ignore\n        comma_separated_text_columns: Text columns to extract\n        id_column: Column to use as document ID\n        timestamp_column: Column containing timestamps (auto-detected if not specified)\n        max_rows: Maximum number of rows to read from CSV\n    \"\"\"\n    from pathlib import Path\n\n    file_name = Path(file_name)\n    if not file_name.exists():\n        raise ValueError(f\"File not found: {file_name}\")\n\n    # Read CSV with optional row limit, handling encoding errors\n    if max_rows is not None:\n        df = pd.read_csv(\n            file_name, nrows=max_rows, encoding=\"utf-8\", on_bad_lines=\"skip\"\n        )\n        logger.info(f\"Limited CSV reading to first {max_rows} rows\")\n    else:\n        df = pd.read_csv(file_name, encoding=\"utf-8\", on_bad_lines=\"skip\")\n    original_df = df.copy()\n\n    # Auto-detect timestamp column if not specified\n    if not timestamp_column:\n        timestamp_candidates = [\n            \"timestamp\",\n            \"datetime\",\n            \"time\",\n            \"date\",\n            \"created_at\",\n            \"updated_at\",\n        ]\n        for candidate in timestamp_candidates:\n            if candidate in df.columns:\n                timestamp_column = candidate\n                logger.info(f\"Auto-detected timestamp column: {timestamp_column}\")\n                break\n\n    # Parse timestamp column to datetime if it exists\n    if timestamp_column and timestamp_column in df.columns:\n        try:\n            df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n            logger.info(f\"Parsed timestamp column '{timestamp_column}' to datetime\")\n        except Exception as e:\n            logger.warning(\n                f\"Could not parse timestamp column '{timestamp_column}': {e}\"\n            )\n\n    if comma_separated_text_columns:\n        text_columns = comma_separated_text_columns.split(\",\")\n    else:\n        text_columns = []\n    # remove text columns from the dataframe\n    for column in text_columns:\n        if column in df.columns:\n            df.drop(column, axis=1, inplace=True)\n    # Set self._df to the numeric part after dropping text columns\n    self._df = df.copy()\n    rows = list(original_df.iterrows())\n\n    # Pre-compile regex patterns once for efficiency instead of inside loops\n    compiled_patterns = []\n    if comma_separated_ignore_words:\n        for word in comma_separated_ignore_words.split(\",\"):\n            pattern = re.compile(r\"\\b\" + word.strip() + r\"\\b\", flags=re.IGNORECASE)\n            compiled_patterns.append(pattern)\n\n    def create_document(args):\n        index, row = args\n        # Use list and join for efficient string concatenation, handle None values\n        text_parts = [\n            (\n                str(row[column])\n                if row[column] is not None\n                and not (\n                    isinstance(row[column], float) and row[column] != row[column]\n                )\n                else \"\"\n            )\n            for column in text_columns\n        ]\n        read_from_file = \" \".join(text_parts)\n        # Apply pre-compiled patterns\n        for pattern in compiled_patterns:\n            read_from_file = pattern.sub(\"\", read_from_file)\n\n        # Extract timestamp if available\n        doc_timestamp = None\n        if timestamp_column and timestamp_column in original_df.columns:\n            ts_value = row[timestamp_column]\n            if ts_value is not None and not pd.isna(ts_value):\n                # Convert to ISO 8601 string\n                try:\n                    if isinstance(ts_value, pd.Timestamp):\n                        doc_timestamp = ts_value.isoformat()\n                    else:\n                        doc_timestamp = pd.to_datetime(ts_value).isoformat()\n                except Exception as exc:\n                    # Intentionally ignore timestamp parsing errors and fall back to no timestamp,\n                    # but log them for diagnostic purposes.\n                    logging.debug(\n                        \"Failed to parse timestamp value %r in column %r: %s\",\n                        ts_value,\n                        timestamp_column,\n                        exc,\n                    )\n\n        _document = Document(\n            text=read_from_file,\n            timestamp=doc_timestamp,\n            metadata={\n                \"source\": str(file_name),\n                \"file_name\": str(file_name),\n                \"row\": index,\n                \"id\": (\n                    row[id_column]\n                    if (id_column != \"\" and id_column in original_df.columns)\n                    else index\n                ),\n            },\n            id=str(index),\n            score=0.0,\n            name=\"\",\n            description=\"\",\n        )\n        return read_from_file, _document\n\n    if len(rows) &lt; 10:\n        results = [\n            create_document(args)\n            for args in tqdm(rows, desc=\"Reading CSV rows\", disable=True)\n        ]\n    else:\n\n        results = []\n        # import multiprocessing\n\n        n_cores = multiprocessing.cpu_count()\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(create_document, args): args for args in rows\n            }\n            with tqdm(\n                total=len(futures),\n                desc=f\"Reading CSV rows (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    results.append(future.result())\n                    pbar.update(1)\n\n    if len(results) &lt; 10:\n        for read_from_file, _document in tqdm(\n            results, desc=\"Finalizing corpus\", disable=True\n        ):\n            self._documents.append(_document)\n    else:\n\n        # import multiprocessing\n\n        n_cores = multiprocessing.cpu_count()\n        with tqdm(\n            results,\n            total=len(results),\n            desc=f\"Finalizing corpus (parallel, {n_cores} cores)\",\n        ) as pbar:\n            for read_from_file, _document in pbar:\n                self._documents.append(_document)\n    logger.info(f\"Corpus read from {file_name}\")\n    self.create_corpus()\n    return self._corpus\n</code></pre>"},{"location":"modules/#read_data.ReadData.write_corpus_to_json","title":"<code>write_corpus_to_json(file_path='', corpus=None)</code>","text":"<p>Write the corpus to a json file.</p> <p>Accepts either a directory path or an explicit file path ending with 'corpus.json'. In both cases, a sibling 'corpus_df.csv' will be written next to the json if a DataFrame is available.</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def write_corpus_to_json(self, file_path=\"\", corpus=None):\n    \"\"\"\n    Write the corpus to a json file.\n\n    Accepts either a directory path or an explicit file path ending with\n    'corpus.json'. In both cases, a sibling 'corpus_df.csv' will be written\n    next to the json if a DataFrame is available.\n    \"\"\"\n    from pathlib import Path\n\n    path = Path(file_path)\n    # Determine targets\n    if path.suffix:  # treat as explicit file path\n        file_name = path\n        df_name = path.with_name(\"corpus_df.csv\")\n    else:\n        file_name = path / \"corpus.json\"\n        df_name = path / \"corpus_df.csv\"\n\n    corp = corpus if corpus is not None else self._corpus\n    if not corp:\n        raise ValueError(\"No corpus found. Please create a corpus first.\")\n    file_name.parent.mkdir(parents=True, exist_ok=True)\n    with open(file_name, \"w\") as f:\n        json.dump(corp.model_dump(exclude={\"df\", \"visualization\"}), f, indent=4)\n    if (\n        corp.df is not None\n        and isinstance(corp.df, pd.DataFrame)\n        and not corp.df.empty\n    ):\n        corp.df.to_csv(df_name, index=False)\n    logger.info(\"Corpus written to %s\", file_name)\n</code></pre>"},{"location":"modules/#read_data.extract_tag_from_filename","title":"<code>extract_tag_from_filename(filename)</code>","text":"<p>Extract a tag from a filename by splitting on - or _ and taking the first part.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename to extract tag from (e.g., \"interview-1.txt\")</p> required <p>Returns:</p> Name Type Description <code>str | None</code> <p>The tag (first part before - or _) or None if no separator found.</p> <code>str | None</code> <p>The tag is the part before the first - or _ character.</p> <code>Example</code> <code>str | None</code> <p>\"interview-1.txt\" -&gt; \"interview\", \"report_2025.pdf\" -&gt; \"report\"</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def extract_tag_from_filename(filename: str) -&gt; str | None:\n    \"\"\"\n    Extract a tag from a filename by splitting on - or _ and taking the first part.\n\n    Args:\n        filename: The filename to extract tag from (e.g., \"interview-1.txt\")\n\n    Returns:\n        The tag (first part before - or _) or None if no separator found.\n        The tag is the part before the first - or _ character.\n        Example: \"interview-1.txt\" -&gt; \"interview\", \"report_2025.pdf\" -&gt; \"report\"\n    \"\"\"\n    if not filename:\n        return None\n\n    # Remove file extension\n    name_without_ext = Path(filename).stem\n\n    # Split on - or _ and get the first part\n    if \"-\" in name_without_ext:\n        tag = name_without_ext.split(\"-\")[0]\n        return tag if tag else None\n    elif \"_\" in name_without_ext:\n        tag = name_without_ext.split(\"_\")[0]\n        return tag if tag else None\n\n    return None\n</code></pre>"},{"location":"modules/#read_data.extract_timestamp_from_text","title":"<code>extract_timestamp_from_text(text)</code>","text":"<p>Extract the first occurrence of a timestamp from text. Supports ISO 8601 format and common date formats.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to search for timestamps</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>ISO 8601 formatted timestamp string or None if not found</p> Source code in <code>src/crisp_t/read_data.py</code> <pre><code>def extract_timestamp_from_text(text: str) -&gt; str | None:\n    \"\"\"\n    Extract the first occurrence of a timestamp from text.\n    Supports ISO 8601 format and common date formats.\n\n    Args:\n        text: Text to search for timestamps\n\n    Returns:\n        ISO 8601 formatted timestamp string or None if not found\n    \"\"\"\n    if not text:\n        return None\n\n    # Pattern for ISO 8601: YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS or variants\n    iso_pattern = (\n        r\"\\d{4}-\\d{2}-\\d{2}(?:T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{3})?(?:Z|[+-]\\d{2}:\\d{2})?)?\"\n    )\n    iso_match = re.search(iso_pattern, text)\n    if iso_match:\n        timestamp_str = iso_match.group(0)\n        try:\n            # Parse and return as ISO 8601 string\n            dt = pd.to_datetime(timestamp_str)\n            return dt.isoformat()\n        except Exception:\n            pass\n\n    # Pattern for common date formats: MM/DD/YYYY or DD/MM/YYYY\n    common_pattern = r\"\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}\"\n    common_match = re.search(common_pattern, text)\n    if common_match:\n        timestamp_str = common_match.group(0)\n        try:\n            dt = pd.to_datetime(timestamp_str)\n            return dt.isoformat()\n        except Exception:\n            pass\n\n    return None\n</code></pre>"},{"location":"modules/#corpuscli.main","title":"<code>main(verbose, id, name, description, docs, remove_docs, metas, relationships, clear_rel, print_corpus, out, inp, df_cols, df_row_count, df_row, doc_ids, doc_id, print_relationships, relationships_for_keyword, semantic, similar_docs, num, semantic_chunks, rec, metadata_df, metadata_keys, tdabm, graph, temporal_link, temporal_filter, temporal_summary, temporal_sentiment, temporal_topics, temporal_subgraphs, embedding_link, embedding_stats, embedding_viz)</code>","text":"<p>CRISP-T Corpus CLI: Create and manipulate corpus data from the command line.</p> <p>This tool allows you to create, modify, query, and manage corpus objects without writing any Python code. Perfect for quick data exploration and corpus management tasks.</p> <p>\b \ud83d\udcda COMMON TASKS:</p> Create a new corpus <p>crispt --id my_corpus --name \"My Research\" --doc \"1|Doc1|Hello world\"</p> Load and modify an existing corpus <p>crispt --inp crisp_input --doc \"2|Doc2|More text\" --out crisp_input</p> Query corpus information <p>crispt --inp crisp_input --doc-ids --df-cols</p> Perform semantic search <p>crispt --inp crisp_input --semantic \"find this topic\" --num 10</p> <p>\b \ud83d\udca1 TIPS: \u2022 Use --print to view the full corpus structure \u2022 Combine multiple operations in a single command \u2022 Use --out to save changes after modifications \u2022 TDABM and graph features require prior analysis</p> <p>\b \ud83d\udcd6 For more examples, see: docs/DEMO.md</p> Source code in <code>src/crisp_t/corpuscli.py</code> <pre><code>@click.command()\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Show detailed progress and debugging information.\",\n)\n@click.option(\n    \"--id\", help=\"Unique identifier for the corpus (required when creating new corpus).\"\n)\n@click.option(\"--name\", default=None, help=\"Descriptive name for the corpus.\")\n@click.option(\n    \"--description\",\n    default=None,\n    help=\"Detailed description of the corpus content and purpose.\",\n)\n@click.option(\n    \"--doc\",\n    \"docs\",\n    multiple=True,\n    help=(\n        \"Add a document to the corpus. Format: 'id|name|text' or 'id|text' (name optional). \"\n        \"Can be used multiple times to add several documents.\"\n    ),\n)\n@click.option(\n    \"--remove-doc\",\n    \"remove_docs\",\n    multiple=True,\n    help=\"Remove a document from the corpus by its ID. Can be used multiple times.\",\n)\n@click.option(\n    \"--meta\",\n    \"metas\",\n    multiple=True,\n    help=\"Add or update corpus metadata. Format: key=value. Can be used multiple times.\",\n)\n@click.option(\n    \"--add-rel\",\n    \"relationships\",\n    multiple=True,\n    help=(\n        \"Add a relationship between textual and numerical data. \"\n        \"Format: 'first|second|relation' (e.g., text:term|num:column|correlates). \"\n        \"Can be used multiple times.\"\n    ),\n)\n@click.option(\n    \"--clear-rel\",\n    is_flag=True,\n    help=\"Remove all relationships from the corpus metadata.\",\n)\n@click.option(\n    \"--print\",\n    \"print_corpus\",\n    is_flag=True,\n    help=\"Display the corpus in a formatted view.\",\n)\n@click.option(\n    \"--out\", default=None, help=\"Save the corpus to a folder or file as corpus.json.\"\n)\n@click.option(\n    \"--inp\",\n    default=None,\n    help=\"Load an existing corpus from a folder or file containing corpus.json.\",\n)\n# New options for Corpus methods\n@click.option(\n    \"--df-cols\", is_flag=True, help=\"Display all column names in the DataFrame.\"\n)\n@click.option(\n    \"--df-row-count\", is_flag=True, help=\"Display the number of rows in the DataFrame.\"\n)\n@click.option(\n    \"--df-row\",\n    default=None,\n    type=int,\n    help=\"Display a specific DataFrame row by its index.\",\n)\n@click.option(\"--doc-ids\", is_flag=True, help=\"Display all document IDs in the corpus.\")\n@click.option(\n    \"--doc-id\", default=None, help=\"Display details of a specific document by its ID.\"\n)\n@click.option(\n    \"--relationships\",\n    \"print_relationships\",\n    is_flag=True,\n    help=\"Display all relationships defined in the corpus.\",\n)\n@click.option(\n    \"--relationships-for-keyword\",\n    default=None,\n    help=\"Display all relationships involving a specific keyword.\",\n)\n@click.option(\n    \"--semantic\",\n    default=None,\n    help=\"Perform semantic search using a query string. Returns documents similar to the query.\",\n)\n@click.option(\n    \"--similar-docs\",\n    default=None,\n    help=\"Find documents similar to a comma-separated list of document IDs. Useful for literature reviews. Use with --num and --rec.\",\n)\n@click.option(\n    \"--num\",\n    default=5,\n    type=int,\n    help=\"Number of results to return for search operations (default: 5).\",\n)\n@click.option(\n    \"--semantic-chunks\",\n    default=None,\n    help=\"Perform semantic search on document chunks within a specific document. Use with --doc-id and --rec for threshold.\",\n)\n@click.option(\n    \"--rec\",\n    default=0.4,\n    type=float,\n    help=\"Similarity threshold for semantic operations (0-1, default: 0.4). Higher values = more similar results only.\",\n)\n@click.option(\n    \"--metadata-df\",\n    is_flag=True,\n    help=\"Export semantic search collection metadata as a DataFrame. Requires semantic search initialization.\",\n)\n@click.option(\n    \"--metadata-keys\",\n    default=None,\n    help=\"Comma-separated list of metadata keys to include when exporting to DataFrame.\",\n)\n@click.option(\n    \"--tdabm\",\n    default=None,\n    help=\"Perform TDABM analysis. Format: 'y_variable:x_variables:radius' (e.g., 'satisfaction:age,income:0.3'). Radius defaults to 0.3 if omitted.\",\n)\n@click.option(\n    \"--graph\",\n    is_flag=True,\n    help=\"Generate a graph representation of the corpus. Requires documents to have keywords assigned (run text analysis first).\",\n)\n@click.option(\n    \"--temporal-link\",\n    default=None,\n    help=\"Link documents to dataframe rows by time. Format: 'method:column:param' where method is 'nearest', 'window', or 'sequence'. Examples: 'nearest:timestamp', 'window:timestamp:300' (\u00b1300 seconds), 'sequence:timestamp:W' (weekly).\",\n)\n@click.option(\n    \"--temporal-filter\",\n    default=None,\n    help=\"Filter corpus by time range. Format: 'start:end' where start and end are ISO 8601 timestamps. Either can be omitted. Examples: '2025-01-01:', ':2025-12-31', '2025-01-01:2025-06-30'.\",\n)\n@click.option(\n    \"--temporal-summary\",\n    default=None,\n    help=\"Generate temporal summary. Format: 'period' where period is 'D' (day), 'W' (week), 'M' (month), or 'Y' (year). Example: 'W' for weekly summary.\",\n)\n@click.option(\n    \"--temporal-sentiment\",\n    default=None,\n    help=\"Analyze sentiment trends over time. Format: 'period:aggregation' where period is 'D'/'W'/'M' and aggregation is 'mean'/'median'. Example: 'W:mean' for weekly average sentiment.\",\n)\n@click.option(\n    \"--temporal-topics\",\n    default=None,\n    help=\"Extract topics over time periods. Format: 'period:top_n' where period is 'D'/'W'/'M' and top_n is number of topics. Example: 'W:5' for top 5 topics per week.\",\n)\n@click.option(\n    \"--temporal-subgraphs\",\n    default=None,\n    help=\"Create time-sliced subgraphs. Format: 'period' where period is 'D'/'W'/'M'. Example: 'W' for weekly subgraphs.\",\n)\n@click.option(\n    \"--embedding-link\",\n    default=None,\n    help=\"Link documents to dataframe rows by embedding similarity. Format: 'metric:top_k:threshold' where metric is 'cosine' or 'euclidean', top_k is number of links per document, threshold is min similarity (0-1). Example: 'cosine:1:0.7'.\",\n)\n@click.option(\n    \"--embedding-stats\",\n    is_flag=True,\n    help=\"Display statistics about embedding-based links in the corpus.\",\n)\n@click.option(\n    \"--embedding-viz\",\n    default=None,\n    help=\"Visualize embedding space using dimensionality reduction. Format: 'method:output_path' where method is 'tsne', 'pca', or 'umap'. Example: 'tsne:embedding_viz.png'.\",\n)\ndef main(\n    verbose: bool,\n    id: str | None,\n    name: str | None,\n    description: str | None,\n    docs: tuple[str, ...],\n    remove_docs: tuple[str, ...],\n    metas: tuple[str, ...],\n    relationships: tuple[str, ...],\n    clear_rel: bool,\n    print_corpus: bool,\n    out: str | None,\n    inp: str | None,\n    df_cols: bool,\n    df_row_count: bool,\n    df_row: int | None,\n    doc_ids: bool,\n    doc_id: str | None,\n    print_relationships: bool,\n    relationships_for_keyword: str | None,\n    semantic: str | None,\n    similar_docs: str | None,\n    num: int,\n    semantic_chunks: str | None,\n    rec: float,\n    metadata_df: bool,\n    metadata_keys: str | None,\n    tdabm: str | None,\n    graph: bool,\n    temporal_link: str | None,\n    temporal_filter: str | None,\n    temporal_summary: str | None,\n    temporal_sentiment: str | None,\n    temporal_topics: str | None,\n    temporal_subgraphs: str | None,\n    embedding_link: str | None,\n    embedding_stats: bool,\n    embedding_viz: str | None,\n):\n    \"\"\"\n    CRISP-T Corpus CLI: Create and manipulate corpus data from the command line.\n\n    This tool allows you to create, modify, query, and manage corpus objects\n    without writing any Python code. Perfect for quick data exploration and\n    corpus management tasks.\n\n    \\b\n    \ud83d\udcda COMMON TASKS:\n\n    Create a new corpus:\n       crispt --id my_corpus --name \"My Research\" --doc \"1|Doc1|Hello world\"\n\n    Load and modify an existing corpus:\n       crispt --inp crisp_input --doc \"2|Doc2|More text\" --out crisp_input\n\n    Query corpus information:\n       crispt --inp crisp_input --doc-ids --df-cols\n\n    Perform semantic search:\n       crispt --inp crisp_input --semantic \"find this topic\" --num 10\n\n    \\b\n    \ud83d\udca1 TIPS:\n    \u2022 Use --print to view the full corpus structure\n    \u2022 Combine multiple operations in a single command\n    \u2022 Use --out to save changes after modifications\n    \u2022 TDABM and graph features require prior analysis\n\n    \\b\n    \ud83d\udcd6 For more examples, see: docs/DEMO.md\n    \"\"\"\n    logging.basicConfig(level=(logging.DEBUG if verbose else logging.WARNING))\n    logger = logging.getLogger(__name__)\n\n    if verbose:\n        click.echo(click.style(\"\u2713 Verbose mode enabled\", fg=\"cyan\"))\n\n    # Display banner\n    click.echo(click.style(\"\\n\" + \"=\" * 60, fg=\"blue\", bold=True))\n    click.echo(click.style(\"CRISP-T Corpus CLI\", fg=\"green\", bold=True))\n    click.echo(click.style(\"=\" * 60 + \"\\n\", fg=\"blue\", bold=True))\n\n    # Load corpus from --inp if provided\n    corpus = initialize_corpus(inp=inp)\n    if not corpus:\n        # Build initial corpus from CLI args\n        if not id:\n            raise click.ClickException(\n                click.style(\"\u274c Error: \", fg=\"red\", bold=True)\n                + \"--id is required when creating a new corpus (not using --inp).\"\n            )\n        corpus = Corpus(\n            id=id,\n            name=name,\n            description=description,\n            score=None,\n            documents=[],\n            df=None,\n            visualization={},\n            metadata={},\n        )\n        click.echo(click.style(\"\u2713 New corpus created\", fg=\"green\"))\n\n    # Add documents\n    for d in docs:\n        doc_id, doc_name, doc_text = _parse_doc(d)\n        document = Document(\n            id=doc_id,\n            name=doc_name,\n            description=None,\n            score=0.0,\n            text=doc_text,\n            metadata={},\n        )\n        corpus.add_document(document)\n    if docs:\n        click.echo(click.style(f\"\u2713 Added {len(docs)} document(s)\", fg=\"green\"))\n\n    # Remove documents\n    for rid in remove_docs:\n        corpus.remove_document_by_id(rid)\n    if remove_docs:\n        click.echo(click.style(f\"\u2713 Removed {len(remove_docs)} document(s)\", fg=\"green\"))\n\n    # Update metadata\n    for m in metas:\n        k, v = _parse_kv(m)\n        corpus.update_metadata(k, v)\n    if metas:\n        count = len(metas)\n        entry_word = \"entry\" if count == 1 else \"entries\"\n        click.echo(click.style(f\"\u2713 Updated {count} metadata {entry_word}\", fg=\"green\"))\n\n    # Relationships\n    for r in relationships:\n        first, second, relation = _parse_relationship(r)\n        corpus.add_relationship(first, second, relation)\n    if relationships:\n        click.echo(\n            click.style(f\"\u2713 Added {len(relationships)} relationship(s)\", fg=\"green\")\n        )\n    if clear_rel:\n        corpus.clear_relationships()\n        click.echo(click.style(\"\u2713 Cleared all relationships\", fg=\"green\"))\n\n    # Print DataFrame column names\n    if df_cols:\n        cols = corpus.get_all_df_column_names()\n        click.echo(\n            click.style(\"\ud83d\udcca DataFrame columns: \", fg=\"cyan\", bold=True) + str(cols)\n        )\n\n    # Print DataFrame row count\n    if df_row_count:\n        count = corpus.get_row_count()\n        click.echo(\n            click.style(\"\ud83d\udcca DataFrame row count: \", fg=\"cyan\", bold=True) + str(count)\n        )\n\n    # Print DataFrame row by index\n    if df_row is not None:\n        row = corpus.get_row_by_index(df_row)\n        if row is not None:\n            click.echo(click.style(f\"\ud83d\udcca DataFrame row {df_row}:\", fg=\"cyan\", bold=True))\n            click.echo(row.to_dict())\n        else:\n            click.echo(click.style(f\"\u26a0\ufe0f  No row found at index {df_row}\", fg=\"yellow\"))\n\n    # Print all document IDs\n    if doc_ids:\n        ids = corpus.get_all_document_ids()\n        click.echo(click.style(\"\ud83d\udcc4 Document IDs: \", fg=\"cyan\", bold=True) + str(ids))\n\n    # Print document by ID\n    if doc_id:\n        doc = corpus.get_document_by_id(doc_id)\n        if doc:\n            click.echo(click.style(f\"\ud83d\udcc4 Document {doc_id}:\", fg=\"cyan\", bold=True))\n            click.echo(doc.model_dump())\n        else:\n            click.echo(\n                click.style(f\"\u26a0\ufe0f  No document found with ID {doc_id}\", fg=\"yellow\")\n            )\n            exit(0)\n\n    # Print relationships\n    if print_relationships:\n        rels = corpus.get_relationships()\n        click.echo(click.style(\"\ud83d\udd17 Relationships: \", fg=\"cyan\", bold=True) + str(rels))\n\n    # Print relationships for keyword\n    if relationships_for_keyword:\n        rels = corpus.get_all_relationships_for_keyword(relationships_for_keyword)\n        click.echo(\n            click.style(\n                f\"\ud83d\udd17 Relationships for keyword '{relationships_for_keyword}': \",\n                fg=\"cyan\",\n                bold=True,\n            )\n            + str(rels)\n        )\n\n    # Semantic search\n    if semantic:\n        try:\n            from .semantic import Semantic\n\n            click.echo(\n                click.style(\"\\n\ud83d\udd0d Performing semantic search for: \", fg=\"yellow\")\n                + click.style(f\"'{semantic}'\", fg=\"cyan\", bold=True)\n            )\n            # Try with default embeddings first, fall back to simple embeddings\n            try:\n                semantic_analyzer = Semantic(corpus)\n            except Exception as network_error:\n                # If network error or download fails, try simple embeddings\n                if (\n                    \"address\" in str(network_error).lower()\n                    or \"download\" in str(network_error).lower()\n                ):\n                    click.echo(\n                        click.style(\n                            \"\u2139\ufe0f  Note: Using simple embeddings (network unavailable)\",\n                            fg=\"blue\",\n                        )\n                    )\n                    semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                else:\n                    raise\n            corpus = semantic_analyzer.get_similar(semantic, n_results=num)\n            click.echo(\n                click.style(\n                    f\"\u2713 Found {len(corpus.documents)} similar document(s)\", fg=\"green\"\n                )\n            )\n            click.echo(click.style(\"\\n\ud83d\udca1 Tips:\", fg=\"cyan\", bold=True))\n            click.echo(\n                f\"   \u2022 Use {click.style('--out &lt;folder&gt;', fg='green')} to save the filtered corpus\"\n            )\n            click.echo(f\"   \u2022 Use {click.style('--print', fg='green')} to view results\")\n        except ImportError as e:\n            click.echo(click.style(f\"\u274c Error: {e}\", fg=\"red\"))\n            click.echo(\n                click.style(\"   Install with: \", fg=\"white\")\n                + click.style(\"pip install chromadb\", fg=\"cyan\", bold=True)\n            )\n        except Exception as e:\n            click.echo(\n                click.style(\"\u274c Error during semantic search: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    # Find similar documents\n    if similar_docs:\n        try:\n            from .semantic import Semantic\n\n            click.echo(f\"\\nFinding documents similar to: '{similar_docs}'\")\n            click.echo(f\"Number of results: {num}\")\n            # Convert rec to 0-1 range if needed (for similar_docs, threshold is 0-1)\n            threshold = rec / 10.0 if rec &gt; 1.0 else rec\n            click.echo(f\"Similarity threshold: {threshold}\")\n\n            # Try with default embeddings first, fall back to simple embeddings\n            try:\n                semantic_analyzer = Semantic(corpus)\n            except Exception as network_error:\n                # If network error or download fails, try simple embeddings\n                if (\n                    \"address\" in str(network_error).lower()\n                    or \"download\" in str(network_error).lower()\n                ):\n                    click.echo(\"Note: Using simple embeddings (network unavailable)\")\n                    semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                else:\n                    raise\n\n            # Get similar document IDs\n            similar_doc_ids = semantic_analyzer.get_similar_documents(\n                document_ids=similar_docs, n_results=num, threshold=threshold\n            )\n\n            click.echo(f\"\u2713 Found {len(similar_doc_ids)} similar documents\")\n            if similar_doc_ids:\n                click.echo(\"\\nSimilar Document IDs:\")\n                for doc_id in similar_doc_ids:\n                    doc = corpus.get_document_by_id(doc_id)\n                    doc_name = f\" ({doc.name})\" if doc and doc.name else \"\"\n                    click.echo(f\"  - {doc_id}{doc_name}\")\n                click.echo(\"\\nHint: Use --doc-id to view individual documents\")\n                click.echo(\n                    \"Hint: This feature is useful for literature reviews to find similar documents\"\n                )\n            else:\n                click.echo(\"No similar documents found above the threshold.\")\n                click.echo(\"Hint: Try lowering the threshold with --rec\")\n\n        except ImportError as e:\n            click.echo(f\"Error: {e}\")\n            click.echo(\"Install chromadb with: pip install chromadb\")\n        except Exception as e:\n            click.echo(f\"Error finding similar documents: {e}\")\n\n    # Semantic chunk search\n    if semantic_chunks:\n        if not doc_id:\n            click.echo(\"Error: --doc-id is required when using --semantic-chunks\")\n        else:\n            try:\n                from .semantic import Semantic\n\n                click.echo(\n                    f\"\\nPerforming semantic chunk search for: '{semantic_chunks}'\"\n                )\n                click.echo(f\"Document ID: {doc_id}\")\n                click.echo(f\"Threshold: {rec}\")\n\n                # Try with default embeddings first, fall back to simple embeddings\n                try:\n                    semantic_analyzer = Semantic(corpus)\n                except Exception as network_error:\n                    # If network error or download fails, try simple embeddings\n                    if (\n                        \"address\" in str(network_error).lower()\n                        or \"download\" in str(network_error).lower()\n                    ):\n                        click.echo(\n                            \"Note: Using simple embeddings (network unavailable)\"\n                        )\n                        semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                    else:\n                        raise\n\n                # Get similar chunks\n                chunks = semantic_analyzer.get_similar_chunks(\n                    query=semantic_chunks,\n                    doc_id=doc_id,\n                    threshold=rec,\n                    n_results=20,  # Get more chunks to filter by threshold\n                )\n\n                click.echo(f\"\u2713 Found {len(chunks)} matching chunks\")\n                click.echo(\"\\nMatching chunks:\")\n                click.echo(\"=\" * 60)\n                for i, chunk in enumerate(chunks, 1):\n                    click.echo(f\"\\nChunk {i}:\")\n                    click.echo(chunk)\n                    click.echo(\"-\" * 60)\n\n                if len(chunks) == 0:\n                    click.echo(\"No chunks matched the query above the threshold.\")\n                    click.echo(\n                        \"Hint: Try lowering the threshold with --rec or use a different query.\"\n                    )\n                else:\n                    click.echo(\n                        f\"\\nHint: These {len(chunks)} chunks can be used for coding/annotating the document.\"\n                    )\n                    click.echo(\n                        \"Hint: Adjust --rec threshold to get more or fewer results.\"\n                    )\n\n            except ImportError as e:\n                click.echo(f\"Error: {e}\")\n                click.echo(\"Install chromadb with: pip install chromadb\")\n            except Exception as e:\n                click.echo(f\"Error during semantic chunk search: {e}\")\n\n    # Export metadata as DataFrame\n    if metadata_df:\n        try:\n            from .semantic import Semantic\n\n            click.echo(\"\\nExporting metadata as DataFrame...\")\n            # Try with default embeddings first, fall back to simple embeddings\n            try:\n                semantic_analyzer = Semantic(corpus)\n            except Exception as network_error:\n                # If network error or download fails, try simple embeddings\n                if (\n                    \"address\" in str(network_error).lower()\n                    or \"download\" in str(network_error).lower()\n                ):\n                    click.echo(\"Note: Using simple embeddings (network unavailable)\")\n                    semantic_analyzer = Semantic(corpus, use_simple_embeddings=True)\n                else:\n                    raise\n            # Parse metadata_keys if provided\n            keys_list = None\n            if metadata_keys:\n                keys_list = [k.strip() for k in metadata_keys.split(\",\")]\n            corpus = semantic_analyzer.get_df(metadata_keys=keys_list)\n            click.echo(\"\u2713 Metadata exported to DataFrame\")\n            if corpus.df is not None:\n                click.echo(f\"DataFrame shape: {corpus.df.shape}\")\n                click.echo(f\"Columns: {list(corpus.df.columns)}\")\n            click.echo(\"Hint: Use --out to save the corpus with the updated DataFrame\")\n        except ImportError as e:\n            click.echo(f\"Error: {e}\")\n            click.echo(\"Install chromadb with: pip install chromadb\")\n        except Exception as e:\n            click.echo(f\"Error exporting metadata: {e}\")\n\n    # TDABM analysis\n    if tdabm:\n        try:\n            # Parse tdabm parameter: y_variable:x_variables:radius\n            parts = tdabm.split(\":\")\n            if len(parts) &lt; 2:\n                raise click.ClickException(\n                    click.style(\"\u274c Invalid format. \", fg=\"red\", bold=True)\n                    + \"Use 'y_variable:x_variables:radius' \"\n                    + \"(e.g., 'satisfaction:age,income:0.3'). Radius defaults to 0.3 if omitted.\"\n                )\n\n            y_var = parts[0].strip()\n            x_vars = parts[1].strip()\n            radius = 0.3  # default\n\n            if len(parts) &gt;= 3:\n                try:\n                    radius = float(parts[2].strip())\n                except ValueError:\n                    raise click.ClickException(\n                        click.style(\"\u274c Invalid radius: \", fg=\"red\", bold=True)\n                        + f\"'{parts[2]}'. Must be a number.\"\n                    ) from None\n\n            click.echo(click.style(\"\\n\ud83d\udcca Performing TDABM analysis...\", fg=\"yellow\"))\n            click.echo(f\"   Y variable: {click.style(y_var, fg='cyan')}\")\n            click.echo(f\"   X variables: {click.style(x_vars, fg='cyan')}\")\n            click.echo(f\"   Radius: {click.style(str(radius), fg='cyan')}\")\n\n            tdabm_analyzer = Tdabm(corpus)\n            result = tdabm_analyzer.generate_tdabm(\n                y=y_var, x_variables=x_vars, radius=radius\n            )\n\n            click.echo(\"\\n\" + result)\n            click.echo(click.style(\"\\n\u2713 TDABM analysis complete\", fg=\"green\"))\n            click.echo(click.style(\"\\n\ud83d\udca1 Next steps:\", fg=\"cyan\", bold=True))\n            click.echo(\"   \u2022 Results stored in corpus metadata['tdabm']\")\n            click.echo(\n                f\"   \u2022 Use {click.style('--out &lt;folder&gt;', fg='green')} to save the corpus\"\n            )\n            click.echo(\n                f\"   \u2022 Use {click.style('crispviz --tdabm', fg='green')} to visualize results\"\n            )\n\n        except ValueError as e:\n            click.echo(click.style(f\"\\n\u274c Error: {e}\", fg=\"red\", bold=True))\n            click.echo(click.style(\"\\n\ud83d\udca1 Troubleshooting:\", fg=\"cyan\", bold=True))\n            click.echo(\n                \"   \u2022 Ensure your corpus has a DataFrame with the specified variables\"\n            )\n            click.echo(\"   \u2022 Y variable must be continuous (not binary)\")\n            click.echo(\"   \u2022 X variables must be numeric or ordinal\")\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error during TDABM analysis: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    # Temporal analysis operations\n    if temporal_link:\n        try:\n            from datetime import timedelta\n\n            from .temporal import TemporalAnalyzer\n\n            click.echo(click.style(\"\\n\u23f0 Performing temporal linking...\", fg=\"yellow\"))\n\n            # Parse temporal_link parameter\n            parts = temporal_link.split(\":\")\n            if len(parts) &lt; 2:\n                raise click.ClickException(\n                    \"Invalid format. Use 'method:column' or 'method:column:param' \"\n                    \"(e.g., 'nearest:timestamp', 'window:timestamp:300', 'sequence:timestamp:W')\"\n                )\n\n            method = parts[0].strip()\n            time_column = parts[1].strip()\n            analyzer = TemporalAnalyzer(corpus)\n\n            if method == \"nearest\":\n                max_gap = None\n                if len(parts) &gt;= 3:\n                    max_gap = timedelta(seconds=float(parts[2]))\n                corpus = analyzer.link_by_nearest_time(\n                    time_column=time_column, max_gap=max_gap\n                )\n                click.echo(\n                    click.style(\n                        \"\u2713 Documents linked to nearest dataframe rows\", fg=\"green\"\n                    )\n                )\n\n            elif method == \"window\":\n                window_seconds = 300  # Default \u00b15 minutes\n                if len(parts) &gt;= 3:\n                    window_seconds = float(parts[2])\n                window = timedelta(seconds=window_seconds)\n                corpus = analyzer.link_by_time_window(\n                    time_column=time_column, window_before=window, window_after=window\n                )\n                click.echo(\n                    click.style(\n                        f\"\u2713 Documents linked within \u00b1{window_seconds}s window\",\n                        fg=\"green\",\n                    )\n                )\n\n            elif method == \"sequence\":\n                period = \"W\"  # Default to weekly\n                if len(parts) &gt;= 3:\n                    period = parts[2].strip()\n                corpus = analyzer.link_by_sequence(\n                    time_column=time_column, period=period\n                )\n                click.echo(\n                    click.style(f\"\u2713 Documents linked by {period} sequences\", fg=\"green\")\n                )\n\n            else:\n                raise click.ClickException(\n                    f\"Unknown method: {method}. Use 'nearest', 'window', or 'sequence'\"\n                )\n\n            click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n            click.echo(\n                \"   \u2022 Temporal links stored in document metadata['temporal_links']\"\n            )\n            click.echo(\n                f\"   \u2022 Use {click.style('--out &lt;folder&gt;', fg='green')} to save the corpus\"\n            )\n\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error in temporal linking: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    if temporal_filter:\n        try:\n            from .temporal import TemporalAnalyzer\n\n            click.echo(click.style(\"\\n\u23f0 Filtering by time range...\", fg=\"yellow\"))\n\n            # Parse temporal_filter parameter\n            parts = temporal_filter.split(\":\")\n            if len(parts) != 2:\n                raise click.ClickException(\n                    \"Invalid format. Use 'start:end' (e.g., '2025-01-01:2025-12-31')\"\n                )\n\n            start_time = parts[0].strip() if parts[0].strip() else None\n            end_time = parts[1].strip() if parts[1].strip() else None\n\n            analyzer = TemporalAnalyzer(corpus)\n            corpus = analyzer.filter_by_time_range(\n                start_time=start_time, end_time=end_time\n            )\n\n            click.echo(\n                click.style(\n                    f\"\u2713 Corpus filtered to {len(corpus.documents)} documents\",\n                    fg=\"green\",\n                )\n            )\n            if corpus.df is not None:\n                click.echo(f\"   \u2022 DataFrame rows: {len(corpus.df)}\")\n\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error in temporal filtering: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    if temporal_summary:\n        try:\n            from .temporal import TemporalAnalyzer\n\n            click.echo(click.style(\"\\n\u23f0 Generating temporal summary...\", fg=\"yellow\"))\n\n            period = temporal_summary.strip()\n            analyzer = TemporalAnalyzer(corpus)\n            summary = analyzer.get_temporal_summary(period=period)\n\n            if not summary.empty:\n                click.echo(click.style(\"\\n\u2713 Temporal summary:\", fg=\"green\"))\n                click.echo(summary)\n                click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n                click.echo(\"   \u2022 Summary stored in corpus metadata\")\n            else:\n                click.echo(\n                    click.style(\"\u26a0 No temporal data available for summary\", fg=\"yellow\")\n                )\n\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error in temporal summary: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    if temporal_sentiment:\n        try:\n            from .temporal import TemporalAnalyzer\n\n            click.echo(click.style(\"\\n\u23f0 Analyzing sentiment trends...\", fg=\"yellow\"))\n\n            # Parse temporal_sentiment parameter\n            parts = temporal_sentiment.split(\":\")\n            period = parts[0].strip() if len(parts) &gt; 0 else \"W\"\n            aggregation = parts[1].strip() if len(parts) &gt; 1 else \"mean\"\n\n            analyzer = TemporalAnalyzer(corpus)\n            trend = analyzer.get_temporal_sentiment_trend(\n                period=period, aggregation=aggregation\n            )\n\n            if not trend.empty:\n                click.echo(click.style(\"\\n\u2713 Sentiment trend:\", fg=\"green\"))\n                click.echo(trend)\n                click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n                click.echo(\"   \u2022 Sentiment trend stored in corpus metadata\")\n                click.echo(\n                    f\"   \u2022 Use {click.style('crispviz --temporal-sentiment', fg='green')} to visualize\"\n                )\n            else:\n                click.echo(\n                    click.style(\n                        \"\u26a0 No sentiment data available for trend analysis\", fg=\"yellow\"\n                    )\n                )\n                click.echo(\n                    \"   \u2022 Run sentiment analysis first with: crisp --inp &lt;folder&gt; --sentiment\"\n                )\n\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error in temporal sentiment: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    if temporal_topics:\n        try:\n            from .temporal import TemporalAnalyzer\n\n            click.echo(click.style(\"\\n\u23f0 Extracting temporal topics...\", fg=\"yellow\"))\n\n            # Parse temporal_topics parameter\n            parts = temporal_topics.split(\":\")\n            period = parts[0].strip() if len(parts) &gt; 0 else \"W\"\n            top_n = int(parts[1].strip()) if len(parts) &gt; 1 else 5\n\n            analyzer = TemporalAnalyzer(corpus)\n            topics = analyzer.get_temporal_topics(period=period, top_n=top_n)\n\n            if topics:\n                click.echo(\n                    click.style(f\"\\n\u2713 Topics over time (top {top_n}):\", fg=\"green\")\n                )\n                for period_key, topic_list in topics.items():\n                    click.echo(\n                        f\"\\n{click.style(period_key, fg='cyan')}: {', '.join(topic_list)}\"\n                    )\n                click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n                click.echo(\"   \u2022 Temporal topics stored in corpus metadata\")\n            else:\n                click.echo(\n                    click.style(\n                        \"\u26a0 No temporal data available for topic extraction\", fg=\"yellow\"\n                    )\n                )\n\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error in temporal topics: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    if temporal_subgraphs:\n        try:\n            from .graph import CrispGraph\n\n            click.echo(click.style(\"\\n\u23f0 Creating temporal subgraphs...\", fg=\"yellow\"))\n\n            period = temporal_subgraphs.strip()\n\n            # Ensure graph exists\n            if \"graph\" not in corpus.metadata:\n                click.echo(click.style(\"\u26a0 Creating base graph first...\", fg=\"yellow\"))\n                graph_gen = CrispGraph(corpus)\n                graph_gen.create_graph()\n\n            graph_gen = CrispGraph(corpus)\n            subgraphs = graph_gen.create_temporal_subgraphs(period=period)\n\n            click.echo(\n                click.style(\n                    f\"\\n\u2713 Created {len(subgraphs)} temporal subgraphs\", fg=\"green\"\n                )\n            )\n            for period_key, subgraph in subgraphs.items():\n                click.echo(\n                    f\"   \u2022 {period_key}: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\"\n                )\n\n            click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n            click.echo(\"   \u2022 Temporal subgraphs stored in corpus metadata\")\n            click.echo(\n                f\"   \u2022 Use {click.style('--out &lt;folder&gt;', fg='green')} to save the corpus\"\n            )\n\n        except Exception as e:\n            click.echo(\n                click.style(\n                    \"\\n\u274c Error creating temporal subgraphs: \", fg=\"red\", bold=True\n                )\n                + str(e)\n            )\n\n    # Embedding-based linking\n    if embedding_link:\n        try:\n            from .embedding_linker import EmbeddingLinker\n\n            click.echo(\n                click.style(\n                    \"\\n\ud83d\udd17 Performing embedding-based cross-modal linking...\",\n                    fg=\"yellow\",\n                )\n            )\n\n            # Parse embedding_link parameter\n            parts = embedding_link.split(\":\")\n            metric = parts[0].strip() if len(parts) &gt; 0 else \"cosine\"\n            top_k = int(parts[1].strip()) if len(parts) &gt; 1 else 1\n            threshold = float(parts[2].strip()) if len(parts) &gt; 2 else None\n\n            if metric not in [\"cosine\", \"euclidean\"]:\n                raise click.ClickException(\n                    f\"Unknown metric: {metric}. Use 'cosine' or 'euclidean'\"\n                )\n\n            click.echo(f\"   \u2022 Metric: {click.style(metric, fg='cyan')}\")\n            click.echo(f\"   \u2022 Top-k: {click.style(str(top_k), fg='cyan')}\")\n            if threshold:\n                click.echo(\n                    f\"   \u2022 Threshold: {click.style(f'{threshold:.2f}', fg='cyan')}\"\n                )\n\n            linker = EmbeddingLinker(\n                corpus,\n                similarity_metric=metric,\n                use_simple_embeddings=False,  # Use default embeddings\n            )\n\n            corpus = linker.link_by_embedding_similarity(\n                threshold=threshold, top_k=top_k\n            )\n\n            stats = linker.get_link_statistics()\n            click.echo(click.style(\"\\n\u2713 Embedding-based linking complete\", fg=\"green\"))\n            click.echo(\n                f\"   \u2022 Linked documents: {click.style(str(stats['linked_documents']), fg='cyan')}/{stats['total_documents']}\"\n            )\n            click.echo(\n                f\"   \u2022 Total links: {click.style(str(stats['total_links']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Avg similarity: {click.style('{:.3f}'.format(stats['avg_similarity']), fg='cyan')}\"\n            )\n\n            click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n            click.echo(\n                \"   \u2022 Embedding links stored in document metadata['embedding_links']\"\n            )\n            click.echo(\n                f\"   \u2022 Use {click.style('--out &lt;folder&gt;', fg='green')} to save the corpus\"\n            )\n            click.echo(\n                f\"   \u2022 Use {click.style('--embedding-stats', fg='green')} to view detailed statistics\"\n            )\n\n        except ImportError as e:\n            click.echo(click.style(f\"\\n\u274c Error: {e}\", fg=\"red\", bold=True))\n            click.echo(click.style(\"\\n\ud83d\udca1 Install ChromaDB:\", fg=\"cyan\", bold=True))\n            click.echo(f\"   {click.style('pip install chromadb', fg='green')}\")\n        except Exception as e:\n            click.echo(\n                click.style(\n                    \"\\n\u274c Error in embedding-based linking: \", fg=\"red\", bold=True\n                )\n                + str(e)\n            )\n\n    if embedding_stats:\n        try:\n            from .embedding_linker import EmbeddingLinker\n\n            click.echo(click.style(\"\\n\ud83d\udcca Embedding Link Statistics\", fg=\"yellow\"))\n\n            # Check if corpus has embedding links\n            has_links = any(\n                \"embedding_links\" in doc.metadata and doc.metadata[\"embedding_links\"]\n                for doc in corpus.documents\n            )\n\n            if not has_links:\n                click.echo(\n                    click.style(\"\\n\u26a0 No embedding links found in corpus\", fg=\"yellow\")\n                )\n                click.echo(\n                    f\"   Run {click.style('--embedding-link', fg='green')} first to create links\"\n                )\n                return\n\n            # Create linker to get stats\n            linker = EmbeddingLinker(corpus, use_simple_embeddings=True)\n            stats = linker.get_link_statistics()\n\n            click.echo(click.style(\"\\n\u2713 Statistics:\", fg=\"green\"))\n            click.echo(\n                f\"   \u2022 Total documents: {click.style(str(stats['total_documents']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Linked documents: {click.style(str(stats['linked_documents']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Total links: {click.style(str(stats['total_links']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Average similarity: {click.style('{:.3f}'.format(stats['avg_similarity']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Min similarity: {click.style('{:.3f}'.format(stats['min_similarity']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Max similarity: {click.style('{:.3f}'.format(stats['max_similarity']), fg='cyan')}\"\n            )\n\n        except Exception as e:\n            click.echo(\n                click.style(\n                    \"\\n\u274c Error getting embedding statistics: \", fg=\"red\", bold=True\n                )\n                + str(e)\n            )\n\n    if embedding_viz:\n        try:\n            from .embedding_linker import EmbeddingLinker\n\n            click.echo(click.style(\"\\n\ud83d\udcc8 Visualizing embedding space...\", fg=\"yellow\"))\n\n            # Parse embedding_viz parameter\n            parts = embedding_viz.split(\":\")\n            method = parts[0].strip() if len(parts) &gt; 0 else \"tsne\"\n            output_path = parts[1].strip() if len(parts) &gt; 1 else \"embedding_viz.png\"\n\n            if method not in [\"tsne\", \"pca\", \"umap\"]:\n                raise click.ClickException(\n                    f\"Unknown method: {method}. Use 'tsne', 'pca', or 'umap'\"\n                )\n\n            click.echo(f\"   \u2022 Method: {click.style(method, fg='cyan')}\")\n            click.echo(f\"   \u2022 Output: {click.style(output_path, fg='cyan')}\")\n\n            linker = EmbeddingLinker(corpus, use_simple_embeddings=True)\n\n            # Generate embeddings first\n            linker._get_text_embeddings()\n            linker._get_numeric_embeddings()\n\n            # Create visualization\n            linker.visualize_embedding_space(output_path=output_path, method=method)\n\n            click.echo(\n                click.style(f\"\\n\u2713 Visualization saved to {output_path}\", fg=\"green\")\n            )\n\n        except ImportError as e:\n            click.echo(\n                click.style(\"\\n\u274c Error: Missing dependencies\", fg=\"red\", bold=True)\n            )\n            click.echo(f\"   {e!s}\")\n            click.echo(\n                click.style(\"\\n\ud83d\udca1 Install required packages:\", fg=\"cyan\", bold=True)\n            )\n            click.echo(\n                f\"   {click.style('pip install matplotlib scikit-learn', fg='green')}\"\n            )\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error creating visualization: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n\n    # Graph generation\n    if graph:\n        try:\n            from .graph import CrispGraph\n\n            click.echo(\n                click.style(\"\\n\ud83d\udd78\ufe0f  Generating graph representation...\", fg=\"yellow\")\n            )\n            graph_gen = CrispGraph(corpus)\n            graph_data = graph_gen.create_graph()\n\n            click.echo(click.style(\"\\n\u2713 Graph created successfully\", fg=\"green\"))\n            click.echo(\n                f\"   \u2022 Nodes: {click.style(str(graph_data['num_nodes']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Edges: {click.style(str(graph_data['num_edges']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Documents: {click.style(str(graph_data['num_documents']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Has keywords: {click.style(str(graph_data['has_keywords']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Has clusters: {click.style(str(graph_data['has_clusters']), fg='cyan')}\"\n            )\n            click.echo(\n                f\"   \u2022 Has metadata: {click.style(str(graph_data['has_metadata']), fg='cyan')}\"\n            )\n\n            click.echo(click.style(\"\\n\ud83d\udca1 Next steps:\", fg=\"cyan\", bold=True))\n            click.echo(\"   \u2022 Graph data stored in corpus metadata['graph']\")\n            click.echo(\n                f\"   \u2022 Use {click.style('--out &lt;folder&gt;', fg='green')} to save the corpus\"\n            )\n            click.echo(\n                f\"   \u2022 Use {click.style('crispviz --graph', fg='green')} to visualize the graph\"\n            )\n\n        except ValueError as e:\n            click.echo(click.style(f\"\\n\u274c Error: {e}\", fg=\"red\", bold=True))\n            click.echo(click.style(\"\\n\ud83d\udca1 Tip:\", fg=\"cyan\", bold=True))\n            click.echo(\"   Documents need keywords assigned first\")\n            click.echo(\n                f\"   Run text analysis with: {click.style('crisp --inp &lt;folder&gt; --assign', fg='green')}\"\n            )\n        except Exception as e:\n            click.echo(\n                click.style(\"\\n\u274c Error generating graph: \", fg=\"red\", bold=True)\n                + str(e)\n            )\n            logger.error(f\"Graph generation error: {e}\", exc_info=True)\n\n    # Save corpus to --out if provided\n    if out:\n        from .read_data import ReadData\n\n        rd = ReadData(corpus=corpus)\n        rd.write_corpus_to_json(out, corpus=corpus)\n        click.echo(\n            click.style(\"\\n\u2713 Corpus saved to: \", fg=\"green\", bold=True)\n            + click.style(str(out), fg=\"cyan\")\n        )\n\n    if print_corpus:\n        click.echo(\n            click.style(\n                \"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\",\n                fg=\"blue\",\n                bold=True,\n            )\n        )\n        click.echo(\n            click.style(\n                \"\u2551  \ud83d\udcca CORPUS DETAILS                           \u2551\", fg=\"blue\", bold=True\n            )\n        )\n        click.echo(\n            click.style(\n                \"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\",\n                fg=\"blue\",\n                bold=True,\n            )\n        )\n        corpus.pretty_print()\n\n    logger.info(\"Corpus CLI finished\")\n</code></pre>"},{"location":"modules/#vizcli.main","title":"<code>main(verbose, inp, out, bins, topics_num, top_n, corr_columns, freq, by_topic, wordcloud, ldavis, top_terms, corr_heatmap, tdabm, graph, graph_nodes, graph_layout)</code>","text":"<p>CRISP-T: Visualization CLI</p> <p>Generate publication-quality visualizations from your corpus data. Supports word clouds, topic distributions, correlation heatmaps, and more.</p> <p>\b \ud83d\udcca GETTING STARTED:</p> <p>Step 1: Ensure you have an analyzed corpus (created with 'crisp' or 'crispt')</p> Create an output directory for your visualizations: <p>mkdir visualizations</p> Generate visualizations: <p>crispviz --inp crisp_input --out visualizations --freq --wordcloud --topics-num 8</p> <p>\b \ud83d\udca1 TIPS: \u2022 Some visualizations require prior analysis (e.g., --wordcloud needs topics) \u2022 Use --ldavis for interactive HTML visualization \u2022 Combine multiple flags to generate several visualizations at once \u2022 Results are saved as PNG images (or HTML for --ldavis)</p> <p>\b \ud83d\udcd6 For examples and detailed usage, see: docs/DEMO.md</p> Source code in <code>src/crisp_t/vizcli.py</code> <pre><code>@click.command()\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Show detailed progress and debugging information.\",\n)\n@click.option(\n    \"--inp\", \"-i\", help=\"Load an existing corpus from a folder containing corpus.json.\"\n)\n@click.option(\n    \"--out\",\n    \"-o\",\n    help=\"Output directory where visualization images (PNG/HTML) will be saved.\",\n)\n@click.option(\n    \"--bins\",\n    default=100,\n    show_default=True,\n    help=\"Number of bins for frequency distribution charts.\",\n)\n@click.option(\n    \"--topics-num\",\n    default=8,\n    show_default=True,\n    help=\"Number of topics for LDA analysis when required (Mettler et al. 2025 recommends 8).\",\n)\n@click.option(\n    \"--top-n\",\n    default=20,\n    show_default=True,\n    help=\"Number of top terms to display in the top-terms bar chart.\",\n)\n@click.option(\n    \"--corr-columns\",\n    default=\"\",\n    help=\"Comma-separated list of numeric columns for correlation heatmap. Auto-selected if empty.\",\n)\n@click.option(\n    \"--freq\", is_flag=True, help=\"Generate word frequency distribution visualization.\"\n)\n@click.option(\n    \"--by-topic\",\n    is_flag=True,\n    help=\"Generate distribution by dominant topic (requires LDA topic modeling first).\",\n)\n@click.option(\n    \"--wordcloud\",\n    is_flag=True,\n    help=\"Generate topic word cloud visualization (requires LDA first).\",\n)\n@click.option(\n    \"--ldavis\",\n    is_flag=True,\n    help=\"Generate interactive LDA visualization as HTML (requires LDA and pyLDAvis).\",\n)\n@click.option(\n    \"--top-terms\",\n    is_flag=True,\n    help=\"Generate top terms bar chart based on word frequencies.\",\n)\n@click.option(\n    \"--corr-heatmap\",\n    is_flag=True,\n    help=\"Generate correlation heatmap from numeric columns in your CSV data.\",\n)\n@click.option(\n    \"--tdabm\",\n    is_flag=True,\n    help=\"Generate TDABM visualization (requires TDABM analysis in corpus metadata). Run 'crispt --tdabm' first.\",\n)\n@click.option(\n    \"--graph\",\n    is_flag=True,\n    help=\"Generate graph visualization (requires graph data in corpus metadata). Run 'crispt --graph' first.\",\n)\n@click.option(\n    \"--graph-nodes\",\n    default=\"\",\n    help=(\n        \"Comma-separated node types to include: document,keyword,cluster,metadata. \"\n        \"Example: --graph-nodes document,keyword. Leave empty or use 'all' for all types.\"\n    ),\n)\n@click.option(\n    \"--graph-layout\",\n    default=\"spring\",\n    show_default=True,\n    help=\"Layout algorithm for graph: spring (default), circular, kamada_kawai, or spectral.\",\n)\ndef main(\n    verbose: bool,\n    inp: str | None,\n    out: str,\n    bins: int,\n    topics_num: int,\n    top_n: int,\n    corr_columns: str,\n    freq: bool,\n    by_topic: bool,\n    wordcloud: bool,\n    ldavis: bool,\n    top_terms: bool,\n    corr_heatmap: bool,\n    tdabm: bool,\n    graph: bool,\n    graph_nodes: str,\n    graph_layout: str,\n):\n    \"\"\"CRISP-T: Visualization CLI\n\n    Generate publication-quality visualizations from your corpus data.\n    Supports word clouds, topic distributions, correlation heatmaps, and more.\n\n    \\b\n    \ud83d\udcca GETTING STARTED:\n\n    Step 1: Ensure you have an analyzed corpus (created with 'crisp' or 'crispt')\n\n    Step 2: Create an output directory for your visualizations:\n       mkdir visualizations\n\n    Step 3: Generate visualizations:\n       crispviz --inp crisp_input --out visualizations --freq --wordcloud --topics-num 8\n\n    \\b\n    \ud83d\udca1 TIPS:\n    \u2022 Some visualizations require prior analysis (e.g., --wordcloud needs topics)\n    \u2022 Use --ldavis for interactive HTML visualization\n    \u2022 Combine multiple flags to generate several visualizations at once\n    \u2022 Results are saved as PNG images (or HTML for --ldavis)\n\n    \\b\n    \ud83d\udcd6 For examples and detailed usage, see: docs/DEMO.md\n    \"\"\"\n\n    if verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n        click.echo(click.style(\"\u2713 Verbose mode enabled\", fg=\"cyan\"))\n\n    # Display banner with colors\n    click.echo(click.style(\"\\n\" + \"=\" * 60, fg=\"blue\", bold=True))\n    click.echo(click.style(\"CRISP-T Visualizations\", fg=\"green\", bold=True))\n    click.echo(click.style(f\"Version: {__version__}\", fg=\"cyan\"))\n    click.echo(click.style(\"=\" * 60 + \"\\n\", fg=\"blue\", bold=True))\n\n    try:\n        out_dir = Path(out)\n    except TypeError:\n        click.echo(\n            click.style(\"\u274c Error: \", fg=\"red\", bold=True)\n            + \"No output directory specified.\"\n        )\n        click.echo(\n            click.style(\"\\n\ud83d\udca1 Tip: \", fg=\"cyan\")\n            + \"Use \"\n            + click.style(\"--out &lt;directory&gt;\", fg=\"green\")\n            + \" to specify where visualizations should be saved\"\n        )\n        raise click.Abort() from None\n    out_dir.mkdir(parents=True, exist_ok=True)\n    click.echo(\n        click.style(\"\u2713 Output directory: \", fg=\"green\")\n        + click.style(str(out_dir), fg=\"cyan\")\n    )\n\n    # Initialize components\n    corpus = None\n\n    click.echo(click.style(\"\\n\ud83d\udcc2 Loading corpus...\", fg=\"yellow\"))\n    corpus = initialize_corpus(inp=inp)\n\n    if not corpus:\n        raise click.ClickException(\n            click.style(\"\u274c Error: \", fg=\"red\", bold=True)\n            + \"No input provided. Use \"\n            + click.style(\"--inp &lt;corpus_folder&gt;\", fg=\"green\")\n        )\n\n    click.echo(\n        click.style(\"\u2713 Corpus loaded: \", fg=\"green\")\n        + f\"{len(corpus.documents)} document(s)\"\n    )\n\n    viz = QRVisualize(corpus=corpus)\n\n    # Helper: build LDA if by-topic or wordcloud requested\n    cluster_instance = None\n\n    def ensure_topics():\n        nonlocal cluster_instance\n        if cluster_instance is None:\n            click.echo(click.style(\"\\n\u2699\ufe0f  Building topic model...\", fg=\"yellow\"))\n            cluster_instance = Cluster(corpus=corpus)\n            cluster_instance.build_lda_model(topics=topics_num)\n            # Populate visualization structures used by QRVisualize\n            cluster_instance.format_topics_sentences(visualize=True)\n            click.echo(\n                click.style(f\"\u2713 Topic model ready ({topics_num} topics)\", fg=\"green\")\n            )\n        return cluster_instance\n\n    click.echo(\n        click.style(\n            \"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\", fg=\"blue\", bold=True\n        )\n    )\n    click.echo(\n        click.style(\n            \"\u2551  \ud83c\udfa8 GENERATING VISUALIZATIONS                \u2551\", fg=\"blue\", bold=True\n        )\n    )\n    click.echo(\n        click.style(\n            \"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\", fg=\"blue\", bold=True\n        )\n    )\n\n    # 1) Word frequency distribution\n    if freq:\n        click.echo(\n            click.style(\"\u25b8 Creating word frequency distribution...\", fg=\"yellow\")\n        )\n        df_text = pd.DataFrame(\n            {\"Text\": [getattr(doc, \"text\", \"\") or \"\" for doc in corpus.documents]}\n        )\n        out_path = out_dir / \"word_frequency.png\"\n        viz.plot_frequency_distribution_of_words(\n            df=df_text, folder_path=str(out_path), bins=bins, show=False\n        )\n        click.echo(\n            click.style(\"  \u2713 Saved: \", fg=\"green\")\n            + click.style(str(out_path), fg=\"cyan\")\n        )\n\n    # 2) Distribution by topic (requires topics)\n    if by_topic:\n        click.echo(click.style(\"\u25b8 Creating distribution by topic...\", fg=\"yellow\"))\n        ensure_topics()\n        out_path = out_dir / \"by_topic.png\"\n        viz.plot_distribution_by_topic(\n            df=None, folder_path=str(out_path), bins=bins, show=False\n        )\n        click.echo(\n            click.style(\"  \u2713 Saved: \", fg=\"green\")\n            + click.style(str(out_path), fg=\"cyan\")\n        )\n\n    # 3) Topic wordcloud (requires topics)\n    if wordcloud:\n        click.echo(click.style(\"\u25b8 Creating topic word cloud...\", fg=\"yellow\"))\n        ensure_topics()\n        out_path = out_dir / \"wordcloud.png\"\n        viz.plot_wordcloud(topics=None, folder_path=str(out_path), show=False)\n        click.echo(\n            click.style(\"  \u2713 Saved: \", fg=\"green\")\n            + click.style(str(out_path), fg=\"cyan\")\n        )\n\n    # 3.5) LDA visualization (requires topics)\n    if ldavis:\n        click.echo(\n            click.style(\"\u25b8 Creating interactive LDA visualization...\", fg=\"yellow\")\n        )\n        cluster = ensure_topics()\n        out_path = out_dir / \"lda_visualization.html\"\n        try:\n            viz.get_lda_viz(\n                lda_model=cluster._lda_model,\n                corpus_bow=cluster._bag_of_words,\n                dictionary=cluster._dictionary,\n                folder_path=str(out_path),\n                show=False,\n            )\n            click.echo(\n                click.style(\"  \u2713 Saved: \", fg=\"green\")\n                + click.style(str(out_path), fg=\"cyan\")\n            )\n        except ImportError as e:\n            click.echo(click.style(f\"  \u26a0\ufe0f  Warning: {e}\", fg=\"yellow\"))\n        except Exception as e:\n            click.echo(click.style(\"  \u274c Error: \", fg=\"red\") + str(e))\n\n    # 4) Top terms (compute from text directly)\n    if top_terms:\n        click.echo(click.style(\"\u25b8 Creating top terms bar chart...\", fg=\"yellow\"))\n        texts = [getattr(doc, \"text\", \"\") or \"\" for doc in corpus.documents]\n        tokens = []\n        for t in texts:\n            tokens.extend((t or \"\").lower().split())\n        freq_map = Counter(tokens)\n        if not freq_map:\n            click.echo(click.style(\"  \u26a0\ufe0f  No tokens found to plot\", fg=\"yellow\"))\n        else:\n            df_terms = pd.DataFrame(\n                {\n                    \"term\": list(freq_map.keys()),\n                    \"frequency\": list(freq_map.values()),\n                }\n            )\n            # QRVisualize sorts internally; we just pass full DF\n            out_path = out_dir / \"top_terms.png\"\n            viz.plot_top_terms(\n                df=df_terms, top_n=top_n, folder_path=str(out_path), show=False\n            )\n            click.echo(\n                click.style(\"  \u2713 Saved: \", fg=\"green\")\n                + click.style(str(out_path), fg=\"cyan\")\n            )\n\n    # 5) Correlation heatmap\n    if corr_heatmap:\n        click.echo(click.style(\"\u25b8 Creating correlation heatmap...\", fg=\"yellow\"))\n        if getattr(corpus, \"df\", None) is None or corpus.df.empty:\n            click.echo(\n                click.style(\n                    \"  \u26a0\ufe0f  No CSV data available for correlation heatmap; skipping.\",\n                    fg=\"yellow\",\n                )\n            )\n        else:\n            df0 = corpus.df.copy()\n            # If user specified columns, attempt to use them; else let visualize auto-select\n            cols = (\n                [c.strip() for c in corr_columns.split(\",\") if c.strip()]\n                if corr_columns\n                else None\n            )\n            out_path = out_dir / \"corr_heatmap.png\"\n            if cols:\n                # Pass subset to avoid rename ambiguity\n                sub = (\n                    df0[cols].copy().select_dtypes(include=[\"number\"])\n                )  # ensure numeric\n                viz.plot_correlation_heatmap(\n                    df=sub, columns=None, folder_path=str(out_path), show=False\n                )\n            else:\n                viz.plot_correlation_heatmap(\n                    df=df0, columns=None, folder_path=str(out_path), show=False\n                )\n            click.echo(\n                click.style(\"  \u2713 Saved: \", fg=\"green\")\n                + click.style(str(out_path), fg=\"cyan\")\n            )\n\n    # TDABM visualization\n    if tdabm:\n        click.echo(click.style(\"\u25b8 Creating TDABM visualization...\", fg=\"yellow\"))\n        if \"tdabm\" not in corpus.metadata:\n            click.echo(\n                click.style(\"  \u26a0\ufe0f  No TDABM data found in corpus metadata.\", fg=\"yellow\")\n            )\n            click.echo(\n                click.style(\"  \ud83d\udca1 Tip: \", fg=\"cyan\")\n                + \"Run TDABM analysis first with: \"\n                + click.style(\n                    \"crispt --tdabm y_var:x_vars:radius --inp &lt;corpus_dir&gt;\", fg=\"green\"\n                )\n            )\n        else:\n            out_path = out_dir / \"tdabm.png\"\n            try:\n                viz.draw_tdabm(corpus=corpus, folder_path=str(out_path), show=False)\n                click.echo(\n                    click.style(\"  \u2713 Saved: \", fg=\"green\")\n                    + click.style(str(out_path), fg=\"cyan\")\n                )\n            except Exception as e:\n                click.echo(click.style(\"  \u274c Error: \", fg=\"red\") + str(e))\n                logger.error(f\"TDABM visualization error: {e}\", exc_info=True)\n\n    # Graph visualization (filtered by node types if provided)\n    if graph or graph_nodes:\n        click.echo(click.style(\"\u25b8 Creating graph visualization...\", fg=\"yellow\"))\n        if \"graph\" not in corpus.metadata:\n            click.echo(\n                click.style(\"  \u26a0\ufe0f  No graph data found in corpus metadata.\", fg=\"yellow\")\n            )\n            click.echo(\n                click.style(\"  \ud83d\udca1 Tip: \", fg=\"cyan\")\n                + \"Run graph generation first with: \"\n                + click.style(\"crispt --graph --inp &lt;corpus_dir&gt;\", fg=\"green\")\n            )\n        else:\n            raw_types = (graph_nodes or \"\").strip().lower()\n            include_all = raw_types in (\"\", \"all\", \"*\")\n            allowed_types = {\"document\", \"keyword\", \"cluster\", \"metadata\"}\n            requested_types = set()\n            if not include_all:\n                for part in raw_types.split(\",\"):\n                    p = part.strip()\n                    if not p:\n                        continue\n                    if p in allowed_types:\n                        requested_types.add(p)\n                    else:\n                        click.echo(\n                            click.style(\n                                f\"  \u26a0\ufe0f  Unknown node type '{p}' ignored. \", fg=\"yellow\"\n                            )\n                            + f\"Allowed: {', '.join(sorted(allowed_types))}\"\n                        )\n                if not requested_types:\n                    click.echo(\n                        click.style(\n                            \"  \u2139\ufe0f  No valid node types specified; defaulting to all.\",\n                            fg=\"blue\",\n                        )\n                    )\n                    include_all = True\n\n            graph_data = corpus.metadata.get(\"graph\", {})\n            nodes = graph_data.get(\"nodes\", [])\n            edges = graph_data.get(\"edges\", [])\n\n            if include_all:\n                filtered_nodes = nodes\n                filtered_edges = edges\n            else:\n                filtered_nodes = [n for n in nodes if n.get(\"label\") in requested_types]\n                kept_ids = {str(n.get(\"id\")) for n in filtered_nodes}\n                filtered_edges = [\n                    e\n                    for e in edges\n                    if str(e.get(\"source\")) in kept_ids\n                    and str(e.get(\"target\")) in kept_ids\n                ]\n\n            # Build a shallow copy of graph metadata with filtered components\n            filtered_graph_meta = dict(graph_data)\n            filtered_graph_meta[\"nodes\"] = filtered_nodes\n            filtered_graph_meta[\"edges\"] = filtered_edges\n            filtered_graph_meta[\"num_nodes\"] = len(filtered_nodes)\n            filtered_graph_meta[\"num_edges\"] = len(filtered_edges)\n            filtered_graph_meta[\"num_documents\"] = sum(\n                1 for n in filtered_nodes if n.get(\"label\") == \"document\"\n            )\n\n            # Inject temporary filtered metadata for visualization\n            original_graph_meta = corpus.metadata.get(\"graph\")\n            corpus.metadata[\"graph\"] = filtered_graph_meta\n            out_path = out_dir / \"graph.png\"\n            try:\n                viz.draw_graph(\n                    corpus=corpus,\n                    folder_path=str(out_path),\n                    show=False,\n                    layout=graph_layout,\n                )\n                click.echo(\n                    click.style(\"  \u2713 Saved: \", fg=\"green\")\n                    + click.style(str(out_path), fg=\"cyan\")\n                )\n                if not include_all:\n                    click.echo(\n                        click.style(\n                            f\"  \u2139\ufe0f  Filtered to node types: {', '.join(sorted(requested_types))}\",\n                            fg=\"blue\",\n                        )\n                    )\n            except Exception as e:\n                click.echo(click.style(\"  \u274c Error: \", fg=\"red\") + str(e))\n                logger.error(f\"Graph visualization error: {e}\", exc_info=True)\n            finally:\n                # Restore original metadata (avoid side-effects)\n                corpus.metadata[\"graph\"] = original_graph_meta\n\n    click.echo(click.style(\"\\n\" + \"=\" * 60, fg=\"green\", bold=True))\n    click.echo(click.style(\"\u2713 Visualization Complete!\", fg=\"green\", bold=True))\n    click.echo(\n        click.style(\"\u2713 All visualizations saved to: \", fg=\"green\")\n        + click.style(str(out_dir), fg=\"cyan\")\n    )\n    click.echo(click.style(\"=\" * 60 + \"\\n\", fg=\"green\", bold=True))\n</code></pre>"},{"location":"modules/#model.corpus.Corpus","title":"<code>Corpus</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Corpus model for storing a collection of documents.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>class Corpus(BaseModel):\n    \"\"\"\n    Corpus model for storing a collection of documents.\n    \"\"\"\n\n    id: str = Field(..., description=\"Unique identifier for the corpus.\")\n    name: str | None = Field(None, description=\"Name of the corpus.\")\n    description: str | None = Field(None, description=\"Description of the corpus.\")\n    score: float | None = Field(\n        None, description=\"Score associated with the corpus.\"\n    )\n    documents: list[Document] = Field(\n        default_factory=list, description=\"List of documents in the corpus.\"\n    )\n    df: pd.DataFrame | None = Field(\n        None, description=\"Numeric data associated with the corpus.\"\n    )\n    visualization: dict[str, Any] = Field(\n        default_factory=dict, description=\"Visualization data associated with the corpus.\"\n    )\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True\n    )  # required for pandas DataFrame\n    metadata: dict = Field(\n        default_factory=dict, description=\"Metadata associated with the corpus.\"\n    )\n\n    def pretty_print(self, show=\"all\"):\n        \"\"\"\n        Print the corpus information in a human-readable format.\n\n        Args:\n            show: Display option. Can be:\n                - \"all\": Show all corpus information\n                - \"documents\": Show first 5 documents\n                - \"documents N\": Show first N documents (e.g., \"documents 10\")\n                - \"documents metadata\": Show document-specific metadata\n                - \"dataframe\": Show DataFrame head\n                - \"dataframe metadata\": Show DataFrame metadata columns (metadata_*)\n                - \"dataframe stats\": Show DataFrame statistics\n                - \"metadata\": Show all corpus metadata\n                - \"metadata KEY\": Show specific metadata key (e.g., \"metadata pca\")\n                - \"stats\": Show DataFrame statistics (deprecated, use \"dataframe stats\")\n        \"\"\"\n        # Color codes for terminal output\n        BLUE = \"\\033[94m\"\n        GREEN = \"\\033[92m\"\n        YELLOW = \"\\033[93m\"\n        CYAN = \"\\033[96m\"\n        MAGENTA = \"\\033[95m\"\n        RED = \"\\033[91m\"\n        RESET = \"\\033[0m\"\n        BOLD = \"\\033[1m\"\n\n        # Parse the show parameter to support subcommands\n        parts = show.split(maxsplit=1)\n        main_command = parts[0]\n        sub_command = parts[1] if len(parts) &gt; 1 else None\n\n        # Print basic corpus info for most commands\n        if main_command in [\"all\", \"documents\", \"dataframe\", \"metadata\"]:\n            print(f\"{BOLD}{BLUE}Corpus ID:{RESET} {self.id}\")\n            print(f\"{BOLD}{BLUE}Name:{RESET} {self.name}\")\n            print(f\"{BOLD}{BLUE}Description:{RESET} {self.description}\")\n\n        # Handle documents command\n        if main_command in [\"all\", \"documents\"]:\n            if sub_command == \"metadata\":\n                # Show document-specific metadata\n                print(f\"\\n{BOLD}{GREEN}=== Document Metadata ==={RESET}\")\n                if not self.documents:\n                    print(\"No documents in corpus\")\n                else:\n                    for i, doc in enumerate(self.documents, 1):\n                        print(f\"\\n{CYAN}Document {i}:{RESET}\")\n                        print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                        print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                        if doc.metadata:\n                            print(f\"  {BOLD}Metadata:{RESET}\")\n                            for key, value in doc.metadata.items():\n                                # Truncate long values\n                                val_str = str(value)\n                                if len(val_str) &gt; 100:\n                                    val_str = val_str[:97] + \"...\"\n                                print(f\"    {YELLOW}{key}:{RESET} {val_str}\")\n                        else:\n                            print(f\"  {BOLD}Metadata:{RESET} (none)\")\n            else:\n                # Determine how many documents to show\n                num_docs = 5  # default\n                if sub_command:\n                    try:\n                        num_docs = int(sub_command)\n                    except ValueError:\n                        print(f\"{RED}Invalid number for documents: {sub_command}. Using default (5).{RESET}\")\n\n                print(f\"\\n{BOLD}{GREEN}=== Documents ==={RESET}\")\n                print(f\"Total documents: {len(self.documents)}\")\n                print(f\"Showing first {min(num_docs, len(self.documents))} document(s):\\n\")\n\n                for i, doc in enumerate(self.documents[:num_docs], 1):\n                    print(f\"{CYAN}Document {i}:{RESET}\")\n                    print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                    print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                    # Show a snippet of text if available\n                    if hasattr(doc, 'text') and doc.text:\n                        text_snippet = doc.text[:200] + \"...\" if len(doc.text) &gt; 200 else doc.text\n                        print(f\"  {BOLD}Text:{RESET} {text_snippet}\")\n                    print()\n\n        # Handle dataframe command\n        if main_command in [\"all\", \"dataframe\"]:\n            if self.df is not None:\n                if sub_command == \"metadata\":\n                    # Show DataFrame metadata columns (columns starting with metadata_)\n                    print(f\"\\n{BOLD}{GREEN}=== DataFrame Metadata Columns ==={RESET}\")\n                    metadata_cols = [col for col in self.df.columns if col.startswith(\"metadata_\")]\n                    if metadata_cols:\n                        print(f\"Found {len(metadata_cols)} metadata column(s):\")\n                        for col in metadata_cols:\n                            print(f\"  {YELLOW}{col}{RESET}\")\n                            # Show some statistics for the metadata column\n                            print(f\"    Non-null values: {self.df[col].notna().sum()}\")\n                            print(f\"    Null values: {self.df[col].isna().sum()}\")\n                            # Show unique values if not too many\n                            unique_count = self.df[col].nunique()\n                            if unique_count &lt;= 10:\n                                print(f\"    Unique values ({unique_count}): {list(self.df[col].unique())}\")\n                            else:\n                                print(f\"    Unique values: {unique_count}\")\n                    else:\n                        print(\"No metadata columns found (columns starting with 'metadata_')\")\n                elif sub_command == \"stats\":\n                    # Show DataFrame statistics\n                    self._print_dataframe_stats()\n                else:\n                    # Show DataFrame head\n                    print(f\"\\n{BOLD}{GREEN}=== DataFrame ==={RESET}\")\n                    print(f\"Shape: {self.df.shape}\")\n                    print(f\"Columns: {list(self.df.columns)}\")\n                    print(\"\\nFirst few rows:\")\n                    print(self.df.head())\n            else:\n                if main_command == \"dataframe\":\n                    print(f\"\\n{BOLD}{RED}No DataFrame available{RESET}\")\n\n        # Handle metadata command\n        if main_command in [\"all\", \"metadata\"]:\n            if sub_command:\n                # Show specific metadata key\n                print(f\"\\n{BOLD}{GREEN}=== Metadata: {sub_command} ==={RESET}\")\n                if sub_command in self.metadata:\n                    value = self.metadata[sub_command]\n                    # Format the output based on the type of value\n                    if isinstance(value, dict):\n                        for k, v in value.items():\n                            print(f\"{YELLOW}{k}:{RESET} {v}\")\n                    elif isinstance(value, list):\n                        for i, item in enumerate(value, 1):\n                            print(f\"{i}. {item}\")\n                    else:\n                        print(value)\n                else:\n                    print(f\"{RED}Metadata key '{sub_command}' not found{RESET}\")\n                    available_keys = list(self.metadata.keys())\n                    if available_keys:\n                        print(f\"Available keys: {', '.join(available_keys)}\")\n            else:\n                # Show all metadata\n                print(f\"\\n{BOLD}{GREEN}=== Corpus Metadata ==={RESET}\")\n                if not self.metadata:\n                    print(\"No metadata available\")\n                else:\n                    for key, value in self.metadata.items():\n                        print(f\"\\n{MAGENTA}{key}:{RESET}\")\n                        # Truncate long values\n                        val_str = str(value)\n                        if len(val_str) &gt; 500:\n                            val_str = val_str[:497] + \"...\"\n                        print(f\"  {val_str}\")\n\n        # Handle stats command (deprecated, redirect to dataframe stats)\n        if main_command == \"stats\":\n            print(f\"{YELLOW}Note: 'stats' is deprecated. Use 'dataframe stats' instead.{RESET}\")\n            if self.df is not None:\n                self._print_dataframe_stats()\n            else:\n                print(f\"{RED}No DataFrame available{RESET}\")\n\n        print(f\"\\n{BOLD}Display completed for '{show}'{RESET}\")\n\n    def _print_dataframe_stats(self):\n        \"\"\"Helper method to print DataFrame statistics.\"\"\"\n        YELLOW = \"\\033[93m\"\n        BOLD = \"\\033[1m\"\n        RESET = \"\\033[0m\"\n        GREEN = \"\\033[92m\"\n\n        print(f\"\\n{BOLD}{GREEN}=== DataFrame Statistics ==={RESET}\")\n        print(self.df.describe())\n        print(f\"\\n{BOLD}Distinct values per column:{RESET}\")\n        for col in self.df.columns:\n            nunique = self.df[col].nunique()\n            print(f\"  {YELLOW}{col}:{RESET} {nunique} distinct value(s)\")\n            # If distinct values &lt; 10, show value counts\n            if nunique &lt;= 10:\n                print(\"    Value counts:\")\n                for val, count in self.df[col].value_counts().items():\n                    print(f\"      {val}: {count}\")\n                print()\n    def get_all_df_column_names(self):\n        \"\"\"\n        Get a list of all column names in the DataFrame.\n\n        Returns:\n            List of column names.\n        \"\"\"\n        if self.df is not None:\n            return self.df.columns.tolist()\n        return []\n\n    def get_descriptive_statistics(self):\n        \"\"\"\n        Get descriptive statistics of the DataFrame.\n\n        Returns:\n            DataFrame containing descriptive statistics, or None if DataFrame is None.\n        \"\"\"\n        if self.df is not None:\n            return self.df.describe()\n        return None\n\n    def get_row_count(self):\n        \"\"\"\n        Get the number of rows in the DataFrame.\n\n        Returns:\n            Number of rows in the DataFrame, or 0 if DataFrame is None.\n        \"\"\"\n        if self.df is not None:\n            return len(self.df)\n        return 0\n\n    def get_row_by_index(self, index: int) -&gt; pd.Series | None:\n        \"\"\"\n        Get a row from the DataFrame by its index.\n\n        Args:\n            index: Index of the row to retrieve.\n        Returns:\n            Row as a pandas Series if index is valid, else None.\n        \"\"\"\n        if self.df is not None and 0 &lt;= index &lt; len(self.df):\n            return self.df.iloc[index]\n        return None\n\n    def get_all_document_ids(self):\n        \"\"\"\n        Get a list of all document IDs in the corpus.\n\n        Returns:\n            List of document IDs.\n        \"\"\"\n        return [doc.id for doc in self.documents]\n\n    def get_document_by_id(self, document_id: str) -&gt; Document | None:\n        \"\"\"\n        Get a document by its ID.\n\n        Args:\n            document_id: ID of the document to retrieve.\n\n        Returns:\n            Document object if found, else None.\n        \"\"\"\n        for doc in self.documents:\n            if doc.id == document_id:\n                return doc\n        return None\n\n    def add_document(self, document: Document):\n        \"\"\"\n        Add a document to the corpus.\n\n        Args:\n            document: Document object to add.\n        \"\"\"\n        self.documents.append(document)\n\n    def remove_document_by_id(self, document_id: str):\n        \"\"\"\n        Remove a document from the corpus by its ID.\n\n        Args:\n            document_id: ID of the document to remove.\n        \"\"\"\n        self.documents = [\n            doc for doc in self.documents if doc.id != document_id\n        ]\n\n    def update_metadata(self, key: str, value: Any):\n        \"\"\"\n        Update the metadata of the corpus.\n\n        Args:\n            key: Metadata key to update.\n            value: New value for the metadata key.\n        \"\"\"\n        self.metadata[key] = value\n\n    def add_relationship(self, first: str, second: str, relation: str):\n        \"\"\"\n        Add a relationship between two documents in the corpus.\n\n        Args:\n            first: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n            second: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n            relation: Description of the relationship. (One of \"correlates\", \"similar to\", \"cites\", \"references\", \"contradicts\", etc.)\n        \"\"\"\n        if \"relationships\" not in self.metadata:\n            self.metadata[\"relationships\"] = []\n        self.metadata[\"relationships\"].append(\n            {\"first\": first, \"second\": second, \"relation\": relation}\n        )\n\n    def clear_relationships(self):\n        \"\"\"\n        Clear all relationships in the corpus metadata.\n        \"\"\"\n        if \"relationships\" in self.metadata:\n            self.metadata[\"relationships\"] = []\n\n    def get_relationships(self):\n        \"\"\"\n        Get all relationships in the corpus metadata.\n\n        Returns:\n            List of relationships, or empty list if none exist.\n        \"\"\"\n        return self.metadata.get(\"relationships\", [])\n\n    def get_all_relationships_for_keyword(self, keyword: str):\n        \"\"\"\n        Get all relationships involving a specific keyword.\n\n        Args:\n            keyword: Keyword to search for in relationships.\n\n        Returns:\n            List of relationships involving the keyword.\n        \"\"\"\n        rels = self.get_relationships()\n        return [\n            rel\n            for rel in rels\n            if keyword in rel[\"first\"] or keyword in rel[\"second\"]\n        ]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.add_document","title":"<code>add_document(document)</code>","text":"<p>Add a document to the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>Document object to add.</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def add_document(self, document: Document):\n    \"\"\"\n    Add a document to the corpus.\n\n    Args:\n        document: Document object to add.\n    \"\"\"\n    self.documents.append(document)\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.add_relationship","title":"<code>add_relationship(first, second, relation)</code>","text":"<p>Add a relationship between two documents in the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>str</code> <p>keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column</p> required <code>second</code> <code>str</code> <p>keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column</p> required <code>relation</code> <code>str</code> <p>Description of the relationship. (One of \"correlates\", \"similar to\", \"cites\", \"references\", \"contradicts\", etc.)</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def add_relationship(self, first: str, second: str, relation: str):\n    \"\"\"\n    Add a relationship between two documents in the corpus.\n\n    Args:\n        first: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n        second: keywords from text documents in the format text:keyword or columns from dataframe in the format numb:column\n        relation: Description of the relationship. (One of \"correlates\", \"similar to\", \"cites\", \"references\", \"contradicts\", etc.)\n    \"\"\"\n    if \"relationships\" not in self.metadata:\n        self.metadata[\"relationships\"] = []\n    self.metadata[\"relationships\"].append(\n        {\"first\": first, \"second\": second, \"relation\": relation}\n    )\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.clear_relationships","title":"<code>clear_relationships()</code>","text":"<p>Clear all relationships in the corpus metadata.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def clear_relationships(self):\n    \"\"\"\n    Clear all relationships in the corpus metadata.\n    \"\"\"\n    if \"relationships\" in self.metadata:\n        self.metadata[\"relationships\"] = []\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_all_df_column_names","title":"<code>get_all_df_column_names()</code>","text":"<p>Get a list of all column names in the DataFrame.</p> <p>Returns:</p> Type Description <p>List of column names.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_all_df_column_names(self):\n    \"\"\"\n    Get a list of all column names in the DataFrame.\n\n    Returns:\n        List of column names.\n    \"\"\"\n    if self.df is not None:\n        return self.df.columns.tolist()\n    return []\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_all_document_ids","title":"<code>get_all_document_ids()</code>","text":"<p>Get a list of all document IDs in the corpus.</p> <p>Returns:</p> Type Description <p>List of document IDs.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_all_document_ids(self):\n    \"\"\"\n    Get a list of all document IDs in the corpus.\n\n    Returns:\n        List of document IDs.\n    \"\"\"\n    return [doc.id for doc in self.documents]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_all_relationships_for_keyword","title":"<code>get_all_relationships_for_keyword(keyword)</code>","text":"<p>Get all relationships involving a specific keyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>Keyword to search for in relationships.</p> required <p>Returns:</p> Type Description <p>List of relationships involving the keyword.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_all_relationships_for_keyword(self, keyword: str):\n    \"\"\"\n    Get all relationships involving a specific keyword.\n\n    Args:\n        keyword: Keyword to search for in relationships.\n\n    Returns:\n        List of relationships involving the keyword.\n    \"\"\"\n    rels = self.get_relationships()\n    return [\n        rel\n        for rel in rels\n        if keyword in rel[\"first\"] or keyword in rel[\"second\"]\n    ]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_descriptive_statistics","title":"<code>get_descriptive_statistics()</code>","text":"<p>Get descriptive statistics of the DataFrame.</p> <p>Returns:</p> Type Description <p>DataFrame containing descriptive statistics, or None if DataFrame is None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_descriptive_statistics(self):\n    \"\"\"\n    Get descriptive statistics of the DataFrame.\n\n    Returns:\n        DataFrame containing descriptive statistics, or None if DataFrame is None.\n    \"\"\"\n    if self.df is not None:\n        return self.df.describe()\n    return None\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_document_by_id","title":"<code>get_document_by_id(document_id)</code>","text":"<p>Get a document by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>str</code> <p>ID of the document to retrieve.</p> required <p>Returns:</p> Type Description <code>Document | None</code> <p>Document object if found, else None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_document_by_id(self, document_id: str) -&gt; Document | None:\n    \"\"\"\n    Get a document by its ID.\n\n    Args:\n        document_id: ID of the document to retrieve.\n\n    Returns:\n        Document object if found, else None.\n    \"\"\"\n    for doc in self.documents:\n        if doc.id == document_id:\n            return doc\n    return None\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_relationships","title":"<code>get_relationships()</code>","text":"<p>Get all relationships in the corpus metadata.</p> <p>Returns:</p> Type Description <p>List of relationships, or empty list if none exist.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_relationships(self):\n    \"\"\"\n    Get all relationships in the corpus metadata.\n\n    Returns:\n        List of relationships, or empty list if none exist.\n    \"\"\"\n    return self.metadata.get(\"relationships\", [])\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_row_by_index","title":"<code>get_row_by_index(index)</code>","text":"<p>Get a row from the DataFrame by its index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the row to retrieve.</p> required <p>Returns:     Row as a pandas Series if index is valid, else None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_row_by_index(self, index: int) -&gt; pd.Series | None:\n    \"\"\"\n    Get a row from the DataFrame by its index.\n\n    Args:\n        index: Index of the row to retrieve.\n    Returns:\n        Row as a pandas Series if index is valid, else None.\n    \"\"\"\n    if self.df is not None and 0 &lt;= index &lt; len(self.df):\n        return self.df.iloc[index]\n    return None\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.get_row_count","title":"<code>get_row_count()</code>","text":"<p>Get the number of rows in the DataFrame.</p> <p>Returns:</p> Type Description <p>Number of rows in the DataFrame, or 0 if DataFrame is None.</p> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def get_row_count(self):\n    \"\"\"\n    Get the number of rows in the DataFrame.\n\n    Returns:\n        Number of rows in the DataFrame, or 0 if DataFrame is None.\n    \"\"\"\n    if self.df is not None:\n        return len(self.df)\n    return 0\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.pretty_print","title":"<code>pretty_print(show='all')</code>","text":"<p>Print the corpus information in a human-readable format.</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <p>Display option. Can be: - \"all\": Show all corpus information - \"documents\": Show first 5 documents - \"documents N\": Show first N documents (e.g., \"documents 10\") - \"documents metadata\": Show document-specific metadata - \"dataframe\": Show DataFrame head - \"dataframe metadata\": Show DataFrame metadata columns (metadata_*) - \"dataframe stats\": Show DataFrame statistics - \"metadata\": Show all corpus metadata - \"metadata KEY\": Show specific metadata key (e.g., \"metadata pca\") - \"stats\": Show DataFrame statistics (deprecated, use \"dataframe stats\")</p> <code>'all'</code> Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def pretty_print(self, show=\"all\"):\n    \"\"\"\n    Print the corpus information in a human-readable format.\n\n    Args:\n        show: Display option. Can be:\n            - \"all\": Show all corpus information\n            - \"documents\": Show first 5 documents\n            - \"documents N\": Show first N documents (e.g., \"documents 10\")\n            - \"documents metadata\": Show document-specific metadata\n            - \"dataframe\": Show DataFrame head\n            - \"dataframe metadata\": Show DataFrame metadata columns (metadata_*)\n            - \"dataframe stats\": Show DataFrame statistics\n            - \"metadata\": Show all corpus metadata\n            - \"metadata KEY\": Show specific metadata key (e.g., \"metadata pca\")\n            - \"stats\": Show DataFrame statistics (deprecated, use \"dataframe stats\")\n    \"\"\"\n    # Color codes for terminal output\n    BLUE = \"\\033[94m\"\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    CYAN = \"\\033[96m\"\n    MAGENTA = \"\\033[95m\"\n    RED = \"\\033[91m\"\n    RESET = \"\\033[0m\"\n    BOLD = \"\\033[1m\"\n\n    # Parse the show parameter to support subcommands\n    parts = show.split(maxsplit=1)\n    main_command = parts[0]\n    sub_command = parts[1] if len(parts) &gt; 1 else None\n\n    # Print basic corpus info for most commands\n    if main_command in [\"all\", \"documents\", \"dataframe\", \"metadata\"]:\n        print(f\"{BOLD}{BLUE}Corpus ID:{RESET} {self.id}\")\n        print(f\"{BOLD}{BLUE}Name:{RESET} {self.name}\")\n        print(f\"{BOLD}{BLUE}Description:{RESET} {self.description}\")\n\n    # Handle documents command\n    if main_command in [\"all\", \"documents\"]:\n        if sub_command == \"metadata\":\n            # Show document-specific metadata\n            print(f\"\\n{BOLD}{GREEN}=== Document Metadata ==={RESET}\")\n            if not self.documents:\n                print(\"No documents in corpus\")\n            else:\n                for i, doc in enumerate(self.documents, 1):\n                    print(f\"\\n{CYAN}Document {i}:{RESET}\")\n                    print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                    print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                    if doc.metadata:\n                        print(f\"  {BOLD}Metadata:{RESET}\")\n                        for key, value in doc.metadata.items():\n                            # Truncate long values\n                            val_str = str(value)\n                            if len(val_str) &gt; 100:\n                                val_str = val_str[:97] + \"...\"\n                            print(f\"    {YELLOW}{key}:{RESET} {val_str}\")\n                    else:\n                        print(f\"  {BOLD}Metadata:{RESET} (none)\")\n        else:\n            # Determine how many documents to show\n            num_docs = 5  # default\n            if sub_command:\n                try:\n                    num_docs = int(sub_command)\n                except ValueError:\n                    print(f\"{RED}Invalid number for documents: {sub_command}. Using default (5).{RESET}\")\n\n            print(f\"\\n{BOLD}{GREEN}=== Documents ==={RESET}\")\n            print(f\"Total documents: {len(self.documents)}\")\n            print(f\"Showing first {min(num_docs, len(self.documents))} document(s):\\n\")\n\n            for i, doc in enumerate(self.documents[:num_docs], 1):\n                print(f\"{CYAN}Document {i}:{RESET}\")\n                print(f\"  {BOLD}Name:{RESET} {doc.name}\")\n                print(f\"  {BOLD}ID:{RESET} {doc.id}\")\n                # Show a snippet of text if available\n                if hasattr(doc, 'text') and doc.text:\n                    text_snippet = doc.text[:200] + \"...\" if len(doc.text) &gt; 200 else doc.text\n                    print(f\"  {BOLD}Text:{RESET} {text_snippet}\")\n                print()\n\n    # Handle dataframe command\n    if main_command in [\"all\", \"dataframe\"]:\n        if self.df is not None:\n            if sub_command == \"metadata\":\n                # Show DataFrame metadata columns (columns starting with metadata_)\n                print(f\"\\n{BOLD}{GREEN}=== DataFrame Metadata Columns ==={RESET}\")\n                metadata_cols = [col for col in self.df.columns if col.startswith(\"metadata_\")]\n                if metadata_cols:\n                    print(f\"Found {len(metadata_cols)} metadata column(s):\")\n                    for col in metadata_cols:\n                        print(f\"  {YELLOW}{col}{RESET}\")\n                        # Show some statistics for the metadata column\n                        print(f\"    Non-null values: {self.df[col].notna().sum()}\")\n                        print(f\"    Null values: {self.df[col].isna().sum()}\")\n                        # Show unique values if not too many\n                        unique_count = self.df[col].nunique()\n                        if unique_count &lt;= 10:\n                            print(f\"    Unique values ({unique_count}): {list(self.df[col].unique())}\")\n                        else:\n                            print(f\"    Unique values: {unique_count}\")\n                else:\n                    print(\"No metadata columns found (columns starting with 'metadata_')\")\n            elif sub_command == \"stats\":\n                # Show DataFrame statistics\n                self._print_dataframe_stats()\n            else:\n                # Show DataFrame head\n                print(f\"\\n{BOLD}{GREEN}=== DataFrame ==={RESET}\")\n                print(f\"Shape: {self.df.shape}\")\n                print(f\"Columns: {list(self.df.columns)}\")\n                print(\"\\nFirst few rows:\")\n                print(self.df.head())\n        else:\n            if main_command == \"dataframe\":\n                print(f\"\\n{BOLD}{RED}No DataFrame available{RESET}\")\n\n    # Handle metadata command\n    if main_command in [\"all\", \"metadata\"]:\n        if sub_command:\n            # Show specific metadata key\n            print(f\"\\n{BOLD}{GREEN}=== Metadata: {sub_command} ==={RESET}\")\n            if sub_command in self.metadata:\n                value = self.metadata[sub_command]\n                # Format the output based on the type of value\n                if isinstance(value, dict):\n                    for k, v in value.items():\n                        print(f\"{YELLOW}{k}:{RESET} {v}\")\n                elif isinstance(value, list):\n                    for i, item in enumerate(value, 1):\n                        print(f\"{i}. {item}\")\n                else:\n                    print(value)\n            else:\n                print(f\"{RED}Metadata key '{sub_command}' not found{RESET}\")\n                available_keys = list(self.metadata.keys())\n                if available_keys:\n                    print(f\"Available keys: {', '.join(available_keys)}\")\n        else:\n            # Show all metadata\n            print(f\"\\n{BOLD}{GREEN}=== Corpus Metadata ==={RESET}\")\n            if not self.metadata:\n                print(\"No metadata available\")\n            else:\n                for key, value in self.metadata.items():\n                    print(f\"\\n{MAGENTA}{key}:{RESET}\")\n                    # Truncate long values\n                    val_str = str(value)\n                    if len(val_str) &gt; 500:\n                        val_str = val_str[:497] + \"...\"\n                    print(f\"  {val_str}\")\n\n    # Handle stats command (deprecated, redirect to dataframe stats)\n    if main_command == \"stats\":\n        print(f\"{YELLOW}Note: 'stats' is deprecated. Use 'dataframe stats' instead.{RESET}\")\n        if self.df is not None:\n            self._print_dataframe_stats()\n        else:\n            print(f\"{RED}No DataFrame available{RESET}\")\n\n    print(f\"\\n{BOLD}Display completed for '{show}'{RESET}\")\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.remove_document_by_id","title":"<code>remove_document_by_id(document_id)</code>","text":"<p>Remove a document from the corpus by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>document_id</code> <code>str</code> <p>ID of the document to remove.</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def remove_document_by_id(self, document_id: str):\n    \"\"\"\n    Remove a document from the corpus by its ID.\n\n    Args:\n        document_id: ID of the document to remove.\n    \"\"\"\n    self.documents = [\n        doc for doc in self.documents if doc.id != document_id\n    ]\n</code></pre>"},{"location":"modules/#model.corpus.Corpus.update_metadata","title":"<code>update_metadata(key, value)</code>","text":"<p>Update the metadata of the corpus.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Metadata key to update.</p> required <code>value</code> <code>Any</code> <p>New value for the metadata key.</p> required Source code in <code>src/crisp_t/model/corpus.py</code> <pre><code>def update_metadata(self, key: str, value: Any):\n    \"\"\"\n    Update the metadata of the corpus.\n\n    Args:\n        key: Metadata key to update.\n        value: New value for the metadata key.\n    \"\"\"\n    self.metadata[key] = value\n</code></pre>"},{"location":"modules/#model.document.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Document model for storing text and metadata.</p> Source code in <code>src/crisp_t/model/document.py</code> <pre><code>class Document(BaseModel):\n    \"\"\"\n    Document model for storing text and metadata.\n    \"\"\"\n    id: str = Field(..., description=\"Unique identifier for the document.\")\n    name: str | None = Field(None, description=\"Name of the corpus.\")\n    description: str | None = Field(None, description=\"Description of the corpus.\")\n    score: float = Field(0.0, description=\"Score associated with the document.\")\n    text: str = Field(..., description=\"The text content of the document.\")\n    timestamp: str | None = Field(None, description=\"Optional timestamp in ISO 8601 format (e.g., '2025-01-15T10:30:00Z').\")\n    metadata: dict = Field(\n        default_factory=dict, description=\"Metadata associated with the document.\"\n    )\n\n    def pretty_print(self):\n        \"\"\"\n        Print the document information in a human-readable format.\n        \"\"\"\n        print(f\"Document ID: {self.id}\")\n        print(f\"Name: {self.name}\")\n        print(f\"Description: {self.description}\")\n        print(f\"Score: {self.score}\")\n        print(f\"Text: {self.text[:100]}...\")  # Print first 100 characters of text\n        print(f\"Metadata: {self.metadata}\")\n        print(f\"Length of Text: {len(self.text)} characters\")\n        print(f\"Number of Metadata Entries: {len(self.metadata)}\")\n</code></pre>"},{"location":"modules/#model.document.Document.pretty_print","title":"<code>pretty_print()</code>","text":"<p>Print the document information in a human-readable format.</p> Source code in <code>src/crisp_t/model/document.py</code> <pre><code>def pretty_print(self):\n    \"\"\"\n    Print the document information in a human-readable format.\n    \"\"\"\n    print(f\"Document ID: {self.id}\")\n    print(f\"Name: {self.name}\")\n    print(f\"Description: {self.description}\")\n    print(f\"Score: {self.score}\")\n    print(f\"Text: {self.text[:100]}...\")  # Print first 100 characters of text\n    print(f\"Metadata: {self.metadata}\")\n    print(f\"Length of Text: {len(self.text)} characters\")\n    print(f\"Number of Metadata Entries: {len(self.metadata)}\")\n</code></pre>"},{"location":"modules/#csv.Csv","title":"<code>Csv</code>","text":"Source code in <code>src/crisp_t/csv.py</code> <pre><code>class Csv:\n\n    def __init__(\n        self,\n        corpus: Corpus | None = None,\n        comma_separated_text_columns: str = \"\",\n        comma_separated_ignore_columns: str = \"\",\n        id_column: str = \"id\",\n    ):\n        \"\"\"\n        Initialize the Csv object.\n        \"\"\"\n        self._corpus = corpus\n        if self._corpus is None:\n            self._df = pd.DataFrame()\n            logger.info(\"No corpus provided. Creating an empty DataFrame.\")\n        else:\n            self._df = self._corpus.df\n            if self._df is None:\n                logger.info(\"No DataFrame found in the corpus. Creating a new one.\")\n                self._df = pd.DataFrame()\n        self._df_original = self._df.copy()\n        self._comma_separated_text_columns = comma_separated_text_columns\n        self._comma_separated_ignore_columns = comma_separated_ignore_columns\n        self._id_column = id_column\n        self._X = None\n        self._y = None\n        self._X_original = None\n        self._y_original = None\n        self._id_column = id_column\n\n    @property\n    def corpus(self) -&gt; Corpus | None:\n        if self._corpus is not None and self._df is not None:\n            self._corpus.df = self._df\n        return self._corpus\n\n    @property\n    def df(self) -&gt; pd.DataFrame:\n        if self._df is None:\n            return pd.DataFrame()\n        return self._df\n\n    @property\n    def comma_separated_text_columns(self) -&gt; str:\n        return self._comma_separated_text_columns\n\n    @property\n    def comma_separated_ignore_columns(self) -&gt; str:\n        return self._comma_separated_ignore_columns\n\n    @comma_separated_ignore_columns.setter\n    def comma_separated_ignore_columns(self, value: str) -&gt; None:\n        self._comma_separated_ignore_columns = value\n        logger.info(\"Comma-separated ignore columns set successfully.\")\n        logger.debug(\n            f\"Comma-separated ignore columns: {self._comma_separated_ignore_columns}\"\n        )\n        self._process_columns()\n\n    @property\n    def id_column(self) -&gt; str:\n        return self._id_column\n\n    @corpus.setter\n    def corpus(self, value: Corpus) -&gt; None:\n        self._corpus = value\n        if self._corpus is not None:\n            self._df = self._corpus.df\n            if self._df is None:\n                logger.info(\"No DataFrame found in the corpus. Creating a new one.\")\n                self._df = pd.DataFrame()\n            self._df_original = self._df.copy()\n            logger.info(\"Corpus set successfully.\")\n            logger.debug(f\"DataFrame content: {self._df.head()}\")\n            logger.debug(f\"DataFrame shape: {self._df.shape}\")\n            logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n        else:\n            logger.error(\"Failed to set corpus. Corpus is None.\")\n\n    @df.setter\n    def df(self, value: pd.DataFrame) -&gt; None:\n        self._df = value\n        logger.info(\"DataFrame set successfully.\")\n        logger.debug(f\"DataFrame content: {self._df.head()}\")\n        logger.debug(f\"DataFrame shape: {self._df.shape}\")\n        logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n\n    @comma_separated_text_columns.setter\n    def comma_separated_text_columns(self, value: str) -&gt; None:\n        self._comma_separated_text_columns = value\n        logger.info(\"Comma-separated text columns set successfully.\")\n        logger.debug(\n            f\"Comma-separated text columns: {self._comma_separated_text_columns}\"\n        )\n        self._process_columns()\n\n    @id_column.setter\n    def id_column(self, value: str) -&gt; None:\n        self._id_column = value\n        # Add id column to the list of ignored columns\n        ignore_cols = [\n            col\n            for col in self._comma_separated_ignore_columns.split(\",\")\n            if col.strip()\n        ]\n        if value not in ignore_cols:\n            ignore_cols.append(value)\n            self._comma_separated_ignore_columns = \",\".join(ignore_cols)\n            logger.debug(\n                f\"ID column '{value}' added to ignore columns: {self._comma_separated_ignore_columns}\"\n            )\n        logger.info(\"ID column set successfully.\")\n        logger.debug(f\"ID column: {self._id_column}\")\n\n    # TODO remove @deprecated\n    #! Do not use\n    def read_csv(self, file_path: str):\n        \"\"\"\n        Read a CSV file and create a DataFrame.\n        Handles invalid UTF-8 byte sequences by ignoring them.\n        \"\"\"\n        try:\n            self._df = pd.read_csv(file_path, encoding=\"utf-8\", on_bad_lines=\"skip\")\n            logger.info(f\"CSV file {file_path} read successfully.\")\n            logger.debug(f\"DataFrame content: {self._df.head()}\")\n            logger.debug(f\"DataFrame shape: {self._df.shape}\")\n            logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n        except Exception as e:\n            logger.exception(f\"Error reading CSV file: {e}\")\n            raise\n        return self._process_columns()\n\n    def _process_columns(self):\n        # ignore comma-separated ignore columns\n        if self._comma_separated_ignore_columns:\n            ignore_columns = [\n                col.strip()\n                for col in self._comma_separated_ignore_columns.split(\",\")\n                if col.strip()\n            ]\n            self._df.drop(columns=ignore_columns, inplace=True, errors=\"ignore\")\n            logger.info(\n                f\"Ignored columns: {ignore_columns}. Updated DataFrame shape: {self._df.shape}\"\n            )\n            logger.debug(f\"DataFrame content after dropping columns: {self._df.head()}\")\n        # ignore comma-separated text columns\n        if self._comma_separated_text_columns:\n            text_columns = [\n                col.strip()\n                for col in self._comma_separated_text_columns.split(\",\")\n                if col.strip()\n            ]\n            for col in text_columns:\n                if col in self._df.columns:\n                    self._df[col] = self._df[col].astype(str)\n                    logger.info(f\"Column {col} converted to string.\")\n                    logger.debug(f\"Column {col} content: {self._df[col].head()}\")\n                else:\n                    logger.warning(f\"Column {col} not found in DataFrame.\")\n        # ignore all columns with names starting with \"metadata_\"\n        self._df = self._df.loc[:, ~self._df.columns.str.startswith(\"metadata_\")]\n        return self._df\n\n    def write_csv(self, file_path: str, index: bool = False) -&gt; None:\n        if self._df is not None:\n            self._df.to_csv(file_path, index=index)\n            logger.info(f\"DataFrame written to {file_path}\")\n            logger.debug(f\"DataFrame content: {self._df.head()}\")\n            logger.debug(f\"Index: {index}\")\n        else:\n            logger.error(\"DataFrame is None. Cannot write to CSV.\")\n\n    def mark_missing(self):\n        \"\"\"Mark missing values in the DataFrame.\n        Missing values are considered as empty strings and are replaced with NaN.\n        Rows with NaN values are then dropped from the DataFrame.\n        \"\"\"\n        if self._df is not None:\n            self._df.replace(\"\", np.nan, inplace=True)\n            self._df.dropna(inplace=True)\n        else:\n            logger.error(\"DataFrame is None. Cannot mark missing values.\")\n\n    def mark_duplicates(self):\n        \"\"\"Mark duplicate rows in the DataFrame.\n        Duplicate rows are identified and dropped from the DataFrame.\n        \"\"\"\n        if self._df is not None:\n            self._df.drop_duplicates(inplace=True)\n        else:\n            logger.error(\"DataFrame is None. Cannot mark duplicates.\")\n\n    def restore_df(self):\n        self._df = self._df_original.copy()\n\n    def get_shape(self):\n        if self._df is not None:\n            return self._df.shape\n        else:\n            logger.error(\"DataFrame is None. Cannot get shape.\")\n            return None\n\n    def get_columns(self):\n        \"\"\"Get the list of columns in the DataFrame.\"\"\"\n        if self._df is not None:\n            return self._df.columns.tolist()\n        else:\n            logger.error(\"DataFrame is None. Cannot get columns.\")\n            return []\n\n    def get_column_types(self):\n        \"\"\"Get the data types of columns in the DataFrame.\"\"\"\n        if self._df is not None:\n            return self._df.dtypes.to_dict()\n        else:\n            logger.error(\"DataFrame is None. Cannot get column types.\")\n            return {}\n\n    def get_column_values(self, column_name: str):\n        \"\"\"Get the unique values in a column of the DataFrame.\"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            return self._df[column_name].tolist()\n        else:\n            logger.error(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n            return None\n\n    def retain_numeric_columns_only(self):\n        \"\"\"Retain only numeric columns in the DataFrame.\"\"\"\n        if self._df is not None:\n            self._df = self._df.select_dtypes(include=[np.number])\n            logger.info(\"DataFrame filtered to numeric columns only.\")\n        else:\n            logger.error(\"DataFrame is None. Cannot filter to numeric columns.\")\n\n    def comma_separated_include_columns(self, include_cols: str = \"\"):\n        \"\"\"Retain only specified columns in the DataFrame.\"\"\"\n        if include_cols == \"\":\n            return\n        if self._df is not None:\n            cols = [\n                col.strip()\n                for col in include_cols.split(\",\")\n                if col.strip() and col in self._df.columns\n            ]\n            self._df = self._df[cols]\n            logger.info(f\"DataFrame filtered to include columns: {cols}\")\n        else:\n            logger.error(\"DataFrame is None. Cannot filter to include columns.\")\n\n    def read_xy(self, y: str):\n        \"\"\"\n        Read X and y variables from the DataFrame.\n        \"\"\"\n        if self._df is None:\n            logger.error(\"DataFrame is None. Cannot read X and y.\")\n            return None, None\n        # Split into X and y\n        if y == \"\":\n            self._y = None\n        else:\n            self._y = self._df[y]\n        if y != \"\":\n            self._X = self._df.drop(columns=[y])\n        else:\n            self._X = self._df.copy()\n        logger.info(f\"X and y variables set. X shape: {self._X.shape}\")\n        return self._X, self._y\n\n    def drop_na(self):\n        \"\"\"Drop rows with any NA values from the DataFrame.\"\"\"\n        if self._df is not None:\n            self._df.dropna(inplace=True)\n            logger.info(\"Missing values dropped from DataFrame.\")\n        else:\n            logger.error(\"DataFrame is None. Cannot drop missing values.\")\n\n    def oversample(self, mcp: bool = False):\n        self._X_original = self._X\n        self._y_original = self._y\n        try:\n            from imblearn.over_sampling import RandomOverSampler\n\n            ros = RandomOverSampler(random_state=0)\n        except ImportError:\n            logger.info(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            return\n\n        result = ros.fit_resample(self._X, self._y)\n        if len(result) == 2:\n            X, y = result\n        elif len(result) == 3:\n            X, y, _ = result\n        else:\n            logger.error(\"Unexpected number of values returned from fit_resample.\")\n            return\n        self._X = X\n        self._y = y\n        if mcp:\n            return f\"Oversampling completed. New X shape: {self._X.shape}\"\n        return X, y\n\n    def restore_oversample(self, mcp: bool = False):\n        self._X = self._X_original\n        self._y = self._y_original\n        if mcp:\n            return f\"Oversampling restored. X shape: {self._X.shape}, y shape: {self._y.shape}\"  # type: ignore\n\n    def prepare_data(self, y: str, oversample=False, one_hot_encode_all=False):\n        self.mark_missing()\n        if oversample:\n            self.oversample()\n        self.one_hot_encode_strings_in_df()\n        if one_hot_encode_all:\n            self.one_hot_encode_all_columns()\n        return self.read_xy(y)\n\n    def bin_a_column(self, column_name: str, bins: int = 2):\n        \"\"\"Bin a numeric column into specified number of bins.\"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            if pd.api.types.is_numeric_dtype(self._df[column_name]):\n                self._df[column_name] = pd.cut(\n                    self._df[column_name], bins=bins, labels=False\n                )\n                logger.info(f\"Column {column_name} binned into {bins} bins.\")\n                return \"I have binned the column. Please proceed.\"\n            else:\n                logger.warning(f\"Column {column_name} is not numeric. Cannot bin.\")\n        else:\n            logger.warning(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n        return \"I cannot bin the column. Please check the logs for more information.\"\n\n    def one_hot_encode_column(self, column_name: str):\n        \"\"\"One-hot encode a specific column in the DataFrame.\n        This method converts a categorical column into one-hot encoded columns.\n        Used when # ValueError: could not convert string to float.\n        \"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            if pd.api.types.is_object_dtype(self._df[column_name]):\n                self._df = pd.get_dummies(\n                    self._df, columns=[column_name], drop_first=True\n                )\n                logger.info(f\"One-hot encoding applied to column {column_name}.\")\n                return \"I have one-hot encoded the column. Please proceed.\"\n            else:\n                logger.warning(f\"Column {column_name} is not of object type.\")\n        else:\n            logger.error(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n        return \"I cannot one-hot encode the column. Please check the logs for more information.\"\n\n    def one_hot_encode_strings_in_df(self, n=10, filter_high_cardinality=False):\n        \"\"\"One-hot encode string (object) columns in the DataFrame.\n        This method converts categorical string columns into one-hot encoded columns.\n        Columns with more than n unique values can be optionally filtered out.\n        Used when # ValueError: could not convert string to float.\n        \"\"\"\n        if self._df is not None:\n            categorical_cols = self._df.select_dtypes(\n                include=[\"object\"]\n            ).columns.tolist()\n            # Remove categorical columns with more than n unique values\n            if filter_high_cardinality:\n                categorical_cols = [\n                    col for col in categorical_cols if self._df[col].nunique() &lt;= n\n                ]\n            if categorical_cols:\n                self._df = pd.get_dummies(\n                    self._df, columns=categorical_cols, drop_first=True\n                )\n                logger.info(\"One-hot encoding applied to string columns.\")\n            else:\n                logger.info(\"No string (object) columns found for one-hot encoding.\")\n        else:\n            logger.error(\"DataFrame is None. Cannot apply one-hot encoding.\")\n\n    def one_hot_encode_all_columns(self):\n        \"\"\"One-hot encode all columns in the DataFrame.\n        This method converts all values in the DataFrame to boolean values.\n        Used for apriori algorithm which requires boolean values.\n        \"\"\"\n        if self._df is not None:\n\n            def to_one_hot(x):\n                if x in [1, True]:\n                    return True\n                elif x in [0, False]:\n                    return False\n                else:\n                    # logger.warning(\n                    #     f\"Unexpected value '{x}' encountered during one-hot encoding; mapping to 1.\"\n                    # )\n                    return True\n\n            self._df = self._df.applymap(to_one_hot)  # type: ignore\n\n    def filter_rows_by_column_value(self, column_name: str, value, mcp: bool = False):\n        \"\"\"Select rows from the DataFrame where the specified column matches the given value.\n        Additionally, filter self._corpus.documents by id_column if present in DataFrame.\n        \"\"\"\n        if self._df is not None and column_name in self._df.columns:\n            selected_df = self._df[self._df[column_name] == value]\n            if selected_df.empty:\n                # try int search\n                try:\n                    selected_df = self._df[self._df[column_name] == int(value)]\n                except (ValueError, TypeError):\n                    logger.warning(\n                        f\"Could not convert value '{value}' to int for column '{column_name}'.\"\n                    )\n            logger.info(\n                f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n            )\n            self._df = selected_df\n\n            # Check for id_column in DataFrame\n            if (\n                self._corpus is not None\n                and hasattr(self._corpus, \"df\")\n                and self._id_column in self._corpus.df.columns\n            ):\n                logger.info(f\"id_column '{self._id_column}' exists in DataFrame.\")\n                valid_ids = set(self._corpus.df[self._id_column].tolist())\n                if (\n                    hasattr(self._corpus, \"documents\")\n                    and self._corpus.documents is not None\n                ):\n                    filtered_docs = [\n                        doc\n                        for doc in self._corpus.documents\n                        if getattr(doc, self._id_column, None) in valid_ids\n                    ]\n                    self._corpus.documents = filtered_docs\n            else:\n                logger.warning(\n                    f\"id_column '{self._id_column}' does not exist in DataFrame.\"\n                )\n\n            if mcp:\n                return f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n        else:\n            logger.warning(\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n            if mcp:\n                return (\n                    f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n                )\n            return pd.DataFrame()\n</code></pre>"},{"location":"modules/#csv.Csv.__init__","title":"<code>__init__(corpus=None, comma_separated_text_columns='', comma_separated_ignore_columns='', id_column='id')</code>","text":"<p>Initialize the Csv object.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def __init__(\n    self,\n    corpus: Corpus | None = None,\n    comma_separated_text_columns: str = \"\",\n    comma_separated_ignore_columns: str = \"\",\n    id_column: str = \"id\",\n):\n    \"\"\"\n    Initialize the Csv object.\n    \"\"\"\n    self._corpus = corpus\n    if self._corpus is None:\n        self._df = pd.DataFrame()\n        logger.info(\"No corpus provided. Creating an empty DataFrame.\")\n    else:\n        self._df = self._corpus.df\n        if self._df is None:\n            logger.info(\"No DataFrame found in the corpus. Creating a new one.\")\n            self._df = pd.DataFrame()\n    self._df_original = self._df.copy()\n    self._comma_separated_text_columns = comma_separated_text_columns\n    self._comma_separated_ignore_columns = comma_separated_ignore_columns\n    self._id_column = id_column\n    self._X = None\n    self._y = None\n    self._X_original = None\n    self._y_original = None\n    self._id_column = id_column\n</code></pre>"},{"location":"modules/#csv.Csv.bin_a_column","title":"<code>bin_a_column(column_name, bins=2)</code>","text":"<p>Bin a numeric column into specified number of bins.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def bin_a_column(self, column_name: str, bins: int = 2):\n    \"\"\"Bin a numeric column into specified number of bins.\"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        if pd.api.types.is_numeric_dtype(self._df[column_name]):\n            self._df[column_name] = pd.cut(\n                self._df[column_name], bins=bins, labels=False\n            )\n            logger.info(f\"Column {column_name} binned into {bins} bins.\")\n            return \"I have binned the column. Please proceed.\"\n        else:\n            logger.warning(f\"Column {column_name} is not numeric. Cannot bin.\")\n    else:\n        logger.warning(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n    return \"I cannot bin the column. Please check the logs for more information.\"\n</code></pre>"},{"location":"modules/#csv.Csv.comma_separated_include_columns","title":"<code>comma_separated_include_columns(include_cols='')</code>","text":"<p>Retain only specified columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def comma_separated_include_columns(self, include_cols: str = \"\"):\n    \"\"\"Retain only specified columns in the DataFrame.\"\"\"\n    if include_cols == \"\":\n        return\n    if self._df is not None:\n        cols = [\n            col.strip()\n            for col in include_cols.split(\",\")\n            if col.strip() and col in self._df.columns\n        ]\n        self._df = self._df[cols]\n        logger.info(f\"DataFrame filtered to include columns: {cols}\")\n    else:\n        logger.error(\"DataFrame is None. Cannot filter to include columns.\")\n</code></pre>"},{"location":"modules/#csv.Csv.drop_na","title":"<code>drop_na()</code>","text":"<p>Drop rows with any NA values from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def drop_na(self):\n    \"\"\"Drop rows with any NA values from the DataFrame.\"\"\"\n    if self._df is not None:\n        self._df.dropna(inplace=True)\n        logger.info(\"Missing values dropped from DataFrame.\")\n    else:\n        logger.error(\"DataFrame is None. Cannot drop missing values.\")\n</code></pre>"},{"location":"modules/#csv.Csv.filter_rows_by_column_value","title":"<code>filter_rows_by_column_value(column_name, value, mcp=False)</code>","text":"<p>Select rows from the DataFrame where the specified column matches the given value. Additionally, filter self._corpus.documents by id_column if present in DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def filter_rows_by_column_value(self, column_name: str, value, mcp: bool = False):\n    \"\"\"Select rows from the DataFrame where the specified column matches the given value.\n    Additionally, filter self._corpus.documents by id_column if present in DataFrame.\n    \"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        selected_df = self._df[self._df[column_name] == value]\n        if selected_df.empty:\n            # try int search\n            try:\n                selected_df = self._df[self._df[column_name] == int(value)]\n            except (ValueError, TypeError):\n                logger.warning(\n                    f\"Could not convert value '{value}' to int for column '{column_name}'.\"\n                )\n        logger.info(\n            f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n        )\n        self._df = selected_df\n\n        # Check for id_column in DataFrame\n        if (\n            self._corpus is not None\n            and hasattr(self._corpus, \"df\")\n            and self._id_column in self._corpus.df.columns\n        ):\n            logger.info(f\"id_column '{self._id_column}' exists in DataFrame.\")\n            valid_ids = set(self._corpus.df[self._id_column].tolist())\n            if (\n                hasattr(self._corpus, \"documents\")\n                and self._corpus.documents is not None\n            ):\n                filtered_docs = [\n                    doc\n                    for doc in self._corpus.documents\n                    if getattr(doc, self._id_column, None) in valid_ids\n                ]\n                self._corpus.documents = filtered_docs\n        else:\n            logger.warning(\n                f\"id_column '{self._id_column}' does not exist in DataFrame.\"\n            )\n\n        if mcp:\n            return f\"Selected {selected_df.shape[0]} rows where {column_name} == {value}.\"\n    else:\n        logger.warning(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n        if mcp:\n            return (\n                f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n            )\n        return pd.DataFrame()\n</code></pre>"},{"location":"modules/#csv.Csv.get_column_types","title":"<code>get_column_types()</code>","text":"<p>Get the data types of columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def get_column_types(self):\n    \"\"\"Get the data types of columns in the DataFrame.\"\"\"\n    if self._df is not None:\n        return self._df.dtypes.to_dict()\n    else:\n        logger.error(\"DataFrame is None. Cannot get column types.\")\n        return {}\n</code></pre>"},{"location":"modules/#csv.Csv.get_column_values","title":"<code>get_column_values(column_name)</code>","text":"<p>Get the unique values in a column of the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def get_column_values(self, column_name: str):\n    \"\"\"Get the unique values in a column of the DataFrame.\"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        return self._df[column_name].tolist()\n    else:\n        logger.error(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n        return None\n</code></pre>"},{"location":"modules/#csv.Csv.get_columns","title":"<code>get_columns()</code>","text":"<p>Get the list of columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def get_columns(self):\n    \"\"\"Get the list of columns in the DataFrame.\"\"\"\n    if self._df is not None:\n        return self._df.columns.tolist()\n    else:\n        logger.error(\"DataFrame is None. Cannot get columns.\")\n        return []\n</code></pre>"},{"location":"modules/#csv.Csv.mark_duplicates","title":"<code>mark_duplicates()</code>","text":"<p>Mark duplicate rows in the DataFrame. Duplicate rows are identified and dropped from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def mark_duplicates(self):\n    \"\"\"Mark duplicate rows in the DataFrame.\n    Duplicate rows are identified and dropped from the DataFrame.\n    \"\"\"\n    if self._df is not None:\n        self._df.drop_duplicates(inplace=True)\n    else:\n        logger.error(\"DataFrame is None. Cannot mark duplicates.\")\n</code></pre>"},{"location":"modules/#csv.Csv.mark_missing","title":"<code>mark_missing()</code>","text":"<p>Mark missing values in the DataFrame. Missing values are considered as empty strings and are replaced with NaN. Rows with NaN values are then dropped from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def mark_missing(self):\n    \"\"\"Mark missing values in the DataFrame.\n    Missing values are considered as empty strings and are replaced with NaN.\n    Rows with NaN values are then dropped from the DataFrame.\n    \"\"\"\n    if self._df is not None:\n        self._df.replace(\"\", np.nan, inplace=True)\n        self._df.dropna(inplace=True)\n    else:\n        logger.error(\"DataFrame is None. Cannot mark missing values.\")\n</code></pre>"},{"location":"modules/#csv.Csv.one_hot_encode_all_columns","title":"<code>one_hot_encode_all_columns()</code>","text":"<p>One-hot encode all columns in the DataFrame. This method converts all values in the DataFrame to boolean values. Used for apriori algorithm which requires boolean values.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def one_hot_encode_all_columns(self):\n    \"\"\"One-hot encode all columns in the DataFrame.\n    This method converts all values in the DataFrame to boolean values.\n    Used for apriori algorithm which requires boolean values.\n    \"\"\"\n    if self._df is not None:\n\n        def to_one_hot(x):\n            if x in [1, True]:\n                return True\n            elif x in [0, False]:\n                return False\n            else:\n                # logger.warning(\n                #     f\"Unexpected value '{x}' encountered during one-hot encoding; mapping to 1.\"\n                # )\n                return True\n\n        self._df = self._df.applymap(to_one_hot)  # type: ignore\n</code></pre>"},{"location":"modules/#csv.Csv.one_hot_encode_column","title":"<code>one_hot_encode_column(column_name)</code>","text":"<p>One-hot encode a specific column in the DataFrame. This method converts a categorical column into one-hot encoded columns. Used when # ValueError: could not convert string to float.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def one_hot_encode_column(self, column_name: str):\n    \"\"\"One-hot encode a specific column in the DataFrame.\n    This method converts a categorical column into one-hot encoded columns.\n    Used when # ValueError: could not convert string to float.\n    \"\"\"\n    if self._df is not None and column_name in self._df.columns:\n        if pd.api.types.is_object_dtype(self._df[column_name]):\n            self._df = pd.get_dummies(\n                self._df, columns=[column_name], drop_first=True\n            )\n            logger.info(f\"One-hot encoding applied to column {column_name}.\")\n            return \"I have one-hot encoded the column. Please proceed.\"\n        else:\n            logger.warning(f\"Column {column_name} is not of object type.\")\n    else:\n        logger.error(\n            f\"Column {column_name} not found in DataFrame or DataFrame is None.\"\n        )\n    return \"I cannot one-hot encode the column. Please check the logs for more information.\"\n</code></pre>"},{"location":"modules/#csv.Csv.one_hot_encode_strings_in_df","title":"<code>one_hot_encode_strings_in_df(n=10, filter_high_cardinality=False)</code>","text":"<p>One-hot encode string (object) columns in the DataFrame. This method converts categorical string columns into one-hot encoded columns. Columns with more than n unique values can be optionally filtered out. Used when # ValueError: could not convert string to float.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def one_hot_encode_strings_in_df(self, n=10, filter_high_cardinality=False):\n    \"\"\"One-hot encode string (object) columns in the DataFrame.\n    This method converts categorical string columns into one-hot encoded columns.\n    Columns with more than n unique values can be optionally filtered out.\n    Used when # ValueError: could not convert string to float.\n    \"\"\"\n    if self._df is not None:\n        categorical_cols = self._df.select_dtypes(\n            include=[\"object\"]\n        ).columns.tolist()\n        # Remove categorical columns with more than n unique values\n        if filter_high_cardinality:\n            categorical_cols = [\n                col for col in categorical_cols if self._df[col].nunique() &lt;= n\n            ]\n        if categorical_cols:\n            self._df = pd.get_dummies(\n                self._df, columns=categorical_cols, drop_first=True\n            )\n            logger.info(\"One-hot encoding applied to string columns.\")\n        else:\n            logger.info(\"No string (object) columns found for one-hot encoding.\")\n    else:\n        logger.error(\"DataFrame is None. Cannot apply one-hot encoding.\")\n</code></pre>"},{"location":"modules/#csv.Csv.read_csv","title":"<code>read_csv(file_path)</code>","text":"<p>Read a CSV file and create a DataFrame. Handles invalid UTF-8 byte sequences by ignoring them.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def read_csv(self, file_path: str):\n    \"\"\"\n    Read a CSV file and create a DataFrame.\n    Handles invalid UTF-8 byte sequences by ignoring them.\n    \"\"\"\n    try:\n        self._df = pd.read_csv(file_path, encoding=\"utf-8\", on_bad_lines=\"skip\")\n        logger.info(f\"CSV file {file_path} read successfully.\")\n        logger.debug(f\"DataFrame content: {self._df.head()}\")\n        logger.debug(f\"DataFrame shape: {self._df.shape}\")\n        logger.debug(f\"DataFrame columns: {self._df.columns.tolist()}\")\n    except Exception as e:\n        logger.exception(f\"Error reading CSV file: {e}\")\n        raise\n    return self._process_columns()\n</code></pre>"},{"location":"modules/#csv.Csv.read_xy","title":"<code>read_xy(y)</code>","text":"<p>Read X and y variables from the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def read_xy(self, y: str):\n    \"\"\"\n    Read X and y variables from the DataFrame.\n    \"\"\"\n    if self._df is None:\n        logger.error(\"DataFrame is None. Cannot read X and y.\")\n        return None, None\n    # Split into X and y\n    if y == \"\":\n        self._y = None\n    else:\n        self._y = self._df[y]\n    if y != \"\":\n        self._X = self._df.drop(columns=[y])\n    else:\n        self._X = self._df.copy()\n    logger.info(f\"X and y variables set. X shape: {self._X.shape}\")\n    return self._X, self._y\n</code></pre>"},{"location":"modules/#csv.Csv.retain_numeric_columns_only","title":"<code>retain_numeric_columns_only()</code>","text":"<p>Retain only numeric columns in the DataFrame.</p> Source code in <code>src/crisp_t/csv.py</code> <pre><code>def retain_numeric_columns_only(self):\n    \"\"\"Retain only numeric columns in the DataFrame.\"\"\"\n    if self._df is not None:\n        self._df = self._df.select_dtypes(include=[np.number])\n        logger.info(\"DataFrame filtered to numeric columns only.\")\n    else:\n        logger.error(\"DataFrame is None. Cannot filter to numeric columns.\")\n</code></pre>"},{"location":"modules/#text.Text","title":"<code>Text</code>","text":"Source code in <code>src/crisp_t/text.py</code> <pre><code>class Text:\n\n    def __init__(\n        self, corpus: Corpus | None = None, lang=\"en_core_web_sm\", max_length=1100000\n    ):\n        self._corpus = corpus\n        self._lang = lang\n        self._spacy_manager = SpacyManager(self._lang)\n        self._max_length = max_length\n        self._initial_document_count = len(self._corpus.documents) if corpus else 0  # type: ignore\n\n        self._spacy_doc = None\n        self._lemma = {}\n        self._pos = {}\n        self._pos_ = {}\n        self._word = {}\n        self._sentiment = {}\n        self._tag = {}\n        self._dep = {}\n        self._prob = {}\n        self._idx = {}\n\n    @property\n    def corpus(self):\n        \"\"\"\n        Get the corpus.\n        \"\"\"\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        return self._corpus\n\n    @property\n    def max_length(self):\n        \"\"\"\n        Get the maximum length of the corpus.\n        \"\"\"\n        return self._max_length\n\n    @property\n    def lang(self):\n        \"\"\"\n        Get the language of the corpus.\n        \"\"\"\n        return self._lang\n\n    @property\n    def initial_document_count(self):\n        \"\"\"\n        Get the initial document count.\n        \"\"\"\n        return self._initial_document_count\n\n    @corpus.setter\n    def corpus(self, corpus: Corpus):\n        \"\"\"\n        Set the corpus.\n        \"\"\"\n        if not isinstance(corpus, Corpus):\n            raise ValueError(\"Corpus must be of type Corpus\")\n        self._corpus = corpus\n        spacy_doc, results = self.process_tokens(self._corpus.id if self._corpus else None)\n        self._spacy_doc = spacy_doc\n        self._lemma = results[\"lemma\"]\n        self._pos = results[\"pos\"]\n        self._pos_ = results[\"pos_\"]\n        self._word = results[\"word\"]\n        self._sentiment = results[\"sentiment\"]\n        self._tag = results[\"tag\"]\n        self._dep = results[\"dep\"]\n        self._prob = results[\"prob\"]\n        self._idx = results[\"idx\"]\n\n    @max_length.setter\n    def max_length(self, max_length: int):\n        \"\"\"\n        Set the maximum length of the corpus.\n        \"\"\"\n        if not isinstance(max_length, int):\n            raise ValueError(\"max_length must be an integer\")\n        self._max_length = max_length\n        if self._spacy_doc is not None:\n            self._spacy_doc.max_length = max_length\n\n    @lang.setter\n    def lang(self, lang: str):\n        \"\"\"\n        Set the language of the corpus.\n        \"\"\"\n        if not isinstance(lang, str):\n            raise ValueError(\"lang must be a string\")\n        self._lang = lang\n        spacy_doc, results = self.process_tokens(self._corpus.id if self._corpus else None)\n        self._spacy_doc = spacy_doc\n        self._lemma = results[\"lemma\"]\n        self._pos = results[\"pos\"]\n        self._pos_ = results[\"pos_\"]\n        self._word = results[\"word\"]\n        self._sentiment = results[\"sentiment\"]\n        self._tag = results[\"tag\"]\n        self._dep = results[\"dep\"]\n        self._prob = results[\"prob\"]\n        self._idx = results[\"idx\"]\n\n    def make_spacy_doc(self):\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        # Use list and join for efficient string concatenation instead of +=\n        text_parts = []\n        for document in tqdm(\n            self._corpus.documents,\n            desc=\"Processing documents\",\n            disable=len(self._corpus.documents) &lt; 10,\n        ):\n            text_parts.append(self.process_text(document.text))\n        text = \" \\n\".join(text_parts)\n        nlp = self._spacy_manager.get_model()\n        nlp.max_length = self._max_length\n        if len(text) &gt; self._max_length:\n            logger.warning(\n                f\"Text length {len(text)} exceeds max_length {self._max_length}.\"\n            )\n            text_chunks = [\n                text[i : i + self._max_length]\n                for i in range(0, len(text), self._max_length)\n            ]\n            spacy_docs = []\n            for chunk in tqdm(\n                text_chunks, desc=\"Processing text as chunks of max_length\"\n            ):\n                spacy_doc = nlp(chunk)\n                spacy_docs.append(spacy_doc)\n            self._spacy_doc = spacy_docs[0]\n            for doc in tqdm(spacy_docs[1:], desc=\"Merging spacy docs\"):\n                self._spacy_doc = Doc.from_docs([self._spacy_doc, doc])  # type: ignore\n        else:\n            self._spacy_doc = nlp(text)\n        return self._spacy_doc\n\n    # @lru_cache(maxsize=3)\n    def make_each_document_into_spacy_doc(self, id=\"corpus\"):\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n\n        # ! if cached file exists, load it\n        cache_dir = Path(\"cache\")\n        cache_file = cache_dir / f\"spacy_docs_{id}.pkl\"\n        if cache_file.exists():\n            with open(cache_file, \"rb\") as f:\n                spacy_docs, ids = pickle.load(f)\n            # logger.info(\"Loaded cached spacy docs and ids.\")\n            return spacy_docs, ids\n\n        spacy_docs = []\n        ids = []\n        # Load SpaCy model once outside the loop for efficiency\n        nlp = self._spacy_manager.get_model()\n        nlp.max_length = self._max_length\n        for document in tqdm(\n            self._corpus.documents,\n            desc=\"Creating spacy docs\",\n            disable=len(self._corpus.documents) &lt; 10,\n        ):\n            text = self.process_text(document.text)\n            spacy_doc = nlp(text)\n            spacy_docs.append(spacy_doc)\n            ids.append(document.id)\n\n        # ! dump spacy_docs, ids to a file for caching with the corpus id\n        cache_dir = Path(\"cache\")\n        cache_dir.mkdir(exist_ok=True)\n        cache_file = cache_dir / f\"spacy_docs_{id}.pkl\"\n        with open(cache_file, \"wb\") as f:\n            pickle.dump((spacy_docs, ids), f)\n        return spacy_docs, ids\n\n    def process_text(self, text: str) -&gt; str:\n        \"\"\"\n        Process the text by removing unwanted characters and normalizing it.\n        \"\"\"\n        # Remove unwanted characters\n        text = preprocessing.replace.urls(text)\n        text = preprocessing.replace.emails(text)\n        text = preprocessing.replace.phone_numbers(text)\n        text = preprocessing.replace.currency_symbols(text)\n        text = preprocessing.replace.hashtags(text)\n        text = preprocessing.replace.numbers(text)\n\n        # lowercase the text\n        text = text.lower()\n        return text\n\n    # @lru_cache(maxsize=3)\n    def process_tokens(self, id=\"corpus\"):\n        \"\"\"\n        Process tokens in the spacy document and extract relevant information.\n        \"\"\"\n\n        # ! if cached file exists, load it\n        cache_dir = Path(\"cache\")\n        cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n        if cache_file.exists():\n            with open(cache_file, \"rb\") as f:\n                spacy_doc, results = pickle.load(f)\n            # logger.info(\"Loaded cached spacy doc and results.\")\n            return spacy_doc, results\n\n        spacy_doc = self.make_spacy_doc()\n        logger.info(\"Spacy doc created.\")\n\n        n_cores = multiprocessing.cpu_count()\n\n        def process_token(token):\n            if token.is_stop or token.is_digit or token.is_punct or token.is_space:\n                return None\n            if token.like_url or token.like_num or token.like_email:\n                return None\n            if len(token.text) &lt; 3 or token.text.isupper():\n                return None\n            return {\n                \"text\": token.text,\n                \"lemma\": token.lemma_,\n                \"pos\": token.pos_,\n                \"pos_\": token.pos,\n                \"word\": token.lemma_,\n                \"sentiment\": token.sentiment,\n                \"tag\": token.tag_,\n                \"dep\": token.dep_,\n                \"prob\": token.prob,\n                \"idx\": token.idx,\n            }\n\n        tokens = list(spacy_doc)\n        _lemma = {}\n        _pos = {}\n        _pos_ = {}\n        _word = {}\n        _sentiment = {}\n        _tag = {}\n        _dep = {}\n        _prob = {}\n        _idx = {}\n        with ThreadPoolExecutor() as executor:\n            futures = {executor.submit(process_token, token): token for token in tokens}\n            with tqdm(\n                total=len(futures),\n                desc=f\"Processing tokens (parallel, {n_cores} cores)\",\n            ) as pbar:\n                for future in as_completed(futures):\n                    result = future.result()\n                    if result is not None:\n                        _lemma[result[\"text\"]] = result[\"lemma\"]\n                        _pos[result[\"text\"]] = result[\"pos\"]\n                        _pos_[result[\"text\"]] = result[\"pos_\"]\n                        _word[result[\"text\"]] = result[\"word\"]\n                        _sentiment[result[\"text\"]] = result[\"sentiment\"]\n                        _tag = result[\"tag\"]\n                        _dep = result[\"dep\"]\n                        _prob = result[\"prob\"]\n                        _idx = result[\"idx\"]\n                    pbar.update(1)\n        logger.info(\"Token processing complete.\")\n        results = {\n            \"lemma\": _lemma,\n            \"pos\": _pos,\n            \"pos_\": _pos_,\n            \"word\": _word,\n            \"sentiment\": _sentiment,\n            \"tag\": _tag,\n            \"dep\": _dep,\n            \"prob\": _prob,\n            \"idx\": _idx,\n        }\n        # ! dump spacy_doc, results to a file for caching with the corpus id\n        cache_dir = Path(\"cache\")\n        cache_dir.mkdir(exist_ok=True)\n        cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n        with open(cache_file, \"wb\") as f:\n            pickle.dump((spacy_doc, results), f)\n\n        return spacy_doc, results\n\n    def map_spacy_doc(self):\n        spacy_doc, results = self.process_tokens(self._corpus.id if self._corpus else None)\n        self._spacy_doc = spacy_doc\n        self._lemma = results[\"lemma\"]\n        self._pos = results[\"pos\"]\n        self._pos_ = results[\"pos_\"]\n        self._word = results[\"word\"]\n        self._sentiment = results[\"sentiment\"]\n        self._tag = results[\"tag\"]\n        self._dep = results[\"dep\"]\n        self._prob = results[\"prob\"]\n        self._idx = results[\"idx\"]\n\n    def common_words(self, index=10):\n        self.map_spacy_doc()\n        _words = {}\n        for key, value in self._word.items():\n            _words[value] = _words.get(value, 0) + 1\n        return sorted(_words.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def common_nouns(self, index=10):\n        self.map_spacy_doc()\n        _words = {}\n        for key, value in self._word.items():\n            if self._pos.get(key, None) == \"NOUN\":\n                _words[value] = _words.get(value, 0) + 1\n        return sorted(_words.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def common_verbs(self, index=10):\n        self.map_spacy_doc()\n        _words = {}\n        for key, value in self._word.items():\n            if self._pos.get(key, None) == \"VERB\":\n                _words[value] = _words.get(value, 0) + 1\n        return sorted(_words.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def print_coding_dictionary(self, num=10, top_n=5):\n        \"\"\"Prints a coding dictionary based on common verbs, attributes, and dimensions.\n        \"CATEGORY\" is the common verb\n        \"PROPERTY\" is the common nouns associated with the verb\n        \"DIMENSION\" is the common adjectives/adverbs/verbs associated with the property\n        Args:\n            num (int, optional): Number of common verbs to consider. Defaults to 10.\n            top_n (int, optional): Number of top attributes and dimensions to consider for each verb. Defaults to 5.\n\n        \"\"\"\n        self.map_spacy_doc()\n        output = []\n        coding_dict = []\n        output.append((\"CATEGORY\", \"PROPERTY\", \"DIMENSION\"))\n        verbs = self.common_verbs(num)\n        _verbs = []\n        for verb, freq in verbs:\n            _verbs.append(verb)\n        for verb, freq in verbs:\n            for attribute, f2 in self.attributes(verb, top_n):\n                for dimension, f3 in self.dimensions(attribute, top_n):\n                    if dimension not in _verbs:\n                        output.append((verb, attribute, dimension))\n                        coding_dict.append(f\"{verb} &gt; {attribute} &gt; {dimension}\")\n        # Add coding_dict to corpus metadata\n        if self._corpus is not None:\n            self._corpus.metadata[\"coding_dict\"] = coding_dict\n        print(\"\\n---Coding Dictionary---\")\n        QRUtils.print_table(output)\n        print(\"---------------------------\\n\")\n        return output\n\n    def sentences_with_common_nouns(self, index=10):\n        self.map_spacy_doc()\n        _nouns = self.common_nouns(index)\n        # Let's look at the sentences\n        sents = []\n        # Ensure self._spacy_doc is initialized\n        if self._spacy_doc is None:\n            self._spacy_doc = self.make_spacy_doc()\n        # the \"sents\" property returns spans\n        # spans have indices into the original string\n        # where each index value represents a token\n        for span in self._spacy_doc.sents:\n            # go from the start to the end of each span, returning each token in the sentence\n            # combine each token using join()\n            sent = \" \".join(\n                self._spacy_doc[i].text for i in range(span.start, span.end)\n            ).strip()\n            for noun, freq in _nouns:\n                if noun in sent:\n                    sents.append(sent)\n        return sents\n\n    def spans_with_common_nouns(self, word):\n        self.map_spacy_doc()\n        # Let's look at the sentences\n        spans = []\n        # the \"sents\" property returns spans\n        # spans have indices into the original string\n        # where each index value represents a token\n        if self._spacy_doc is None:\n            self._spacy_doc = self.make_spacy_doc()\n        for span in self._spacy_doc.sents:\n            # go from the start to the end of each span, returning each token in the sentence\n            # combine each token using join()\n            for token in span.text.split():\n                if word in self._word.get(token, \" \"):\n                    spans.append(span)\n        return spans\n\n    def dimensions(self, word, index=3):\n        self.map_spacy_doc()\n        _spans = self.spans_with_common_nouns(word)\n        _ad = {}\n        for span in _spans:\n            for token in span.text.split():\n                if self._pos.get(token, None) == \"ADJ\":\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n                if self._pos.get(token, None) == \"ADV\":\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n                if self._pos.get(token, None) == \"VERB\":\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n        return sorted(_ad.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    def attributes(self, word, index=3):\n        self.map_spacy_doc()\n        _spans = self.spans_with_common_nouns(word)\n        _ad = {}\n        for span in _spans:\n            for token in span.text.split():\n                if self._pos.get(token, None) == \"NOUN\" and word not in self._word.get(\n                    token, \"\"\n                ):\n                    _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n                    # if self._pos.get(token, None) == 'VERB':\n                    # _ad[self._word.get(token)] = _ad.get(self._word.get(token), 0) + 1\n        return sorted(_ad.items(), key=operator.itemgetter(1), reverse=True)[:index]\n\n    # filter documents in the corpus based on metadata\n    def filter_documents(self, metadata_key, metadata_value, mcp=False, id_column=\"id\"):\n        \"\"\"\n        Filter documents in the corpus based on metadata.\n        If id_column exists in self._corpus.df, filter the DataFrame to match filtered documents' ids.\n        \"\"\"\n        # * filter does not require spacy mapping\n        # self.map_spacy_doc()\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        filtered_documents = []\n        for document in tqdm(\n            self._corpus.documents,\n            desc=\"Filtering documents\",\n            disable=len(self._corpus.documents) &lt; 10,\n        ):\n            meta_val = document.metadata.get(metadata_key)\n            # Check meta_val is not None and is iterable (str, list, tuple, set)\n            if meta_val is not None and isinstance(meta_val, (str, list, tuple, set)) and metadata_value in meta_val:\n                filtered_documents.append(document)\n            # Check document.id and document.text are not None and are str\n            if isinstance(document.id, str) and metadata_value in document.id:\n                filtered_documents.append(document)\n            if isinstance(document.name, str) and metadata_value in document.name:\n                filtered_documents.append(document)\n        self._corpus.documents = filtered_documents\n\n        # Check for id_column in self._corpus.df and filter df if present\n        if (\n            hasattr(self._corpus, \"df\")\n            and self._corpus.df is not None\n            and id_column in self._corpus.df.columns\n        ):\n            logger.info(f\"id_column '{id_column}' exists in DataFrame.\")\n            filtered_ids = [doc.id for doc in filtered_documents]\n            # Convert id_column to string before comparison\n            self._corpus.df = self._corpus.df[\n                self._corpus.df[id_column]\n                .astype(str)\n                .isin([str(i) for i in filtered_ids])\n            ]\n        else:\n            logger.warning(f\"id_column '{id_column}' does not exist in DataFrame.\")\n\n        if mcp:\n            return f\"Filtered {len(filtered_documents)} documents with {metadata_key} containing {metadata_value}\"\n        return filtered_documents\n\n    # get the count of documents in the corpus\n    def document_count(self):\n        \"\"\"\n        Get the count of documents in the corpus.\n        \"\"\"\n        if self._corpus is None:\n            raise ValueError(\"Corpus is not set\")\n        return len(self._corpus.documents)\n\n    def generate_summary(self, weight=10):\n        \"\"\"[summary]\n\n        Args:\n            weight (int, optional): Parameter for summary generation weight. Defaults to 10.\n\n        Returns:\n            list: A list of summary lines\n        \"\"\"\n        self.map_spacy_doc()\n        words = self.common_words()\n        spans = []\n        ct = 0\n        for key, value in words:\n            ct += 1\n            if ct &gt; weight:\n                continue\n            for span in self.spans_with_common_nouns(key):\n                spans.append(span.text)\n        if self._corpus is not None:\n            self._corpus.metadata[\"summary\"] = list(\n                dict.fromkeys(spans)\n            )  # remove duplicates\n        return list(dict.fromkeys(spans))  # remove duplicates\n\n    def print_categories(self, spacy_doc=None, num=10):\n        self.map_spacy_doc()\n        bot = self._spacy_doc._.to_bag_of_terms( # type: ignore\n            by=\"lemma_\",\n            weighting=\"freq\",\n            ngs=(1, 2, 3),\n            ents=True,\n            ncs=True,\n            dedupe=True,\n        )\n        categories = sorted(bot.items(), key=lambda x: x[1], reverse=True)[:num]\n        output = []\n        to_return = []\n        print(\"\\n---Categories with count---\")\n        output.append((\"CATEGORY\", \"WEIGHT\"))\n        for category, count in categories:\n            output.append((category, str(count)))\n            to_return.append(category)\n        QRUtils.print_table(output)\n        print(\"---------------------------\\n\")\n        if self._corpus is not None:\n            self._corpus.metadata[\"categories\"] = output\n        return to_return\n\n    def category_basket(self, num=10):\n        item_basket = []\n        spacy_docs, ids = self.make_each_document_into_spacy_doc()\n        for spacy_doc in spacy_docs:\n            item_basket.append(self.print_categories(spacy_doc, num))\n        documents_copy = []\n        documents = self._corpus.documents if self._corpus is not None else []\n        # add cateogies to respective documents\n        for i, document in enumerate(documents):\n            if i &lt; len(item_basket):\n                document.metadata[\"categories\"] = item_basket[i]\n                documents_copy.append(document)\n        # update the corpus with the new documents\n        if self._corpus is not None:\n            self._corpus.documents = documents_copy\n        return item_basket\n        # Example return:\n        # [['GT', 'Strauss', 'coding', 'ground', 'theory', 'seminal', 'Corbin', 'code',\n        # 'structure', 'ground theory'], ['category', 'theory', 'comparison', 'incident',\n        # 'GT', 'structure', 'coding', 'Classical', 'Grounded', 'Theory'],\n        # ['theory', 'GT', 'evaluation'], ['open', 'coding', 'category', 'QRMine',\n        # 'open coding', 'researcher', 'step', 'data', 'break', 'analytically'],\n        # ['ground', 'theory', 'GT', 'ground theory'], ['category', 'comparison', 'incident',\n        # 'category comparison', 'Theory', 'theory']]\n\n    def category_association(self, num=10):\n        \"\"\"Generates the support for itemsets\n\n        Args:\n            num (int, optional): number of categories to generate for each doc in corpus. . Defaults to 10.\n        \"\"\"\n        self.map_spacy_doc()\n        basket = self.category_basket(num)\n        te = TransactionEncoder()\n        te_ary = te.fit(basket).transform(basket)\n        df = pd.DataFrame(te_ary, columns=te.columns_)  # type: ignore\n        _apriori = apriori(df, min_support=0.6, use_colnames=True)\n        # Example\n        #    support      itemsets\n        # 0  0.666667          (GT)\n        # 1  0.833333      (theory)\n        # 2  0.666667  (theory, GT)\n        documents_copy = []\n        documents = self._corpus.documents if self._corpus is not None else []\n        # TODO (Change) Add association rules to each document\n        for i, document in enumerate(documents):\n            if i &lt; len(basket):\n                # ! fix document.metadata[\"association_rules\"] = _apriori #TODO This is a corpus metadata, not a document one\n                documents_copy.append(document)\n        # Add to corpus metadata\n        if self._corpus is not None:\n            self._corpus.metadata[\"association_rules\"] = _apriori\n        # Update the corpus with the new documents\n        if self._corpus is not None:\n            self._corpus.documents = documents_copy\n        return _apriori\n</code></pre>"},{"location":"modules/#text.Text.corpus","title":"<code>corpus</code>  <code>property</code> <code>writable</code>","text":"<p>Get the corpus.</p>"},{"location":"modules/#text.Text.initial_document_count","title":"<code>initial_document_count</code>  <code>property</code>","text":"<p>Get the initial document count.</p>"},{"location":"modules/#text.Text.lang","title":"<code>lang</code>  <code>property</code> <code>writable</code>","text":"<p>Get the language of the corpus.</p>"},{"location":"modules/#text.Text.max_length","title":"<code>max_length</code>  <code>property</code> <code>writable</code>","text":"<p>Get the maximum length of the corpus.</p>"},{"location":"modules/#text.Text.category_association","title":"<code>category_association(num=10)</code>","text":"<p>Generates the support for itemsets</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>number of categories to generate for each doc in corpus. . Defaults to 10.</p> <code>10</code> Source code in <code>src/crisp_t/text.py</code> <pre><code>def category_association(self, num=10):\n    \"\"\"Generates the support for itemsets\n\n    Args:\n        num (int, optional): number of categories to generate for each doc in corpus. . Defaults to 10.\n    \"\"\"\n    self.map_spacy_doc()\n    basket = self.category_basket(num)\n    te = TransactionEncoder()\n    te_ary = te.fit(basket).transform(basket)\n    df = pd.DataFrame(te_ary, columns=te.columns_)  # type: ignore\n    _apriori = apriori(df, min_support=0.6, use_colnames=True)\n    # Example\n    #    support      itemsets\n    # 0  0.666667          (GT)\n    # 1  0.833333      (theory)\n    # 2  0.666667  (theory, GT)\n    documents_copy = []\n    documents = self._corpus.documents if self._corpus is not None else []\n    # TODO (Change) Add association rules to each document\n    for i, document in enumerate(documents):\n        if i &lt; len(basket):\n            # ! fix document.metadata[\"association_rules\"] = _apriori #TODO This is a corpus metadata, not a document one\n            documents_copy.append(document)\n    # Add to corpus metadata\n    if self._corpus is not None:\n        self._corpus.metadata[\"association_rules\"] = _apriori\n    # Update the corpus with the new documents\n    if self._corpus is not None:\n        self._corpus.documents = documents_copy\n    return _apriori\n</code></pre>"},{"location":"modules/#text.Text.document_count","title":"<code>document_count()</code>","text":"<p>Get the count of documents in the corpus.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def document_count(self):\n    \"\"\"\n    Get the count of documents in the corpus.\n    \"\"\"\n    if self._corpus is None:\n        raise ValueError(\"Corpus is not set\")\n    return len(self._corpus.documents)\n</code></pre>"},{"location":"modules/#text.Text.filter_documents","title":"<code>filter_documents(metadata_key, metadata_value, mcp=False, id_column='id')</code>","text":"<p>Filter documents in the corpus based on metadata. If id_column exists in self._corpus.df, filter the DataFrame to match filtered documents' ids.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def filter_documents(self, metadata_key, metadata_value, mcp=False, id_column=\"id\"):\n    \"\"\"\n    Filter documents in the corpus based on metadata.\n    If id_column exists in self._corpus.df, filter the DataFrame to match filtered documents' ids.\n    \"\"\"\n    # * filter does not require spacy mapping\n    # self.map_spacy_doc()\n    if self._corpus is None:\n        raise ValueError(\"Corpus is not set\")\n    filtered_documents = []\n    for document in tqdm(\n        self._corpus.documents,\n        desc=\"Filtering documents\",\n        disable=len(self._corpus.documents) &lt; 10,\n    ):\n        meta_val = document.metadata.get(metadata_key)\n        # Check meta_val is not None and is iterable (str, list, tuple, set)\n        if meta_val is not None and isinstance(meta_val, (str, list, tuple, set)) and metadata_value in meta_val:\n            filtered_documents.append(document)\n        # Check document.id and document.text are not None and are str\n        if isinstance(document.id, str) and metadata_value in document.id:\n            filtered_documents.append(document)\n        if isinstance(document.name, str) and metadata_value in document.name:\n            filtered_documents.append(document)\n    self._corpus.documents = filtered_documents\n\n    # Check for id_column in self._corpus.df and filter df if present\n    if (\n        hasattr(self._corpus, \"df\")\n        and self._corpus.df is not None\n        and id_column in self._corpus.df.columns\n    ):\n        logger.info(f\"id_column '{id_column}' exists in DataFrame.\")\n        filtered_ids = [doc.id for doc in filtered_documents]\n        # Convert id_column to string before comparison\n        self._corpus.df = self._corpus.df[\n            self._corpus.df[id_column]\n            .astype(str)\n            .isin([str(i) for i in filtered_ids])\n        ]\n    else:\n        logger.warning(f\"id_column '{id_column}' does not exist in DataFrame.\")\n\n    if mcp:\n        return f\"Filtered {len(filtered_documents)} documents with {metadata_key} containing {metadata_value}\"\n    return filtered_documents\n</code></pre>"},{"location":"modules/#text.Text.generate_summary","title":"<code>generate_summary(weight=10)</code>","text":"<p>[summary]</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>int</code> <p>Parameter for summary generation weight. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of summary lines</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def generate_summary(self, weight=10):\n    \"\"\"[summary]\n\n    Args:\n        weight (int, optional): Parameter for summary generation weight. Defaults to 10.\n\n    Returns:\n        list: A list of summary lines\n    \"\"\"\n    self.map_spacy_doc()\n    words = self.common_words()\n    spans = []\n    ct = 0\n    for key, value in words:\n        ct += 1\n        if ct &gt; weight:\n            continue\n        for span in self.spans_with_common_nouns(key):\n            spans.append(span.text)\n    if self._corpus is not None:\n        self._corpus.metadata[\"summary\"] = list(\n            dict.fromkeys(spans)\n        )  # remove duplicates\n    return list(dict.fromkeys(spans))  # remove duplicates\n</code></pre>"},{"location":"modules/#text.Text.print_coding_dictionary","title":"<code>print_coding_dictionary(num=10, top_n=5)</code>","text":"<p>Prints a coding dictionary based on common verbs, attributes, and dimensions. \"CATEGORY\" is the common verb \"PROPERTY\" is the common nouns associated with the verb \"DIMENSION\" is the common adjectives/adverbs/verbs associated with the property Args:     num (int, optional): Number of common verbs to consider. Defaults to 10.     top_n (int, optional): Number of top attributes and dimensions to consider for each verb. Defaults to 5.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def print_coding_dictionary(self, num=10, top_n=5):\n    \"\"\"Prints a coding dictionary based on common verbs, attributes, and dimensions.\n    \"CATEGORY\" is the common verb\n    \"PROPERTY\" is the common nouns associated with the verb\n    \"DIMENSION\" is the common adjectives/adverbs/verbs associated with the property\n    Args:\n        num (int, optional): Number of common verbs to consider. Defaults to 10.\n        top_n (int, optional): Number of top attributes and dimensions to consider for each verb. Defaults to 5.\n\n    \"\"\"\n    self.map_spacy_doc()\n    output = []\n    coding_dict = []\n    output.append((\"CATEGORY\", \"PROPERTY\", \"DIMENSION\"))\n    verbs = self.common_verbs(num)\n    _verbs = []\n    for verb, freq in verbs:\n        _verbs.append(verb)\n    for verb, freq in verbs:\n        for attribute, f2 in self.attributes(verb, top_n):\n            for dimension, f3 in self.dimensions(attribute, top_n):\n                if dimension not in _verbs:\n                    output.append((verb, attribute, dimension))\n                    coding_dict.append(f\"{verb} &gt; {attribute} &gt; {dimension}\")\n    # Add coding_dict to corpus metadata\n    if self._corpus is not None:\n        self._corpus.metadata[\"coding_dict\"] = coding_dict\n    print(\"\\n---Coding Dictionary---\")\n    QRUtils.print_table(output)\n    print(\"---------------------------\\n\")\n    return output\n</code></pre>"},{"location":"modules/#text.Text.process_text","title":"<code>process_text(text)</code>","text":"<p>Process the text by removing unwanted characters and normalizing it.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def process_text(self, text: str) -&gt; str:\n    \"\"\"\n    Process the text by removing unwanted characters and normalizing it.\n    \"\"\"\n    # Remove unwanted characters\n    text = preprocessing.replace.urls(text)\n    text = preprocessing.replace.emails(text)\n    text = preprocessing.replace.phone_numbers(text)\n    text = preprocessing.replace.currency_symbols(text)\n    text = preprocessing.replace.hashtags(text)\n    text = preprocessing.replace.numbers(text)\n\n    # lowercase the text\n    text = text.lower()\n    return text\n</code></pre>"},{"location":"modules/#text.Text.process_tokens","title":"<code>process_tokens(id='corpus')</code>","text":"<p>Process tokens in the spacy document and extract relevant information.</p> Source code in <code>src/crisp_t/text.py</code> <pre><code>def process_tokens(self, id=\"corpus\"):\n    \"\"\"\n    Process tokens in the spacy document and extract relevant information.\n    \"\"\"\n\n    # ! if cached file exists, load it\n    cache_dir = Path(\"cache\")\n    cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n    if cache_file.exists():\n        with open(cache_file, \"rb\") as f:\n            spacy_doc, results = pickle.load(f)\n        # logger.info(\"Loaded cached spacy doc and results.\")\n        return spacy_doc, results\n\n    spacy_doc = self.make_spacy_doc()\n    logger.info(\"Spacy doc created.\")\n\n    n_cores = multiprocessing.cpu_count()\n\n    def process_token(token):\n        if token.is_stop or token.is_digit or token.is_punct or token.is_space:\n            return None\n        if token.like_url or token.like_num or token.like_email:\n            return None\n        if len(token.text) &lt; 3 or token.text.isupper():\n            return None\n        return {\n            \"text\": token.text,\n            \"lemma\": token.lemma_,\n            \"pos\": token.pos_,\n            \"pos_\": token.pos,\n            \"word\": token.lemma_,\n            \"sentiment\": token.sentiment,\n            \"tag\": token.tag_,\n            \"dep\": token.dep_,\n            \"prob\": token.prob,\n            \"idx\": token.idx,\n        }\n\n    tokens = list(spacy_doc)\n    _lemma = {}\n    _pos = {}\n    _pos_ = {}\n    _word = {}\n    _sentiment = {}\n    _tag = {}\n    _dep = {}\n    _prob = {}\n    _idx = {}\n    with ThreadPoolExecutor() as executor:\n        futures = {executor.submit(process_token, token): token for token in tokens}\n        with tqdm(\n            total=len(futures),\n            desc=f\"Processing tokens (parallel, {n_cores} cores)\",\n        ) as pbar:\n            for future in as_completed(futures):\n                result = future.result()\n                if result is not None:\n                    _lemma[result[\"text\"]] = result[\"lemma\"]\n                    _pos[result[\"text\"]] = result[\"pos\"]\n                    _pos_[result[\"text\"]] = result[\"pos_\"]\n                    _word[result[\"text\"]] = result[\"word\"]\n                    _sentiment[result[\"text\"]] = result[\"sentiment\"]\n                    _tag = result[\"tag\"]\n                    _dep = result[\"dep\"]\n                    _prob = result[\"prob\"]\n                    _idx = result[\"idx\"]\n                pbar.update(1)\n    logger.info(\"Token processing complete.\")\n    results = {\n        \"lemma\": _lemma,\n        \"pos\": _pos,\n        \"pos_\": _pos_,\n        \"word\": _word,\n        \"sentiment\": _sentiment,\n        \"tag\": _tag,\n        \"dep\": _dep,\n        \"prob\": _prob,\n        \"idx\": _idx,\n    }\n    # ! dump spacy_doc, results to a file for caching with the corpus id\n    cache_dir = Path(\"cache\")\n    cache_dir.mkdir(exist_ok=True)\n    cache_file = cache_dir / f\"spacy_doc_{id}.pkl\"\n    with open(cache_file, \"wb\") as f:\n        pickle.dump((spacy_doc, results), f)\n\n    return spacy_doc, results\n</code></pre>"},{"location":"modules/#ml.ML","title":"<code>ML</code>","text":"Source code in <code>src/crisp_t/ml.py</code> <pre><code>class ML:\n    def __init__(\n        self,\n        csv: Csv,\n    ):\n        if not ML_INSTALLED:\n            raise ImportError(\"ML dependencies are not installed.\")\n        self._csv = csv\n        self._epochs = 3\n        self._samplesize = 0\n\n    @property\n    def csv(self):\n        return self._csv\n\n    @property\n    def corpus(self):\n        return self._csv.corpus\n\n    @csv.setter\n    def csv(self, value):\n        if isinstance(value, Csv):\n            self._csv = value\n        else:\n            raise ValueError(f\"The input belongs to {type(value)} instead of Csv.\")\n\n    def get_kmeans(self, number_of_clusters=3, seed=42, verbose=True, mcp=False):\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling get_kmeans.\"\n            )\n        X, _ = self._csv.read_xy(\"\")  # No output variable for clustering\n        if X is None:\n            raise ValueError(\n                \"Input features X are None. Cannot perform KMeans clustering.\"\n            )\n        kmeans = KMeans(\n            n_clusters=number_of_clusters, init=\"k-means++\", random_state=seed\n        )\n        self._clusters = kmeans.fit_predict(X)\n        members = self._get_members(self._clusters, number_of_clusters)\n        # Add cluster info to csv to metadata_cluster column\n        if self._csv is not None and getattr(self._csv, \"df\", None) is not None:\n            self._csv.df[\"metadata_cluster\"] = self._clusters\n        if verbose:\n            print(\"KMeans Cluster Centers:\\n\", kmeans.cluster_centers_)\n            print(\n                \"KMeans Inertia (Sum of squared distances to closest cluster center):\\n\",\n                kmeans.inertia_,\n            )\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[ml_config.METADATA_KEY_KMEANS] = (\n                    f\"KMeans clustering with {number_of_clusters} clusters. Inertia: {kmeans.inertia_}\"\n                )\n        # Add members info to corpus metadata\n        members_info = \"\\n\".join(\n            [\n                f\"Cluster {i}: {len(members[i])} members\"\n                for i in range(number_of_clusters)\n            ]\n        )\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"kmeans_members\"] = (\n                f\"KMeans clustering members:\\n{members_info}\"\n            )\n        if mcp:\n            return members_info\n        return self._clusters, members\n\n    def _get_members(self, clusters, number_of_clusters=3):\n        _df = self._csv.df\n        self._csv.df = _df\n        members = []\n        for i in range(number_of_clusters):\n            members.append([])\n        for i, cluster in enumerate(clusters):\n            members[cluster].append(i)\n        return members\n\n    def profile(self, members, number_of_clusters=3):\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling profile.\"\n            )\n        _corpus = self._csv.corpus\n        _numeric_clusters = \"\"\n        for i in range(number_of_clusters):\n            print(\"Cluster: \", i)\n            print(\"Cluster Length: \", len(members[i]))\n            print(\"Cluster Members\")\n            if self._csv is not None and getattr(self._csv, \"df\", None) is not None:\n                print(self._csv.df.iloc[members[i], :])\n                print(\"Centroids\")\n                print(self._csv.df.iloc[members[i], :].mean(axis=0))\n                _numeric_clusters += f\"Cluster {i} with {len(members[i])} members\\n has the following centroids (mean values):\\n\"\n                _numeric_clusters += (\n                    f\"{self._csv.df.iloc[members[i], :].mean(axis=0)}\\n\"\n                )\n            else:\n                print(\"DataFrame (self._csv.df) is not set.\")\n        if _corpus is not None:\n            _corpus.metadata[\"numeric_clusters\"] = _numeric_clusters\n            self._csv.corpus = _corpus\n        return members\n\n    def get_nnet_predictions(self, y: str, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n        \"\"\"\n        Extended: Handles binary (BCELoss) and multi-class (CrossEntropyLoss).\n        Returns list of predicted original class labels.\n\n        Args:\n            y (str): Target column name OR text metadata field name (when linkage_method is specified).\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n        \"\"\"\n        if ML_INSTALLED is False:\n            logger.info(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            return None\n\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling profile.\"\n            )\n        _corpus = self._csv.corpus\n\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n\n        unique_classes = np.unique(Y_raw)\n        num_classes = unique_classes.size\n        if num_classes &lt; 2:\n            raise ValueError(f\"Need at least 2 classes; found {num_classes}.\")\n\n        vnum = X_np.shape[1]\n\n        # Binary path\n        if num_classes == 2:\n            # Map to {0.0,1.0} for BCELoss if needed\n            mapping_applied = False\n            class_mapping = {}\n            inverse_mapping = {}\n            # Ensure deterministic order\n            sorted_classes = sorted(unique_classes.tolist())\n            if not (sorted_classes == [0, 1] or sorted_classes == [0.0, 1.0]):\n                class_mapping = {sorted_classes[0]: 0.0, sorted_classes[1]: 1.0}\n                inverse_mapping = {v: k for k, v in class_mapping.items()}\n                Y_mapped = np.vectorize(class_mapping.get)(Y_raw).astype(np.float32)\n                mapping_applied = True\n            else:\n                Y_mapped = Y_raw.astype(np.float32)\n\n            model = NeuralNet(vnum)\n            try:\n                criterion = nn.BCELoss()  # type: ignore\n                optimizer = optim.Adam(model.parameters(), lr=ml_config.NNET_LEARNING_RATE)  # type: ignore\n\n                X_tensor = torch.from_numpy(X_np)  # type: ignore\n                y_tensor = torch.from_numpy(Y_mapped.astype(np.float32)).view(-1, 1)  # type: ignore\n\n                dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n                dataloader = DataLoader(dataset, batch_size=ml_config.NNET_BATCH_SIZE, shuffle=True)  # type: ignore\n            except Exception as e:\n                logger.exception(f\"Error occurred while creating DataLoader: {e}\")\n                return None\n\n            for _ in range(self._epochs):\n                for batch_X, batch_y in dataloader:\n                    optimizer.zero_grad()\n                    outputs = model(batch_X)\n                    loss = criterion(outputs, batch_y)\n                    if torch.isnan(loss):  # type: ignore\n                        raise RuntimeError(\"NaN loss encountered.\")\n                    loss.backward()\n                    optimizer.step()\n\n            # Predictions\n            bin_preds_internal = None\n            if torch:\n                with torch.no_grad():\n                    probs = model(torch.from_numpy(X_np)).view(-1).cpu().numpy()\n                bin_preds_internal = (probs &gt;= 0.5).astype(int)\n\n            if mapping_applied:\n                preds = [inverse_mapping[float(p)] for p in bin_preds_internal]  # type: ignore\n                y_eval = np.vectorize(class_mapping.get)(Y_raw).astype(int)\n                preds_eval = bin_preds_internal\n            else:\n                preds = bin_preds_internal.tolist()  # type: ignore\n                y_eval = Y_mapped.astype(int)\n                preds_eval = bin_preds_internal\n\n            accuracy = (preds_eval == y_eval).sum() / len(y_eval)\n            print(\n                f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n            )\n            if _corpus is not None:\n                _corpus.metadata[ml_config.METADATA_KEY_NNET] = (\n                    f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n                )\n            if mcp:\n                return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n            return preds\n\n        # Multi-class path\n        # Map original classes to indices\n        sorted_classes = sorted(unique_classes.tolist())\n        class_to_idx = {c: i for i, c in enumerate(sorted_classes)}\n        idx_to_class = {i: c for c, i in class_to_idx.items()}\n        Y_idx = np.vectorize(class_to_idx.get)(Y_raw).astype(np.int64)\n\n        model = MultiClassNet(vnum, num_classes)\n        criterion = nn.CrossEntropyLoss()  # type: ignore\n        optimizer = optim.Adam(model.parameters(), lr=ml_config.NNET_LEARNING_RATE)  # type: ignore\n\n        X_tensor = torch.from_numpy(X_np)  # type: ignore\n        y_tensor = torch.from_numpy(Y_idx)  # type: ignore\n\n        dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n        dataloader = DataLoader(dataset, batch_size=ml_config.NNET_BATCH_SIZE, shuffle=True)  # type: ignore\n\n        for _ in range(self._epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                logits = model(batch_X)\n                loss = criterion(logits, batch_y)\n                if torch.isnan(loss):  # type: ignore\n                    raise RuntimeError(\"NaN loss encountered.\")\n                loss.backward()\n                optimizer.step()\n\n        with torch.no_grad():  # type: ignore\n            logits_full = model(torch.from_numpy(X_np))  # type: ignore\n            pred_indices = torch.argmax(logits_full, dim=1).cpu().numpy()  # type: ignore\n\n        preds = [idx_to_class[i] for i in pred_indices]\n        accuracy = (pred_indices == Y_idx).sum() / len(Y_idx)\n        print(\n            f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n        )\n        if _corpus is not None:\n            _corpus.metadata[ml_config.METADATA_KEY_NNET] = (\n                f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n            )\n        if mcp:\n            return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n        return preds\n\n    def _convert_to_binary(self, Y):\n        unique_values = np.unique(Y)\n        if len(unique_values) != 2:\n            logger.warning(\n                \"Target variable has more than two unique values.\"\n            )\n            # convert unique_values[0] to 0, rest to 1\n            mapping = {val: (0 if val == unique_values[0] else 1) for val in unique_values}\n        else:\n            mapping = {unique_values[0]: 0, unique_values[1]: 1}\n        Y_binary = np.vectorize(mapping.get)(Y)\n        print(f\"Converted target variable to binary using mapping: {mapping}\")\n        return Y_binary\n\n    def svm_confusion_matrix(self, y: str, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.KMEANS_RANDOM_STATE, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n        \"\"\"Generate confusion matrix for SVM\n\n        Args:\n            y (str): Target column name OR text metadata field name (when linkage_method is specified).\n            test_size (float): Proportion of dataset to include in test split.\n            random_state (int): Random state for reproducibility.\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n        Returns:\n            [list] -- [description]\n        \"\"\"\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n        Y = self._convert_to_binary(Y)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, Y, test_size=test_size, random_state=random_state\n        )\n        sc = StandardScaler()\n        # Issue #22\n        y_test = y_test.astype(\"int\")\n        y_train = y_train.astype(\"int\")\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.transform(X_test)\n        classifier = SVC(kernel=\"linear\", random_state=ml_config.KMEANS_RANDOM_STATE)\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        # Issue #22\n        y_pred = y_pred.astype(\"int\")\n        _confusion_matrix = confusion_matrix(y_test, y_pred)\n        print(f\"Confusion Matrix for SVM predicting {y}:\\n{_confusion_matrix}\")\n        # Output\n        # [[2 0]\n        #  [2 0]]\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[ml_config.METADATA_KEY_SVM_CONFUSION] = (\n                f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n            )\n\n        if mcp:\n            return f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n\n        return _confusion_matrix\n\n    def format_confusion_matrix_to_human_readable(\n        self, confusion_matrix: np.ndarray\n    ) -&gt; str:\n        \"\"\"Format the confusion matrix to a human-readable string.\n\n        Args:\n            confusion_matrix (np.ndarray): The confusion matrix to format.\n\n        Returns:\n            str: The formatted confusion matrix with true positive, false positive, true negative, and false negative counts.\n        \"\"\"\n        tn, fp, fn, tp = confusion_matrix.ravel()\n        return (\n            f\"True Positive: {tp}\\n\"\n            f\"False Positive: {fp}\\n\"\n            f\"True Negative: {tn}\\n\"\n            f\"False Negative: {fn}\\n\"\n        )\n\n    # https://stackoverflow.com/questions/45419203/python-numpy-extracting-a-row-from-an-array\n    def knn_search(self, y: str, n=3, r=3, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n        \"\"\"\n        Perform K-Nearest Neighbors search.\n\n        Args:\n            y (str): Target column name OR text metadata field name (when linkage_method is specified).\n            n (int): Number of nearest neighbors to find.\n            r (int): Record number to search from (1-based index).\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n        \"\"\"\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n        kdt = KDTree(X_np, leaf_size=2, metric=\"euclidean\")\n        dist, ind = kdt.query(X_np[r - 1 : r, :], k=n)\n        # Display results as human readable (1-based)\n        ind = (ind + 1).tolist()  # Convert to 1-based index\n        dist = dist.tolist()\n        print(\n            f\"\\nKNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\\n\"\n        )\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"knn_search\"] = (\n                f\"KNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\"\n            )\n        if mcp:\n            return f\"KNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\"\n        return dist, ind\n\n    def _extract_outcome_from_text_metadata(\n        self,\n        metadata_field: str,\n        linkage_method: str,\n        aggregation: str = \"majority\"\n    ) -&gt; tuple[pd.Series, list[int]]:\n        \"\"\"\n        Extract outcome values from text document metadata using specified linkage method.\n\n        Args:\n            metadata_field (str): Name of the metadata field in documents containing outcome values\n            linkage_method (str): Linkage method to use ('id', 'embedding', 'temporal', 'keyword')\n            aggregation (str): Strategy for aggregating multiple documents linked to same row\n                              - 'majority': Majority vote for classification (default)\n                              - 'mean': Mean value for regression\n                              - 'first': Use first document's value\n                              - 'mode': Mode (most common) value\n\n        Returns:\n            Tuple[pd.Series, List[int]]: (outcome_series, valid_indices)\n                - outcome_series: Series with outcome values aligned to DataFrame rows\n                - valid_indices: List of DataFrame indices that have linked documents\n        \"\"\"\n        if not self._csv.corpus or not self._csv.corpus.documents:\n            raise ValueError(\"Corpus documents not available. Cannot extract text metadata outcomes.\")\n\n        if self._csv.df is None:\n            raise ValueError(\"DataFrame not available.\")\n\n        # Build mapping from DataFrame index to document metadata values\n        df_index_to_values: dict[int, list] = {}\n\n        if linkage_method == \"id\":\n            # ID linkage: Match document.id with df['id'] column\n            if \"id\" not in self._csv.df.columns:\n                raise ValueError(\"ID linkage requires 'id' column in DataFrame\")\n\n            # Create a mapping from id to df index\n            id_to_df_index = {\n                str(row_id): idx\n                for idx, row_id in self._csv.df['id'].items()\n            }\n\n            for doc in self._csv.corpus.documents:\n                if metadata_field in doc.metadata:\n                    doc_id = str(doc.id)\n                    if doc_id in id_to_df_index:\n                        df_idx = id_to_df_index[doc_id]\n                        if df_idx not in df_index_to_values:\n                            df_index_to_values[df_idx] = []\n                        df_index_to_values[df_idx].append(doc.metadata[metadata_field])\n\n        elif linkage_method in [\"embedding\", \"temporal\"]:\n            # Embedding or temporal linkage: Use links stored in document metadata\n            link_key = f\"{linkage_method}_links\"\n\n            for doc in self._csv.corpus.documents:\n                if metadata_field in doc.metadata and link_key in doc.metadata:\n                    links = doc.metadata[link_key]\n                    if isinstance(links, list):\n                        for link in links:\n                            if isinstance(link, dict) and \"df_index\" in link:\n                                df_idx = link[\"df_index\"]\n                                if df_idx not in df_index_to_values:\n                                    df_index_to_values[df_idx] = []\n                                df_index_to_values[df_idx].append(doc.metadata[metadata_field])\n\n        elif linkage_method == \"keyword\":\n            # Keyword linkage: Use keyword_links stored in document metadata\n            for doc in self._csv.corpus.documents:\n                if metadata_field in doc.metadata and \"keyword_links\" in doc.metadata:\n                    links = doc.metadata[\"keyword_links\"]\n                    if isinstance(links, list):\n                        for link in links:\n                            if isinstance(link, dict) and \"df_index\" in link:\n                                df_idx = link[\"df_index\"]\n                                if df_idx not in df_index_to_values:\n                                    df_index_to_values[df_idx] = []\n                                df_index_to_values[df_idx].append(doc.metadata[metadata_field])\n\n        else:\n            raise ValueError(\n                f\"Unsupported linkage method: {linkage_method}. \"\n                f\"Supported methods: 'id', 'embedding', 'temporal', 'keyword'\"\n            )\n\n        if not df_index_to_values:\n            raise ValueError(\n                f\"No documents with '{metadata_field}' metadata field are linked to DataFrame rows \"\n                f\"using '{linkage_method}' linkage method.\"\n            )\n\n        # Aggregate values for each DataFrame row\n        aggregated_values = {}\n        for df_idx, values in df_index_to_values.items():\n            if aggregation == \"majority\":\n                # Majority vote (for classification)\n                counter = Counter(values)\n                aggregated_values[df_idx] = counter.most_common(1)[0][0]\n            elif aggregation == \"mean\":\n                # Mean value (for regression)\n                try:\n                    numeric_values = [float(v) for v in values]\n                    aggregated_values[df_idx] = np.mean(numeric_values)\n                except (ValueError, TypeError):\n                    # Fall back to majority if not numeric\n                    logger.warning(\n                        f\"Non-numeric values found for df_index {df_idx}, using majority vote instead\"\n                    )\n                    counter = Counter(values)\n                    aggregated_values[df_idx] = counter.most_common(1)[0][0]\n            elif aggregation == \"first\":\n                # Use first document's value\n                aggregated_values[df_idx] = values[0]\n            elif aggregation == \"mode\":\n                # Mode (most common value)\n                counter = Counter(values)\n                aggregated_values[df_idx] = counter.most_common(1)[0][0]\n            else:\n                raise ValueError(\n                    f\"Unsupported aggregation method: {aggregation}. \"\n                    f\"Supported methods: 'majority', 'mean', 'first', 'mode'\"\n                )\n\n        # Create Series aligned with DataFrame\n        valid_indices = sorted(aggregated_values.keys())\n        outcome_series = pd.Series(\n            [aggregated_values[idx] for idx in valid_indices],\n            index=valid_indices,\n            name=metadata_field\n        )\n\n        logger.info(\n            f\"Extracted outcome from text metadata field '{metadata_field}' \"\n            f\"using '{linkage_method}' linkage and '{aggregation}' aggregation. \"\n            f\"Linked {len(valid_indices)} out of {len(self._csv.df)} DataFrame rows.\"\n        )\n\n        return outcome_series, valid_indices\n\n    def _process_xy(self, y: str, oversample=False, one_hot_encode_all=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n        \"\"\"\n        Process features and outcome data for ML models.\n\n        Args:\n            y (str): Either a DataFrame column name OR a text metadata field name (when linkage_method is specified)\n            oversample (bool): Whether to oversample minority classes\n            one_hot_encode_all (bool): Whether to one-hot encode all columns\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword')\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode')\n\n        Returns:\n            Tuple: (X_np, Y_raw, X, Y) - Feature matrix, outcome array, original DataFrames\n        \"\"\"\n        # Check if y is a text metadata field (requires linkage_method)\n        if linkage_method:\n            # Extract outcome from text metadata using specified linkage\n            outcome_series, valid_indices = self._extract_outcome_from_text_metadata(\n                metadata_field=y,\n                linkage_method=linkage_method,\n                aggregation=aggregation\n            )\n\n            # Filter DataFrame to only include rows with linked documents\n            filtered_df = self._csv.df.loc[valid_indices].copy()\n\n            # Add the outcome column to the filtered DataFrame\n            filtered_df[f\"_text_outcome_{y}\"] = outcome_series\n\n            # Temporarily swap the DataFrame\n            original_df = self._csv.df\n            self._csv.df = filtered_df\n\n            try:\n                # Prepare data using the temporary outcome column\n                X, Y = self._csv.prepare_data(\n                    y=f\"_text_outcome_{y}\",\n                    oversample=oversample,\n                    one_hot_encode_all=one_hot_encode_all\n                )\n            finally:\n                # Restore original DataFrame and remove temporary column\n                filtered_df.drop(columns=[f\"_text_outcome_{y}\"], inplace=True)\n                self._csv.df = original_df\n\n            # Rename Y back to the original metadata field name\n            if hasattr(Y, 'name'):\n                Y.name = y\n        else:\n            # Standard path: y is a DataFrame column\n            X, Y = self._csv.prepare_data(\n                y=y, oversample=oversample, one_hot_encode_all=one_hot_encode_all\n            )\n\n        if X is None or Y is None:\n            raise ValueError(\"prepare_data returned None for X or Y.\")\n\n        # To numpy float32\n        X_np = (\n            X.to_numpy(dtype=np.float32)\n            if hasattr(X, \"to_numpy\")\n            else np.asarray(X, dtype=np.float32)\n        )\n        Y_raw = Y.to_numpy() if hasattr(Y, \"to_numpy\") else np.asarray(Y)\n\n        # Handle NaNs\n        if np.isnan(X_np).any():\n            raise ValueError(\"NaN detected in feature matrix.\")\n        if np.isnan(Y_raw.astype(float, copy=False)).any():\n            raise ValueError(\"NaN detected in target vector.\")\n\n        return X_np, Y_raw, X, Y\n\n    def get_decision_tree_classes(\n        self, y: str, top_n=5, test_size=0.5, random_state=1, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"\n    ):\n        \"\"\"\n        Train a Decision Tree classifier and return feature importances.\n\n        Args:\n            y (str): Target column name OR text metadata field name (when linkage_method is specified).\n            top_n (int): Number of top features to display.\n            test_size (float): Proportion of dataset to include in test split.\n            random_state (int): Random state for reproducibility.\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n        Returns:\n            dict or str: Feature importances and accuracy metrics.\n        \"\"\"\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n        Y_raw = self._convert_to_binary(Y_raw)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_np, Y_raw, test_size=test_size, random_state=random_state\n        )\n\n        # print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n        # print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n\n        # Train a RandomForestClassifier\n        clf = RandomForestClassifier(n_estimators=100, random_state=ml_config.CLASSIFIER_RANDOM_STATE)\n        clf.fit(X_train, y_train)\n\n        # Compute permutation importance\n        results = permutation_importance(\n            clf, X_test, y_test, n_repeats=10, random_state=ml_config.CLASSIFIER_RANDOM_STATE\n        )\n\n        # classifier = DecisionTreeClassifier(random_state=random_state) # type: ignore\n        # classifier.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        _confusion_matrix = confusion_matrix(y_test, y_pred)\n        print(\n            f\"Confusion Matrix for Decision Tree predicting {y}:\\n{_confusion_matrix}\"\n        )\n        # Output\n        # [[2 0]\n        #  [2 0]]\n\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"\\nAccuracy: {accuracy}\\n\")\n\n        # Retrieve feature importance scores\n        importance = results.importances_mean\n\n        # Get indices of top N important features\n        top_n_indices = np.argsort(importance)[-top_n:][::-1]\n\n        # Display feature importance\n        print(f\"==== Top {top_n} important features ====\\n\")\n        _importance = \"\"\n        for i, v in enumerate(top_n_indices):\n            print(f\"Feature: {X.columns[v]}, Score: {importance[v]:.5f}\")\n            _importance += f\"Feature: {X.columns[v]}, Score: {importance[v]:.5f}\\n\"\n\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"decision_tree_accuracy\"] = (\n                f\"Decision Tree accuracy for predicting {y}: {accuracy*100:.2f}%\"\n            )\n            self._csv.corpus.metadata[\"decision_tree_confusion_matrix\"] = (\n                f\"Confusion Matrix for Decision Tree predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n            )\n            self._csv.corpus.metadata[\"decision_tree_feature_importance\"] = _importance\n        if mcp:\n            return f\"\"\"\n            Confusion Matrix for Decision Tree predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\\nTop {top_n} important features:\\n{_importance}\n            Accuracy: {accuracy*100:.2f}%\n            \"\"\"\n        return _confusion_matrix, importance\n\n    def get_xgb_classes(\n        self, y: str, oversample=False, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.KMEANS_RANDOM_STATE, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"\n    ):\n        \"\"\"\n        Train an XGBoost classifier and return feature importances.\n\n        Args:\n            y (str): Target column name OR text metadata field name (when linkage_method is specified).\n            oversample (bool): Whether to oversample minority classes.\n            test_size (float): Proportion of dataset to include in test split.\n            random_state (int): Random state for reproducibility.\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n        Returns:\n            dict or str: Feature importances and accuracy metrics.\n        \"\"\"\n        try:\n            from xgboost import XGBClassifier  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"XGBoost is not installed. Please install it via `pip install crisp-t[xg]`.\"\n            ) from None\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n        if ML_INSTALLED:\n            # ValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1 2]\n            # convert y to binary\n            Y_binary = (Y_raw == 1).astype(int)\n            X_train, X_test, y_train, y_test = train_test_split(\n                X_np, Y_binary, test_size=test_size, random_state=random_state\n            )\n            classifier = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")  # type: ignore\n            classifier.fit(X_train, y_train)\n            y_pred = classifier.predict(X_test)\n            _confusion_matrix = confusion_matrix(y_test, y_pred)\n            print(f\"Confusion Matrix for XGBoost predicting {y}:\\n{_confusion_matrix}\")\n            # Output\n            # [[2 0]\n            #  [2 0]]\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"xgb_confusion_matrix\"] = (\n                    f\"Confusion Matrix for XGBoost predicting {y}:\\n{_confusion_matrix}\"\n                )\n            if mcp:\n                return f\"\"\"\n                Confusion Matrix for XGBoost predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\n                \"\"\"\n            return _confusion_matrix\n        else:\n            raise ImportError(\"ML dependencies are not installed.\")\n\n    def get_apriori(\n        self, y: str, min_support=0.9, use_colnames=True, min_threshold=0.5, mcp=False\n    ):\n        if ML_INSTALLED:\n            X_np, Y_raw, X, Y = self._process_xy(y=y, one_hot_encode_all=True)\n            frequent_itemsets = apriori(X, min_support=min_support, use_colnames=use_colnames)  # type: ignore\n            # rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_threshold) # type: ignore\n            human_readable = tabulate(\n                frequent_itemsets.head(10), headers=\"keys\", tablefmt=\"pretty\"  # type: ignore\n            )\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"apriori_frequent_itemsets\"] = human_readable\n            if mcp:\n                return f\"Frequent itemsets (top 10):\\n{human_readable}\"\n            return frequent_itemsets  # , rules\n        else:\n            raise ImportError(\"ML dependencies are not installed.\")\n\n    def get_pca(self, y: str, n: int = 3, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n        \"\"\"\n        Perform a manual PCA (no sklearn PCA) on the feature matrix for target y.\n\n        Args:\n            y (str): Target column name (used only for data preparation) OR text metadata field name (when linkage_method is specified).\n            n (int): Number of principal components to keep.\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n        Returns:\n            dict: {\n                'covariance_matrix': cov_mat,\n                'eigenvalues': eig_vals_sorted,\n                'eigenvectors': eig_vecs_sorted,\n                'explained_variance_ratio': var_exp,\n                'cumulative_explained_variance_ratio': cum_var_exp,\n                'projection_matrix': matrix_w,\n                'transformed': X_pca\n            }\n        \"\"\"\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n        X_std = StandardScaler().fit_transform(X_np)\n\n        cov_mat = np.cov(X_std.T)\n        eig_vals, eig_vecs = np.linalg.eigh(cov_mat)  # symmetric matrix -&gt; eigh\n\n        # Sort eigenvalues (and vectors) descending\n        idx = np.argsort(eig_vals)[::-1]\n        eig_vals_sorted = eig_vals[idx]\n        eig_vecs_sorted = eig_vecs[:, idx]\n\n        factors = X_std.shape[1]\n        n = max(1, min(n, factors))\n\n        # Explained variance ratios\n        tot = eig_vals_sorted.sum()\n        var_exp = (eig_vals_sorted / tot) * 100.0\n        cum_var_exp = np.cumsum(var_exp)\n\n        # Projection matrix (first n eigenvectors)\n        matrix_w = eig_vecs_sorted[:, :n]\n\n        # Project data\n        X_pca = X_std @ matrix_w\n\n        # Optional prints (retain original behavior)\n        print(\"Covariance matrix:\\n\", cov_mat)\n        print(\"Eigenvalues (desc):\\n\", eig_vals_sorted)\n        print(\"Explained variance (%):\\n\", var_exp[:n])\n        print(\"Cumulative explained variance (%):\\n\", cum_var_exp[:n])\n        print(\"Projection matrix (W):\\n\", matrix_w)\n        print(\"Transformed (first 5 rows):\\n\", X_pca[:5])\n\n        result = {\n            \"covariance_matrix\": cov_mat,\n            \"eigenvalues\": eig_vals_sorted,\n            \"eigenvectors\": eig_vecs_sorted,\n            \"explained_variance_ratio\": var_exp,\n            \"cumulative_explained_variance_ratio\": cum_var_exp,\n            \"projection_matrix\": matrix_w,\n            \"transformed\": X_pca,\n        }\n\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[ml_config.METADATA_KEY_PCA] = (\n                f\"PCA kept {n} components explaining \"\n                f\"{cum_var_exp[n-1]:.2f}% variance.\"\n            )\n        if mcp:\n            return (\n                f\"PCA kept {n} components explaining {cum_var_exp[n-1]:.2f}% variance.\"\n            )\n        return result\n\n    def get_regression(self, y: str, mcp=False, linkage_method: str | None = None, aggregation: str = \"mean\"):\n        \"\"\"\n        Perform linear or logistic regression based on the outcome variable type.\n\n        If the outcome is binary, fit a logistic regression model.\n        Otherwise, fit a linear regression model.\n\n        Args:\n            y (str): Target column name for the regression OR text metadata field name (when linkage_method is specified).\n            mcp (bool): Whether to return MCP-formatted string.\n            linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n            aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n                             Default is 'mean' for regression tasks.\n\n        Returns:\n            dict: Regression results including coefficients, intercept, and metrics.\n        \"\"\"\n        if ML_INSTALLED is False:\n            logger.info(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            return None\n\n        if self._csv is None:\n            raise ValueError(\n                \"CSV data is not set. Please set self.csv before calling get_regression.\"\n            )\n\n        X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n\n        # Check if outcome is binary (logistic) or continuous (linear)\n        unique_values = np.unique(Y_raw)\n        num_unique = len(unique_values)\n\n        # Determine if binary classification or regression\n        is_binary = num_unique == 2\n\n        if is_binary:\n            # Logistic Regression\n            print(f\"\\n=== Logistic Regression for {y} ===\")\n            print(f\"Binary outcome detected with values: {unique_values}\")\n\n            model = LogisticRegression(max_iter=1000, random_state=ml_config.CLASSIFIER_RANDOM_STATE)\n            model.fit(X_np, Y_raw)\n\n            # Predictions\n            y_pred = model.predict(X_np)\n\n            # Accuracy\n            accuracy = accuracy_score(Y_raw, y_pred)\n            print(f\"\\nAccuracy: {accuracy*100:.2f}%\")\n\n            # Coefficients and Intercept\n            print(\"\\nCoefficients:\")\n            for i, coef in enumerate(model.coef_[0]):\n                feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n                print(f\"  {feature_name}: {coef:.5f}\")\n\n            print(f\"\\nIntercept: {model.intercept_[0]:.5f}\")\n\n            coef_str = \"\\n\".join(\n                [\n                    f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                    for i, coef in enumerate(model.coef_[0])\n                ]\n            )\n\n            # Store in metadata\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"logistic_regression_accuracy\"] = (\n                    f\"Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\"\n                )\n                self._csv.corpus.metadata[\"logistic_regression_coefficients\"] = (\n                    f\"Coefficients:\\n{coef_str}\"\n                )\n                self._csv.corpus.metadata[\"logistic_regression_intercept\"] = (\n                    f\"Intercept: {model.intercept_[0]:.5f}\"\n                )\n\n            if mcp:\n                return f\"\"\"\n                Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\n                Coefficients:\n                {coef_str}\n                Intercept: {model.intercept_[0]:.5f}\n                \"\"\"\n            return {\n                \"model_type\": \"logistic\",\n                \"accuracy\": accuracy,\n                \"coefficients\": model.coef_[0],\n                \"intercept\": model.intercept_[0],\n                \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n            }\n        else:\n            # Linear Regression\n            print(f\"\\n=== Linear Regression for {y} ===\")\n            print(f\"Continuous outcome detected with {num_unique} unique values\")\n\n            model = LinearRegression()\n            model.fit(X_np, Y_raw)\n\n            # Predictions\n            y_pred = model.predict(X_np)\n\n            # Metrics\n            mse = mean_squared_error(Y_raw, y_pred)\n            r2 = r2_score(Y_raw, y_pred)\n            print(f\"\\nMean Squared Error (MSE): {mse:.5f}\")\n            print(f\"R\u00b2 Score: {r2:.5f}\")\n\n            # Coefficients and Intercept\n            print(\"\\nCoefficients:\")\n            for i, coef in enumerate(model.coef_):\n                feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n                print(f\"  {feature_name}: {coef:.5f}\")\n\n            print(f\"\\nIntercept: {model.intercept_:.5f}\")\n\n            coef_str = \"\\n\".join(\n                [\n                    f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                    for i, coef in enumerate(model.coef_)\n                ]\n            )\n\n            # Store in metadata\n            if self._csv.corpus is not None:\n                self._csv.corpus.metadata[\"linear_regression_mse\"] = (\n                    f\"Linear Regression MSE for predicting {y}: {mse:.5f}\"\n                )\n                self._csv.corpus.metadata[\"linear_regression_r2\"] = (\n                    f\"Linear Regression R\u00b2 for predicting {y}: {r2:.5f}\"\n                )\n                self._csv.corpus.metadata[\"linear_regression_coefficients\"] = (\n                    f\"Coefficients:\\n{coef_str}\"\n                )\n                self._csv.corpus.metadata[\"linear_regression_intercept\"] = (\n                    f\"Intercept: {model.intercept_:.5f}\"\n                )\n\n            if mcp:\n                return f\"\"\"\n                Linear Regression MSE for predicting {y}: {mse:.5f}\n                R\u00b2: {r2:.5f}\n                Feature Names and Coefficients:\n                {coef_str}\n                Intercept: {model.intercept_:.5f}\n                \"\"\"\n            return {\n                \"model_type\": \"linear\",\n                \"mse\": mse,\n                \"r2\": r2,\n                \"coefficients\": model.coef_,\n                \"intercept\": model.intercept_,\n                \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n            }\n\n    def get_lstm_predictions(self, y: str, mcp=False):\n        \"\"\"\n        Train an LSTM model on text data to predict an outcome variable.\n        This tests if the texts converge towards predicting the outcome.\n\n        Args:\n            y (str): Name of the outcome variable in the DataFrame\n            mcp (bool): If True, return a string format suitable for MCP\n\n        Returns:\n            Evaluation metrics as string (if mcp=True) or dict\n        \"\"\"\n        if ML_INSTALLED is False:\n            logger.error(\n                \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n            )\n            if mcp:\n                return \"ML dependencies are not installed. Please install with: pip install crisp-t[ml]\"\n            return None\n\n        if self._csv is None:\n            logger.error(\"CSV data is not set.\")\n            if mcp:\n                return \"CSV data is not set. Cannot perform LSTM prediction.\"\n            return None\n\n        _corpus = self._csv.corpus\n        if _corpus is None:\n            logger.error(\"Corpus is not available.\")\n            if mcp:\n                return \"Corpus is not available. Cannot perform LSTM prediction.\"\n            return None\n\n        # Check if id_column exists\n        id_column = \"id\"\n        if not hasattr(self._csv, \"df\") or self._csv.df is None:\n            logger.error(\"DataFrame is not available in CSV.\")\n            if mcp:\n                return \"This tool can be used only if texts and outcome variables align. DataFrame is missing.\"\n            return None\n\n        if id_column not in self._csv.df.columns:\n            logger.error(\n                f\"The id_column '{id_column}' does not exist in the DataFrame.\"\n            )\n            if mcp:\n                return f\"This tool can be used only if texts and outcome variables align. The '{id_column}' column is missing from the DataFrame.\"\n            return None\n\n        # Check if outcome variable exists\n        if y not in self._csv.df.columns:\n            logger.error(f\"The outcome variable '{y}' does not exist in the DataFrame.\")\n            if mcp:\n                return f\"The outcome variable '{y}' does not exist in the DataFrame.\"\n            return None\n\n        # Process documents and align with outcome variable\n        try:\n            # Build vocabulary from all documents\n            from collections import Counter\n\n            word_counts = Counter()\n            tokenized_docs = []\n\n            for doc in tqdm(_corpus.documents, desc=\"Tokenizing documents\", disable=len(_corpus.documents) &lt; 10):\n                # Simple tokenization - split on whitespace and lowercase\n                tokens = doc.text.lower().split()\n                tokenized_docs.append(tokens)\n                word_counts.update(tokens)\n\n            # Create vocabulary with most common words (limit to 10000)\n            vocab_size = min(ml_config.LSTM_VOCAB_SIZE, len(word_counts)) + 1  # +1 for padding\n            most_common = word_counts.most_common(vocab_size - 1)\n            word_to_idx = {\n                word: idx + 1 for idx, (word, _) in enumerate(most_common)\n            }  # 0 reserved for padding\n\n            # Convert documents to sequences of indices\n            max_length = ml_config.LSTM_MAX_LENGTH  # Maximum sequence length\n            sequences = []\n            doc_ids = []\n\n            for doc, tokens in tqdm(zip(_corpus.documents, tokenized_docs), total=len(_corpus.documents), desc=\"Converting to sequences\", disable=len(_corpus.documents) &lt; 10):\n                # Convert tokens to indices\n                seq = [word_to_idx.get(token, 0) for token in tokens]\n                # Pad or truncate to max_length\n                if len(seq) &gt; max_length:\n                    seq = seq[:max_length]\n                else:\n                    seq = seq + [0] * (max_length - len(seq))\n                sequences.append(seq)\n                doc_ids.append(doc.id)\n\n            # Align with outcome variable using id column\n            df = self._csv.df.set_index(id_column)\n\n            aligned_sequences = []\n            aligned_outcomes = []\n\n            df_index_str = list(str(idx) for idx in df.index)\n            for doc_id, seq in zip(doc_ids, sequences):\n                if doc_id in df_index_str:\n                    aligned_sequences.append(seq)\n                    # Select y from df where id_column == doc_id, using string comparison\n                    matched_row = df.loc[\n                        [idx for idx in df.index if str(idx) == str(doc_id)]\n                    ]\n                    if not matched_row.empty:\n                        aligned_outcomes.append(matched_row.iloc[0][y])\n\n            if len(aligned_sequences) == 0:\n                logger.error(\"No documents could be aligned with the outcome variable.\")\n                if mcp:\n                    return \"This tool can be used only if texts and outcome variables align. No matching IDs found.\"\n                return None\n\n            # Convert to tensors\n            X_tensor = torch.LongTensor(aligned_sequences)  # type: ignore\n            y_array = np.array(aligned_outcomes)\n\n            # Handle binary classification\n            unique_values = np.unique(y_array)\n            num_classes = len(unique_values)\n\n            if num_classes &lt; 2:\n                logger.error(\n                    f\"Need at least 2 classes for classification, found {num_classes}\"\n                )\n                if mcp:\n                    return f\"Need at least 2 classes for classification, found {num_classes}\"\n                return None\n\n            # Map to 0/1 for binary classification\n            if num_classes == 2:\n                class_mapping = {unique_values[0]: 0.0, unique_values[1]: 1.0}\n                y_mapped = np.array(\n                    [class_mapping[val] for val in y_array], dtype=np.float32\n                )\n            else:\n                # Multi-class not supported in this simple LSTM implementation\n                logger.error(\n                    \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n                )\n                if mcp:\n                    return \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n                return None\n\n            y_tensor = torch.FloatTensor(y_mapped).view(-1, 1)  # type: ignore\n\n            # Split into train/test\n            from sklearn.model_selection import train_test_split\n\n            indices = list(range(len(X_tensor)))\n            train_idx, test_idx = train_test_split(\n                indices, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.CLASSIFIER_RANDOM_STATE\n            )\n\n            X_train = X_tensor[train_idx]\n            y_train = y_tensor[train_idx]\n            X_test = X_tensor[test_idx]\n            y_test = y_tensor[test_idx]\n\n            # Create model\n            model = SimpleLSTM(vocab_size=vocab_size)  # type: ignore\n            criterion = nn.BCELoss()  # type: ignore\n            optimizer = optim.Adam(model.parameters(), lr=ml_config.LSTM_LEARNING_RATE)  # type: ignore\n\n            # Create data loaders\n            train_dataset = TensorDataset(X_train, y_train)  # type: ignore\n            train_loader = DataLoader(train_dataset, batch_size=ml_config.LSTM_BATCH_SIZE, shuffle=True)  # type: ignore\n\n            # Training\n            epochs = max(self._epochs, ml_config.LSTM_EPOCHS)  # Use at least configured epochs for LSTM\n            model.train()\n            for epoch in range(epochs):\n                total_loss = 0\n                for batch_x, batch_y in train_loader:\n                    optimizer.zero_grad()\n                    predictions = model(batch_x)\n                    loss = criterion(predictions, batch_y)\n                    loss.backward()\n                    optimizer.step()\n                    total_loss += loss.item()\n\n                avg_loss = total_loss / len(train_loader)\n                logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n            # Evaluation\n            model.eval()\n            with torch.no_grad():  # type: ignore\n                train_preds = model(X_train)\n                test_preds = model(X_test)\n\n                train_preds_binary = (train_preds &gt;= 0.5).float()\n                test_preds_binary = (test_preds &gt;= 0.5).float()\n\n                train_accuracy = (train_preds_binary == y_train).float().mean().item()\n                test_accuracy = (test_preds_binary == y_test).float().mean().item()\n\n            # Calculate additional metrics for test set\n            y_test_np = y_test.cpu().numpy().flatten()\n            test_preds_np = test_preds_binary.cpu().numpy().flatten()\n\n            # Confusion matrix elements\n            tp = ((test_preds_np == 1) &amp; (y_test_np == 1)).sum()\n            tn = ((test_preds_np == 0) &amp; (y_test_np == 0)).sum()\n            fp = ((test_preds_np == 1) &amp; (y_test_np == 0)).sum()\n            fn = ((test_preds_np == 0) &amp; (y_test_np == 1)).sum()\n\n            # Calculate precision, recall, F1\n            precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n            f1 = (\n                2 * (precision * recall) / (precision + recall)\n                if (precision + recall) &gt; 0\n                else 0\n            )\n\n            result_msg = (\n                f\"LSTM Model Evaluation for predicting '{y}':\\n\"\n                f\"  Vocabulary size: {vocab_size}\\n\"\n                f\"  Training samples: {len(X_train)}, Test samples: {len(X_test)}\\n\"\n                f\"  Epochs: {epochs}\\n\"\n                f\"  Train accuracy: {train_accuracy*100:.2f}%\\n\"\n                f\"  Test accuracy (convergence): {test_accuracy*100:.2f}%\\n\"\n                f\"  True Positive: {tp}, False Positive: {fp}, True Negative: {tn}, False Negative: {fn}\\n\"\n                f\"  Precision: {precision:.3f}\\n\"\n                f\"  Recall: {recall:.3f}\\n\"\n                f\"  F1-Score: {f1:.3f}\\n\"\n            )\n\n            print(f\"\\n{result_msg}\")\n\n            # Store in corpus metadata\n            if _corpus is not None:\n                _corpus.metadata[ml_config.METADATA_KEY_LSTM] = result_msg\n\n            if mcp:\n                return result_msg\n\n            return {\n                \"vocab_size\": vocab_size,\n                \"train_samples\": len(X_train),\n                \"test_samples\": len(X_test),\n                \"epochs\": epochs,\n                \"train_accuracy\": train_accuracy,\n                \"test_accuracy\": test_accuracy,\n                \"true_positive\": tp,\n                \"false_positive\": fp,\n                \"true_negative\": tn,\n                \"false_negative\": fn,\n                \"precision\": precision,\n                \"recall\": recall,\n                \"f1_score\": f1,\n            }\n\n        except Exception as e:\n            logger.exception(f\"Error in LSTM prediction: {e}\")\n            if mcp:\n                return f\"Error in LSTM prediction: {e}\"\n            return None\n</code></pre>"},{"location":"modules/#ml.ML.format_confusion_matrix_to_human_readable","title":"<code>format_confusion_matrix_to_human_readable(confusion_matrix)</code>","text":"<p>Format the confusion matrix to a human-readable string.</p> <p>Parameters:</p> Name Type Description Default <code>confusion_matrix</code> <code>ndarray</code> <p>The confusion matrix to format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The formatted confusion matrix with true positive, false positive, true negative, and false negative counts.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def format_confusion_matrix_to_human_readable(\n    self, confusion_matrix: np.ndarray\n) -&gt; str:\n    \"\"\"Format the confusion matrix to a human-readable string.\n\n    Args:\n        confusion_matrix (np.ndarray): The confusion matrix to format.\n\n    Returns:\n        str: The formatted confusion matrix with true positive, false positive, true negative, and false negative counts.\n    \"\"\"\n    tn, fp, fn, tp = confusion_matrix.ravel()\n    return (\n        f\"True Positive: {tp}\\n\"\n        f\"False Positive: {fp}\\n\"\n        f\"True Negative: {tn}\\n\"\n        f\"False Negative: {fn}\\n\"\n    )\n</code></pre>"},{"location":"modules/#ml.ML.get_decision_tree_classes","title":"<code>get_decision_tree_classes(y, top_n=5, test_size=0.5, random_state=1, mcp=False, linkage_method=None, aggregation='majority')</code>","text":"<p>Train a Decision Tree classifier and return feature importances.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name OR text metadata field name (when linkage_method is specified).</p> required <code>top_n</code> <code>int</code> <p>Number of top features to display.</p> <code>5</code> <code>test_size</code> <code>float</code> <p>Proportion of dataset to include in test split.</p> <code>0.5</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility.</p> <code>1</code> <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').</p> <code>'majority'</code> <p>Returns:</p> Type Description <p>dict or str: Feature importances and accuracy metrics.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_decision_tree_classes(\n    self, y: str, top_n=5, test_size=0.5, random_state=1, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"\n):\n    \"\"\"\n    Train a Decision Tree classifier and return feature importances.\n\n    Args:\n        y (str): Target column name OR text metadata field name (when linkage_method is specified).\n        top_n (int): Number of top features to display.\n        test_size (float): Proportion of dataset to include in test split.\n        random_state (int): Random state for reproducibility.\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n    Returns:\n        dict or str: Feature importances and accuracy metrics.\n    \"\"\"\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n    Y_raw = self._convert_to_binary(Y_raw)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_np, Y_raw, test_size=test_size, random_state=random_state\n    )\n\n    # print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n    # print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n\n    # Train a RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=100, random_state=ml_config.CLASSIFIER_RANDOM_STATE)\n    clf.fit(X_train, y_train)\n\n    # Compute permutation importance\n    results = permutation_importance(\n        clf, X_test, y_test, n_repeats=10, random_state=ml_config.CLASSIFIER_RANDOM_STATE\n    )\n\n    # classifier = DecisionTreeClassifier(random_state=random_state) # type: ignore\n    # classifier.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    _confusion_matrix = confusion_matrix(y_test, y_pred)\n    print(\n        f\"Confusion Matrix for Decision Tree predicting {y}:\\n{_confusion_matrix}\"\n    )\n    # Output\n    # [[2 0]\n    #  [2 0]]\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"\\nAccuracy: {accuracy}\\n\")\n\n    # Retrieve feature importance scores\n    importance = results.importances_mean\n\n    # Get indices of top N important features\n    top_n_indices = np.argsort(importance)[-top_n:][::-1]\n\n    # Display feature importance\n    print(f\"==== Top {top_n} important features ====\\n\")\n    _importance = \"\"\n    for i, v in enumerate(top_n_indices):\n        print(f\"Feature: {X.columns[v]}, Score: {importance[v]:.5f}\")\n        _importance += f\"Feature: {X.columns[v]}, Score: {importance[v]:.5f}\\n\"\n\n    if self._csv.corpus is not None:\n        self._csv.corpus.metadata[\"decision_tree_accuracy\"] = (\n            f\"Decision Tree accuracy for predicting {y}: {accuracy*100:.2f}%\"\n        )\n        self._csv.corpus.metadata[\"decision_tree_confusion_matrix\"] = (\n            f\"Confusion Matrix for Decision Tree predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n        )\n        self._csv.corpus.metadata[\"decision_tree_feature_importance\"] = _importance\n    if mcp:\n        return f\"\"\"\n        Confusion Matrix for Decision Tree predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\\nTop {top_n} important features:\\n{_importance}\n        Accuracy: {accuracy*100:.2f}%\n        \"\"\"\n    return _confusion_matrix, importance\n</code></pre>"},{"location":"modules/#ml.ML.get_lstm_predictions","title":"<code>get_lstm_predictions(y, mcp=False)</code>","text":"<p>Train an LSTM model on text data to predict an outcome variable. This tests if the texts converge towards predicting the outcome.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Name of the outcome variable in the DataFrame</p> required <code>mcp</code> <code>bool</code> <p>If True, return a string format suitable for MCP</p> <code>False</code> <p>Returns:</p> Type Description <p>Evaluation metrics as string (if mcp=True) or dict</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_lstm_predictions(self, y: str, mcp=False):\n    \"\"\"\n    Train an LSTM model on text data to predict an outcome variable.\n    This tests if the texts converge towards predicting the outcome.\n\n    Args:\n        y (str): Name of the outcome variable in the DataFrame\n        mcp (bool): If True, return a string format suitable for MCP\n\n    Returns:\n        Evaluation metrics as string (if mcp=True) or dict\n    \"\"\"\n    if ML_INSTALLED is False:\n        logger.error(\n            \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n        )\n        if mcp:\n            return \"ML dependencies are not installed. Please install with: pip install crisp-t[ml]\"\n        return None\n\n    if self._csv is None:\n        logger.error(\"CSV data is not set.\")\n        if mcp:\n            return \"CSV data is not set. Cannot perform LSTM prediction.\"\n        return None\n\n    _corpus = self._csv.corpus\n    if _corpus is None:\n        logger.error(\"Corpus is not available.\")\n        if mcp:\n            return \"Corpus is not available. Cannot perform LSTM prediction.\"\n        return None\n\n    # Check if id_column exists\n    id_column = \"id\"\n    if not hasattr(self._csv, \"df\") or self._csv.df is None:\n        logger.error(\"DataFrame is not available in CSV.\")\n        if mcp:\n            return \"This tool can be used only if texts and outcome variables align. DataFrame is missing.\"\n        return None\n\n    if id_column not in self._csv.df.columns:\n        logger.error(\n            f\"The id_column '{id_column}' does not exist in the DataFrame.\"\n        )\n        if mcp:\n            return f\"This tool can be used only if texts and outcome variables align. The '{id_column}' column is missing from the DataFrame.\"\n        return None\n\n    # Check if outcome variable exists\n    if y not in self._csv.df.columns:\n        logger.error(f\"The outcome variable '{y}' does not exist in the DataFrame.\")\n        if mcp:\n            return f\"The outcome variable '{y}' does not exist in the DataFrame.\"\n        return None\n\n    # Process documents and align with outcome variable\n    try:\n        # Build vocabulary from all documents\n        from collections import Counter\n\n        word_counts = Counter()\n        tokenized_docs = []\n\n        for doc in tqdm(_corpus.documents, desc=\"Tokenizing documents\", disable=len(_corpus.documents) &lt; 10):\n            # Simple tokenization - split on whitespace and lowercase\n            tokens = doc.text.lower().split()\n            tokenized_docs.append(tokens)\n            word_counts.update(tokens)\n\n        # Create vocabulary with most common words (limit to 10000)\n        vocab_size = min(ml_config.LSTM_VOCAB_SIZE, len(word_counts)) + 1  # +1 for padding\n        most_common = word_counts.most_common(vocab_size - 1)\n        word_to_idx = {\n            word: idx + 1 for idx, (word, _) in enumerate(most_common)\n        }  # 0 reserved for padding\n\n        # Convert documents to sequences of indices\n        max_length = ml_config.LSTM_MAX_LENGTH  # Maximum sequence length\n        sequences = []\n        doc_ids = []\n\n        for doc, tokens in tqdm(zip(_corpus.documents, tokenized_docs), total=len(_corpus.documents), desc=\"Converting to sequences\", disable=len(_corpus.documents) &lt; 10):\n            # Convert tokens to indices\n            seq = [word_to_idx.get(token, 0) for token in tokens]\n            # Pad or truncate to max_length\n            if len(seq) &gt; max_length:\n                seq = seq[:max_length]\n            else:\n                seq = seq + [0] * (max_length - len(seq))\n            sequences.append(seq)\n            doc_ids.append(doc.id)\n\n        # Align with outcome variable using id column\n        df = self._csv.df.set_index(id_column)\n\n        aligned_sequences = []\n        aligned_outcomes = []\n\n        df_index_str = list(str(idx) for idx in df.index)\n        for doc_id, seq in zip(doc_ids, sequences):\n            if doc_id in df_index_str:\n                aligned_sequences.append(seq)\n                # Select y from df where id_column == doc_id, using string comparison\n                matched_row = df.loc[\n                    [idx for idx in df.index if str(idx) == str(doc_id)]\n                ]\n                if not matched_row.empty:\n                    aligned_outcomes.append(matched_row.iloc[0][y])\n\n        if len(aligned_sequences) == 0:\n            logger.error(\"No documents could be aligned with the outcome variable.\")\n            if mcp:\n                return \"This tool can be used only if texts and outcome variables align. No matching IDs found.\"\n            return None\n\n        # Convert to tensors\n        X_tensor = torch.LongTensor(aligned_sequences)  # type: ignore\n        y_array = np.array(aligned_outcomes)\n\n        # Handle binary classification\n        unique_values = np.unique(y_array)\n        num_classes = len(unique_values)\n\n        if num_classes &lt; 2:\n            logger.error(\n                f\"Need at least 2 classes for classification, found {num_classes}\"\n            )\n            if mcp:\n                return f\"Need at least 2 classes for classification, found {num_classes}\"\n            return None\n\n        # Map to 0/1 for binary classification\n        if num_classes == 2:\n            class_mapping = {unique_values[0]: 0.0, unique_values[1]: 1.0}\n            y_mapped = np.array(\n                [class_mapping[val] for val in y_array], dtype=np.float32\n            )\n        else:\n            # Multi-class not supported in this simple LSTM implementation\n            logger.error(\n                \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n            )\n            if mcp:\n                return \"Multi-class classification is not supported for LSTM. Please use binary outcome.\"\n            return None\n\n        y_tensor = torch.FloatTensor(y_mapped).view(-1, 1)  # type: ignore\n\n        # Split into train/test\n        from sklearn.model_selection import train_test_split\n\n        indices = list(range(len(X_tensor)))\n        train_idx, test_idx = train_test_split(\n            indices, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.CLASSIFIER_RANDOM_STATE\n        )\n\n        X_train = X_tensor[train_idx]\n        y_train = y_tensor[train_idx]\n        X_test = X_tensor[test_idx]\n        y_test = y_tensor[test_idx]\n\n        # Create model\n        model = SimpleLSTM(vocab_size=vocab_size)  # type: ignore\n        criterion = nn.BCELoss()  # type: ignore\n        optimizer = optim.Adam(model.parameters(), lr=ml_config.LSTM_LEARNING_RATE)  # type: ignore\n\n        # Create data loaders\n        train_dataset = TensorDataset(X_train, y_train)  # type: ignore\n        train_loader = DataLoader(train_dataset, batch_size=ml_config.LSTM_BATCH_SIZE, shuffle=True)  # type: ignore\n\n        # Training\n        epochs = max(self._epochs, ml_config.LSTM_EPOCHS)  # Use at least configured epochs for LSTM\n        model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch_x, batch_y in train_loader:\n                optimizer.zero_grad()\n                predictions = model(batch_x)\n                loss = criterion(predictions, batch_y)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            avg_loss = total_loss / len(train_loader)\n            logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n        # Evaluation\n        model.eval()\n        with torch.no_grad():  # type: ignore\n            train_preds = model(X_train)\n            test_preds = model(X_test)\n\n            train_preds_binary = (train_preds &gt;= 0.5).float()\n            test_preds_binary = (test_preds &gt;= 0.5).float()\n\n            train_accuracy = (train_preds_binary == y_train).float().mean().item()\n            test_accuracy = (test_preds_binary == y_test).float().mean().item()\n\n        # Calculate additional metrics for test set\n        y_test_np = y_test.cpu().numpy().flatten()\n        test_preds_np = test_preds_binary.cpu().numpy().flatten()\n\n        # Confusion matrix elements\n        tp = ((test_preds_np == 1) &amp; (y_test_np == 1)).sum()\n        tn = ((test_preds_np == 0) &amp; (y_test_np == 0)).sum()\n        fp = ((test_preds_np == 1) &amp; (y_test_np == 0)).sum()\n        fn = ((test_preds_np == 0) &amp; (y_test_np == 1)).sum()\n\n        # Calculate precision, recall, F1\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n        f1 = (\n            2 * (precision * recall) / (precision + recall)\n            if (precision + recall) &gt; 0\n            else 0\n        )\n\n        result_msg = (\n            f\"LSTM Model Evaluation for predicting '{y}':\\n\"\n            f\"  Vocabulary size: {vocab_size}\\n\"\n            f\"  Training samples: {len(X_train)}, Test samples: {len(X_test)}\\n\"\n            f\"  Epochs: {epochs}\\n\"\n            f\"  Train accuracy: {train_accuracy*100:.2f}%\\n\"\n            f\"  Test accuracy (convergence): {test_accuracy*100:.2f}%\\n\"\n            f\"  True Positive: {tp}, False Positive: {fp}, True Negative: {tn}, False Negative: {fn}\\n\"\n            f\"  Precision: {precision:.3f}\\n\"\n            f\"  Recall: {recall:.3f}\\n\"\n            f\"  F1-Score: {f1:.3f}\\n\"\n        )\n\n        print(f\"\\n{result_msg}\")\n\n        # Store in corpus metadata\n        if _corpus is not None:\n            _corpus.metadata[ml_config.METADATA_KEY_LSTM] = result_msg\n\n        if mcp:\n            return result_msg\n\n        return {\n            \"vocab_size\": vocab_size,\n            \"train_samples\": len(X_train),\n            \"test_samples\": len(X_test),\n            \"epochs\": epochs,\n            \"train_accuracy\": train_accuracy,\n            \"test_accuracy\": test_accuracy,\n            \"true_positive\": tp,\n            \"false_positive\": fp,\n            \"true_negative\": tn,\n            \"false_negative\": fn,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n        }\n\n    except Exception as e:\n        logger.exception(f\"Error in LSTM prediction: {e}\")\n        if mcp:\n            return f\"Error in LSTM prediction: {e}\"\n        return None\n</code></pre>"},{"location":"modules/#ml.ML.get_nnet_predictions","title":"<code>get_nnet_predictions(y, mcp=False, linkage_method=None, aggregation='majority')</code>","text":"<p>Extended: Handles binary (BCELoss) and multi-class (CrossEntropyLoss). Returns list of predicted original class labels.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name OR text metadata field name (when linkage_method is specified).</p> required <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').</p> <code>'majority'</code> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_nnet_predictions(self, y: str, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n    \"\"\"\n    Extended: Handles binary (BCELoss) and multi-class (CrossEntropyLoss).\n    Returns list of predicted original class labels.\n\n    Args:\n        y (str): Target column name OR text metadata field name (when linkage_method is specified).\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n    \"\"\"\n    if ML_INSTALLED is False:\n        logger.info(\n            \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n        )\n        return None\n\n    if self._csv is None:\n        raise ValueError(\n            \"CSV data is not set. Please set self.csv before calling profile.\"\n        )\n    _corpus = self._csv.corpus\n\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n\n    unique_classes = np.unique(Y_raw)\n    num_classes = unique_classes.size\n    if num_classes &lt; 2:\n        raise ValueError(f\"Need at least 2 classes; found {num_classes}.\")\n\n    vnum = X_np.shape[1]\n\n    # Binary path\n    if num_classes == 2:\n        # Map to {0.0,1.0} for BCELoss if needed\n        mapping_applied = False\n        class_mapping = {}\n        inverse_mapping = {}\n        # Ensure deterministic order\n        sorted_classes = sorted(unique_classes.tolist())\n        if not (sorted_classes == [0, 1] or sorted_classes == [0.0, 1.0]):\n            class_mapping = {sorted_classes[0]: 0.0, sorted_classes[1]: 1.0}\n            inverse_mapping = {v: k for k, v in class_mapping.items()}\n            Y_mapped = np.vectorize(class_mapping.get)(Y_raw).astype(np.float32)\n            mapping_applied = True\n        else:\n            Y_mapped = Y_raw.astype(np.float32)\n\n        model = NeuralNet(vnum)\n        try:\n            criterion = nn.BCELoss()  # type: ignore\n            optimizer = optim.Adam(model.parameters(), lr=ml_config.NNET_LEARNING_RATE)  # type: ignore\n\n            X_tensor = torch.from_numpy(X_np)  # type: ignore\n            y_tensor = torch.from_numpy(Y_mapped.astype(np.float32)).view(-1, 1)  # type: ignore\n\n            dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n            dataloader = DataLoader(dataset, batch_size=ml_config.NNET_BATCH_SIZE, shuffle=True)  # type: ignore\n        except Exception as e:\n            logger.exception(f\"Error occurred while creating DataLoader: {e}\")\n            return None\n\n        for _ in range(self._epochs):\n            for batch_X, batch_y in dataloader:\n                optimizer.zero_grad()\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                if torch.isnan(loss):  # type: ignore\n                    raise RuntimeError(\"NaN loss encountered.\")\n                loss.backward()\n                optimizer.step()\n\n        # Predictions\n        bin_preds_internal = None\n        if torch:\n            with torch.no_grad():\n                probs = model(torch.from_numpy(X_np)).view(-1).cpu().numpy()\n            bin_preds_internal = (probs &gt;= 0.5).astype(int)\n\n        if mapping_applied:\n            preds = [inverse_mapping[float(p)] for p in bin_preds_internal]  # type: ignore\n            y_eval = np.vectorize(class_mapping.get)(Y_raw).astype(int)\n            preds_eval = bin_preds_internal\n        else:\n            preds = bin_preds_internal.tolist()  # type: ignore\n            y_eval = Y_mapped.astype(int)\n            preds_eval = bin_preds_internal\n\n        accuracy = (preds_eval == y_eval).sum() / len(y_eval)\n        print(\n            f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n        )\n        if _corpus is not None:\n            _corpus.metadata[ml_config.METADATA_KEY_NNET] = (\n                f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n            )\n        if mcp:\n            return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} epochs gave an accuracy (convergence): {accuracy*100:.2f}%\"\n        return preds\n\n    # Multi-class path\n    # Map original classes to indices\n    sorted_classes = sorted(unique_classes.tolist())\n    class_to_idx = {c: i for i, c in enumerate(sorted_classes)}\n    idx_to_class = {i: c for c, i in class_to_idx.items()}\n    Y_idx = np.vectorize(class_to_idx.get)(Y_raw).astype(np.int64)\n\n    model = MultiClassNet(vnum, num_classes)\n    criterion = nn.CrossEntropyLoss()  # type: ignore\n    optimizer = optim.Adam(model.parameters(), lr=ml_config.NNET_LEARNING_RATE)  # type: ignore\n\n    X_tensor = torch.from_numpy(X_np)  # type: ignore\n    y_tensor = torch.from_numpy(Y_idx)  # type: ignore\n\n    dataset = TensorDataset(X_tensor, y_tensor)  # type: ignore\n    dataloader = DataLoader(dataset, batch_size=ml_config.NNET_BATCH_SIZE, shuffle=True)  # type: ignore\n\n    for _ in range(self._epochs):\n        for batch_X, batch_y in dataloader:\n            optimizer.zero_grad()\n            logits = model(batch_X)\n            loss = criterion(logits, batch_y)\n            if torch.isnan(loss):  # type: ignore\n                raise RuntimeError(\"NaN loss encountered.\")\n            loss.backward()\n            optimizer.step()\n\n    with torch.no_grad():  # type: ignore\n        logits_full = model(torch.from_numpy(X_np))  # type: ignore\n        pred_indices = torch.argmax(logits_full, dim=1).cpu().numpy()  # type: ignore\n\n    preds = [idx_to_class[i] for i in pred_indices]\n    accuracy = (pred_indices == Y_idx).sum() / len(Y_idx)\n    print(\n        f\"\\nPredicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\\n\"\n    )\n    if _corpus is not None:\n        _corpus.metadata[ml_config.METADATA_KEY_NNET] = (\n            f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n        )\n    if mcp:\n        return f\"Predicting {y} with {X.shape[1]} features for {self._epochs} gave an accuracy (convergence): {accuracy*100:.2f}%\"\n    return preds\n</code></pre>"},{"location":"modules/#ml.ML.get_pca","title":"<code>get_pca(y, n=3, mcp=False, linkage_method=None, aggregation='majority')</code>","text":"<p>Perform a manual PCA (no sklearn PCA) on the feature matrix for target y.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name (used only for data preparation) OR text metadata field name (when linkage_method is specified).</p> required <code>n</code> <code>int</code> <p>Number of principal components to keep.</p> <code>3</code> <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').</p> <code>'majority'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>{ 'covariance_matrix': cov_mat, 'eigenvalues': eig_vals_sorted, 'eigenvectors': eig_vecs_sorted, 'explained_variance_ratio': var_exp, 'cumulative_explained_variance_ratio': cum_var_exp, 'projection_matrix': matrix_w, 'transformed': X_pca</p> <p>}</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_pca(self, y: str, n: int = 3, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n    \"\"\"\n    Perform a manual PCA (no sklearn PCA) on the feature matrix for target y.\n\n    Args:\n        y (str): Target column name (used only for data preparation) OR text metadata field name (when linkage_method is specified).\n        n (int): Number of principal components to keep.\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n    Returns:\n        dict: {\n            'covariance_matrix': cov_mat,\n            'eigenvalues': eig_vals_sorted,\n            'eigenvectors': eig_vecs_sorted,\n            'explained_variance_ratio': var_exp,\n            'cumulative_explained_variance_ratio': cum_var_exp,\n            'projection_matrix': matrix_w,\n            'transformed': X_pca\n        }\n    \"\"\"\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n    X_std = StandardScaler().fit_transform(X_np)\n\n    cov_mat = np.cov(X_std.T)\n    eig_vals, eig_vecs = np.linalg.eigh(cov_mat)  # symmetric matrix -&gt; eigh\n\n    # Sort eigenvalues (and vectors) descending\n    idx = np.argsort(eig_vals)[::-1]\n    eig_vals_sorted = eig_vals[idx]\n    eig_vecs_sorted = eig_vecs[:, idx]\n\n    factors = X_std.shape[1]\n    n = max(1, min(n, factors))\n\n    # Explained variance ratios\n    tot = eig_vals_sorted.sum()\n    var_exp = (eig_vals_sorted / tot) * 100.0\n    cum_var_exp = np.cumsum(var_exp)\n\n    # Projection matrix (first n eigenvectors)\n    matrix_w = eig_vecs_sorted[:, :n]\n\n    # Project data\n    X_pca = X_std @ matrix_w\n\n    # Optional prints (retain original behavior)\n    print(\"Covariance matrix:\\n\", cov_mat)\n    print(\"Eigenvalues (desc):\\n\", eig_vals_sorted)\n    print(\"Explained variance (%):\\n\", var_exp[:n])\n    print(\"Cumulative explained variance (%):\\n\", cum_var_exp[:n])\n    print(\"Projection matrix (W):\\n\", matrix_w)\n    print(\"Transformed (first 5 rows):\\n\", X_pca[:5])\n\n    result = {\n        \"covariance_matrix\": cov_mat,\n        \"eigenvalues\": eig_vals_sorted,\n        \"eigenvectors\": eig_vecs_sorted,\n        \"explained_variance_ratio\": var_exp,\n        \"cumulative_explained_variance_ratio\": cum_var_exp,\n        \"projection_matrix\": matrix_w,\n        \"transformed\": X_pca,\n    }\n\n    if self._csv.corpus is not None:\n        self._csv.corpus.metadata[ml_config.METADATA_KEY_PCA] = (\n            f\"PCA kept {n} components explaining \"\n            f\"{cum_var_exp[n-1]:.2f}% variance.\"\n        )\n    if mcp:\n        return (\n            f\"PCA kept {n} components explaining {cum_var_exp[n-1]:.2f}% variance.\"\n        )\n    return result\n</code></pre>"},{"location":"modules/#ml.ML.get_regression","title":"<code>get_regression(y, mcp=False, linkage_method=None, aggregation='mean')</code>","text":"<p>Perform linear or logistic regression based on the outcome variable type.</p> <p>If the outcome is binary, fit a logistic regression model. Otherwise, fit a linear regression model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name for the regression OR text metadata field name (when linkage_method is specified).</p> required <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').              Default is 'mean' for regression tasks.</p> <code>'mean'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Regression results including coefficients, intercept, and metrics.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_regression(self, y: str, mcp=False, linkage_method: str | None = None, aggregation: str = \"mean\"):\n    \"\"\"\n    Perform linear or logistic regression based on the outcome variable type.\n\n    If the outcome is binary, fit a logistic regression model.\n    Otherwise, fit a linear regression model.\n\n    Args:\n        y (str): Target column name for the regression OR text metadata field name (when linkage_method is specified).\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n                         Default is 'mean' for regression tasks.\n\n    Returns:\n        dict: Regression results including coefficients, intercept, and metrics.\n    \"\"\"\n    if ML_INSTALLED is False:\n        logger.info(\n            \"ML dependencies are not installed. Please install them by ```pip install crisp-t[ml] to use ML features.\"\n        )\n        return None\n\n    if self._csv is None:\n        raise ValueError(\n            \"CSV data is not set. Please set self.csv before calling get_regression.\"\n        )\n\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n\n    # Check if outcome is binary (logistic) or continuous (linear)\n    unique_values = np.unique(Y_raw)\n    num_unique = len(unique_values)\n\n    # Determine if binary classification or regression\n    is_binary = num_unique == 2\n\n    if is_binary:\n        # Logistic Regression\n        print(f\"\\n=== Logistic Regression for {y} ===\")\n        print(f\"Binary outcome detected with values: {unique_values}\")\n\n        model = LogisticRegression(max_iter=1000, random_state=ml_config.CLASSIFIER_RANDOM_STATE)\n        model.fit(X_np, Y_raw)\n\n        # Predictions\n        y_pred = model.predict(X_np)\n\n        # Accuracy\n        accuracy = accuracy_score(Y_raw, y_pred)\n        print(f\"\\nAccuracy: {accuracy*100:.2f}%\")\n\n        # Coefficients and Intercept\n        print(\"\\nCoefficients:\")\n        for i, coef in enumerate(model.coef_[0]):\n            feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n            print(f\"  {feature_name}: {coef:.5f}\")\n\n        print(f\"\\nIntercept: {model.intercept_[0]:.5f}\")\n\n        coef_str = \"\\n\".join(\n            [\n                f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                for i, coef in enumerate(model.coef_[0])\n            ]\n        )\n\n        # Store in metadata\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"logistic_regression_accuracy\"] = (\n                f\"Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\"\n            )\n            self._csv.corpus.metadata[\"logistic_regression_coefficients\"] = (\n                f\"Coefficients:\\n{coef_str}\"\n            )\n            self._csv.corpus.metadata[\"logistic_regression_intercept\"] = (\n                f\"Intercept: {model.intercept_[0]:.5f}\"\n            )\n\n        if mcp:\n            return f\"\"\"\n            Logistic Regression accuracy for predicting {y}: {accuracy*100:.2f}%\n            Coefficients:\n            {coef_str}\n            Intercept: {model.intercept_[0]:.5f}\n            \"\"\"\n        return {\n            \"model_type\": \"logistic\",\n            \"accuracy\": accuracy,\n            \"coefficients\": model.coef_[0],\n            \"intercept\": model.intercept_[0],\n            \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n        }\n    else:\n        # Linear Regression\n        print(f\"\\n=== Linear Regression for {y} ===\")\n        print(f\"Continuous outcome detected with {num_unique} unique values\")\n\n        model = LinearRegression()\n        model.fit(X_np, Y_raw)\n\n        # Predictions\n        y_pred = model.predict(X_np)\n\n        # Metrics\n        mse = mean_squared_error(Y_raw, y_pred)\n        r2 = r2_score(Y_raw, y_pred)\n        print(f\"\\nMean Squared Error (MSE): {mse:.5f}\")\n        print(f\"R\u00b2 Score: {r2:.5f}\")\n\n        # Coefficients and Intercept\n        print(\"\\nCoefficients:\")\n        for i, coef in enumerate(model.coef_):\n            feature_name = X.columns[i] if hasattr(X, \"columns\") else f\"Feature_{i}\"\n            print(f\"  {feature_name}: {coef:.5f}\")\n\n        print(f\"\\nIntercept: {model.intercept_:.5f}\")\n\n        coef_str = \"\\n\".join(\n            [\n                f\"  {X.columns[i] if hasattr(X, 'columns') else f'Feature_{i}'}: {coef:.5f}\"\n                for i, coef in enumerate(model.coef_)\n            ]\n        )\n\n        # Store in metadata\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"linear_regression_mse\"] = (\n                f\"Linear Regression MSE for predicting {y}: {mse:.5f}\"\n            )\n            self._csv.corpus.metadata[\"linear_regression_r2\"] = (\n                f\"Linear Regression R\u00b2 for predicting {y}: {r2:.5f}\"\n            )\n            self._csv.corpus.metadata[\"linear_regression_coefficients\"] = (\n                f\"Coefficients:\\n{coef_str}\"\n            )\n            self._csv.corpus.metadata[\"linear_regression_intercept\"] = (\n                f\"Intercept: {model.intercept_:.5f}\"\n            )\n\n        if mcp:\n            return f\"\"\"\n            Linear Regression MSE for predicting {y}: {mse:.5f}\n            R\u00b2: {r2:.5f}\n            Feature Names and Coefficients:\n            {coef_str}\n            Intercept: {model.intercept_:.5f}\n            \"\"\"\n        return {\n            \"model_type\": \"linear\",\n            \"mse\": mse,\n            \"r2\": r2,\n            \"coefficients\": model.coef_,\n            \"intercept\": model.intercept_,\n            \"feature_names\": X.columns.tolist() if hasattr(X, \"columns\") else None,\n        }\n</code></pre>"},{"location":"modules/#ml.ML.get_xgb_classes","title":"<code>get_xgb_classes(y, oversample=False, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.KMEANS_RANDOM_STATE, mcp=False, linkage_method=None, aggregation='majority')</code>","text":"<p>Train an XGBoost classifier and return feature importances.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name OR text metadata field name (when linkage_method is specified).</p> required <code>oversample</code> <code>bool</code> <p>Whether to oversample minority classes.</p> <code>False</code> <code>test_size</code> <code>float</code> <p>Proportion of dataset to include in test split.</p> <code>CLASSIFIER_TEST_SIZE</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility.</p> <code>KMEANS_RANDOM_STATE</code> <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').</p> <code>'majority'</code> <p>Returns:</p> Type Description <p>dict or str: Feature importances and accuracy metrics.</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def get_xgb_classes(\n    self, y: str, oversample=False, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.KMEANS_RANDOM_STATE, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"\n):\n    \"\"\"\n    Train an XGBoost classifier and return feature importances.\n\n    Args:\n        y (str): Target column name OR text metadata field name (when linkage_method is specified).\n        oversample (bool): Whether to oversample minority classes.\n        test_size (float): Proportion of dataset to include in test split.\n        random_state (int): Random state for reproducibility.\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n    Returns:\n        dict or str: Feature importances and accuracy metrics.\n    \"\"\"\n    try:\n        from xgboost import XGBClassifier  # type: ignore\n    except ImportError:\n        raise ImportError(\n            \"XGBoost is not installed. Please install it via `pip install crisp-t[xg]`.\"\n        ) from None\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n    if ML_INSTALLED:\n        # ValueError: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1 2]\n        # convert y to binary\n        Y_binary = (Y_raw == 1).astype(int)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_np, Y_binary, test_size=test_size, random_state=random_state\n        )\n        classifier = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")  # type: ignore\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        _confusion_matrix = confusion_matrix(y_test, y_pred)\n        print(f\"Confusion Matrix for XGBoost predicting {y}:\\n{_confusion_matrix}\")\n        # Output\n        # [[2 0]\n        #  [2 0]]\n        if self._csv.corpus is not None:\n            self._csv.corpus.metadata[\"xgb_confusion_matrix\"] = (\n                f\"Confusion Matrix for XGBoost predicting {y}:\\n{_confusion_matrix}\"\n            )\n        if mcp:\n            return f\"\"\"\n            Confusion Matrix for XGBoost predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\n            \"\"\"\n        return _confusion_matrix\n    else:\n        raise ImportError(\"ML dependencies are not installed.\")\n</code></pre>"},{"location":"modules/#ml.ML.knn_search","title":"<code>knn_search(y, n=3, r=3, mcp=False, linkage_method=None, aggregation='majority')</code>","text":"<p>Perform K-Nearest Neighbors search.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name OR text metadata field name (when linkage_method is specified).</p> required <code>n</code> <code>int</code> <p>Number of nearest neighbors to find.</p> <code>3</code> <code>r</code> <code>int</code> <p>Record number to search from (1-based index).</p> <code>3</code> <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').</p> <code>'majority'</code> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def knn_search(self, y: str, n=3, r=3, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n    \"\"\"\n    Perform K-Nearest Neighbors search.\n\n    Args:\n        y (str): Target column name OR text metadata field name (when linkage_method is specified).\n        n (int): Number of nearest neighbors to find.\n        r (int): Record number to search from (1-based index).\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n    \"\"\"\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n    kdt = KDTree(X_np, leaf_size=2, metric=\"euclidean\")\n    dist, ind = kdt.query(X_np[r - 1 : r, :], k=n)\n    # Display results as human readable (1-based)\n    ind = (ind + 1).tolist()  # Convert to 1-based index\n    dist = dist.tolist()\n    print(\n        f\"\\nKNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\\n\"\n    )\n    if self._csv.corpus is not None:\n        self._csv.corpus.metadata[\"knn_search\"] = (\n            f\"KNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\"\n        )\n    if mcp:\n        return f\"KNN search for {y} (n={n}, record no: {r}): {ind} with distances {dist}\"\n    return dist, ind\n</code></pre>"},{"location":"modules/#ml.ML.svm_confusion_matrix","title":"<code>svm_confusion_matrix(y, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.KMEANS_RANDOM_STATE, mcp=False, linkage_method=None, aggregation='majority')</code>","text":"<p>Generate confusion matrix for SVM</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>str</code> <p>Target column name OR text metadata field name (when linkage_method is specified).</p> required <code>test_size</code> <code>float</code> <p>Proportion of dataset to include in test split.</p> <code>CLASSIFIER_TEST_SIZE</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility.</p> <code>KMEANS_RANDOM_STATE</code> <code>mcp</code> <code>bool</code> <p>Whether to return MCP-formatted string.</p> <code>False</code> <code>linkage_method</code> <code>str</code> <p>Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').</p> <code>None</code> <code>aggregation</code> <code>str</code> <p>Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').</p> <code>'majority'</code> <p>Returns:</p> Type Description <p>[list] -- [description]</p> Source code in <code>src/crisp_t/ml.py</code> <pre><code>def svm_confusion_matrix(self, y: str, test_size=ml_config.CLASSIFIER_TEST_SIZE, random_state=ml_config.KMEANS_RANDOM_STATE, mcp=False, linkage_method: str | None = None, aggregation: str = \"majority\"):\n    \"\"\"Generate confusion matrix for SVM\n\n    Args:\n        y (str): Target column name OR text metadata field name (when linkage_method is specified).\n        test_size (float): Proportion of dataset to include in test split.\n        random_state (int): Random state for reproducibility.\n        mcp (bool): Whether to return MCP-formatted string.\n        linkage_method (str, optional): Linkage method for text metadata outcomes ('id', 'embedding', 'temporal', 'keyword').\n        aggregation (str): Aggregation strategy when multiple documents link to one row ('majority', 'mean', 'first', 'mode').\n\n    Returns:\n        [list] -- [description]\n    \"\"\"\n    X_np, Y_raw, X, Y = self._process_xy(y=y, linkage_method=linkage_method, aggregation=aggregation)\n    Y = self._convert_to_binary(Y)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, Y, test_size=test_size, random_state=random_state\n    )\n    sc = StandardScaler()\n    # Issue #22\n    y_test = y_test.astype(\"int\")\n    y_train = y_train.astype(\"int\")\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    classifier = SVC(kernel=\"linear\", random_state=ml_config.KMEANS_RANDOM_STATE)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    # Issue #22\n    y_pred = y_pred.astype(\"int\")\n    _confusion_matrix = confusion_matrix(y_test, y_pred)\n    print(f\"Confusion Matrix for SVM predicting {y}:\\n{_confusion_matrix}\")\n    # Output\n    # [[2 0]\n    #  [2 0]]\n    if self._csv.corpus is not None:\n        self._csv.corpus.metadata[ml_config.METADATA_KEY_SVM_CONFUSION] = (\n            f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n        )\n\n    if mcp:\n        return f\"Confusion Matrix for SVM predicting {y}:\\n{self.format_confusion_matrix_to_human_readable(_confusion_matrix)}\"\n\n    return _confusion_matrix\n</code></pre>"},{"location":"modules/#network.Network","title":"<code>Network</code>","text":"<p>A class to represent a network of documents and their relationships.</p> Source code in <code>src/crisp_t/network.py</code> <pre><code>class Network:\n    \"\"\"\n    A class to represent a network of documents and their relationships.\n    \"\"\"\n\n    def __init__(self, corpus: Corpus):\n        \"\"\"\n        Initialize the Network with a corpus.\n\n        :param corpus: Corpus object containing documents to be included in the network.\n        \"\"\"\n        self._corpus = corpus\n        self._cluster = Cluster(corpus)\n        self._processed_docs = self._cluster.processed_docs\n        self._graph = None\n\n    def cooccurence_network(self, window_size=2):\n        self._graph = network.build_cooccurrence_network(\n            self._processed_docs, window_size=window_size\n        )\n        return self._graph\n\n    def similarity_network(self, method=\"levenshtein\"):\n        text = Text(self._corpus)\n        docs = text.make_spacy_doc()\n        data = [sent.text.lower() for sent in docs.sents]\n        self._graph = network.build_similarity_network(data, method)\n        return self._graph\n\n    def graph_as_dict(self):\n        \"\"\"\n        Convert the graph to a dictionary representation.\n\n        :return: Dictionary representation of the graph.\n        \"\"\"\n        if self._graph is None:\n            raise ValueError(\n                \"Graph has not been created yet. Call cooccurence_network() first.\"\n            )\n        return sorted(self._graph.adjacency())[0]\n</code></pre>"},{"location":"modules/#network.Network.__init__","title":"<code>__init__(corpus)</code>","text":"<p>Initialize the Network with a corpus.</p> <p>:param corpus: Corpus object containing documents to be included in the network.</p> Source code in <code>src/crisp_t/network.py</code> <pre><code>def __init__(self, corpus: Corpus):\n    \"\"\"\n    Initialize the Network with a corpus.\n\n    :param corpus: Corpus object containing documents to be included in the network.\n    \"\"\"\n    self._corpus = corpus\n    self._cluster = Cluster(corpus)\n    self._processed_docs = self._cluster.processed_docs\n    self._graph = None\n</code></pre>"},{"location":"modules/#network.Network.graph_as_dict","title":"<code>graph_as_dict()</code>","text":"<p>Convert the graph to a dictionary representation.</p> <p>:return: Dictionary representation of the graph.</p> Source code in <code>src/crisp_t/network.py</code> <pre><code>def graph_as_dict(self):\n    \"\"\"\n    Convert the graph to a dictionary representation.\n\n    :return: Dictionary representation of the graph.\n    \"\"\"\n    if self._graph is None:\n        raise ValueError(\n            \"Graph has not been created yet. Call cooccurence_network() first.\"\n        )\n    return sorted(self._graph.adjacency())[0]\n</code></pre>"},{"location":"modules/#visualize.QRVisualize","title":"<code>QRVisualize</code>","text":"Source code in <code>src/crisp_t/visualize.py</code> <pre><code>class QRVisualize:\n\n    def __init__(\n        self, corpus: Corpus | None = None, folder_path: str | None = None\n    ) -&gt; None:\n        # Matplotlib figure components assigned lazily by plotting methods\n        self.corpus = corpus\n        self.folder_path = folder_path\n        self.fig: Figure | None = None\n        self.ax: Axes | None = None\n        self.sc: PathCollection | None = None\n        self.annot: Annotation | None = None\n        self.names: list[str] = []\n        self.c: np.ndarray | None = None\n\n    def _ensure_columns(\n        self, df: pd.DataFrame, required: Iterable[str]\n    ) -&gt; pd.DataFrame:\n        \"\"\"Ensure that the DataFrame has the required columns.\n\n        Behavior:\n        - If all required columns already exist, return df unchanged.\n        - If the DataFrame has exactly the same number of columns as required,\n          rename columns positionally to match the required names.\n        - Otherwise, raise a ValueError listing the missing columns.\n        \"\"\"\n        required = list(required)\n        # Fast path: all required columns present\n        missing = [col for col in required if col not in df.columns]\n        if not missing:\n            return df\n\n        # If shape matches, attempt a positional rename\n        if len(df.columns) == len(required):\n            df = df.copy()\n            df.columns = required\n            return df\n\n        # Otherwise, cannot satisfy required columns\n        raise ValueError(f\"Missing required columns: {missing}\")\n\n    def _finalize_plot(\n        self,\n        fig: Figure,\n        folder_path: str | None,\n        show: bool,\n    ) -&gt; Figure:\n        if not folder_path:\n            folder_path = self.folder_path\n        if folder_path:\n            output_path = Path(folder_path)\n            if output_path.parent:\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n            fig.savefig(folder_path)\n        if show:\n            plt.show(block=False)\n        else:\n            plt.close(fig)\n        return fig\n\n    def plot_frequency_distribution_of_words(\n        self,\n        df: pd.DataFrame | None = None,\n        folder_path: str | None = None,\n        text_column: str = \"Text\",\n        bins: int = 100,\n        show: bool = True,\n    ) -&gt; tuple[Figure, Axes]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\") from e\n        df = self._ensure_columns(df, [text_column])\n        doc_lens = df[text_column].dropna().map(len).tolist()\n        if not doc_lens:\n            raise ValueError(\"No documents available to plot frequency distribution.\")\n\n        fig, ax = plt.subplots(figsize=(16, 7), dpi=160)\n        counts, _, _ = ax.hist(doc_lens, bins=bins, color=\"navy\")\n        counts = np.asarray(counts)\n        if counts.size:\n            ax.set_ylim(top=float(counts.max()) * 1.1)\n\n        stats = {\n            \"Mean\": round(np.mean(doc_lens), 2),\n            \"Median\": round(np.median(doc_lens), 2),\n            \"Stdev\": round(np.std(doc_lens), 2),\n            \"1%ile\": round(np.quantile(doc_lens, q=0.01), 2),\n            \"99%ile\": round(np.quantile(doc_lens, q=0.99), 2),\n        }\n        for idx, (label, value) in enumerate(stats.items()):\n            ax.text(\n                0.98,\n                0.98 - idx * 0.05,\n                f\"{label}: {value}\",\n                transform=ax.transAxes,\n                ha=\"right\",\n                va=\"top\",\n                fontsize=11,\n            )\n\n        ax.set(\n            ylabel=\"Number of Documents\",\n            xlabel=\"Document Word Count\",\n            title=\"Distribution of Document Word Counts\",\n        )\n        ax.tick_params(axis=\"both\", labelsize=12)\n        if doc_lens:\n            ax.set_xlim(left=0, right=max(doc_lens) * 1.05)\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, ax\n\n    def plot_distribution_by_topic(\n        self,\n        df: pd.DataFrame | None = None,\n        folder_path: str | None = None,\n        topic_column: str = \"Dominant_Topic\",\n        text_column: str = \"Text\",\n        bins: int = 100,\n        show: bool = True,\n    ) -&gt; tuple[Figure, np.ndarray]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\") from e\n        df = self._ensure_columns(df, [topic_column, text_column])\n        unique_topics = sorted(df[topic_column].dropna().unique())\n        if not unique_topics:\n            raise ValueError(\"No topics found to plot distribution.\")\n\n        n_topics = len(unique_topics)\n        n_cols = min(3, n_topics)\n        n_rows = math.ceil(n_topics / n_cols)\n        cols = list(mcolors.TABLEAU_COLORS.values())\n\n        fig, axes = plt.subplots(\n            n_rows,\n            n_cols,\n            figsize=(6 * n_cols, 5 * n_rows),\n            dpi=160,\n            sharex=True,\n            sharey=True,\n        )\n        if isinstance(axes, np.ndarray):\n            axes_flat = axes.flatten().tolist()\n        else:\n            axes_flat = [axes]\n\n        for idx, topic in enumerate(unique_topics):\n            ax = axes_flat[idx]\n            topic_series = cast(\n                pd.Series,\n                df.loc[df[topic_column] == topic, text_column],\n            )\n            topic_docs = topic_series.dropna()\n            doc_lens = topic_docs.map(len).tolist()\n            color = cols[idx % len(cols)]\n            if doc_lens:\n                ax.hist(doc_lens, bins=bins, color=color, alpha=0.7)\n                sns.kdeplot(\n                    doc_lens,\n                    color=\"black\",\n                    fill=False,\n                    ax=ax.twinx(),\n                    warn_singular=False,\n                )\n            ax.set(xlabel=\"Document Word Count\")\n            ax.set_ylabel(\"Number of Documents\", color=color)\n            ax.set_title(f\"Topic: {topic}\", fontdict=dict(size=14, color=color))\n            ax.tick_params(axis=\"y\", labelcolor=color, color=color)\n\n        for extra_ax in axes_flat[len(unique_topics) :]:\n            extra_ax.set_visible(False)\n\n        fig.tight_layout()\n        fig.suptitle(\n            \"Distribution of Document Word Counts by Dominant Topic\",\n            fontsize=20,\n            y=1.02,\n        )\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        axes_array = np.array(axes_flat, dtype=object).reshape(n_rows, n_cols)\n        return fig, axes_array\n\n    def plot_wordcloud(\n        self,\n        topics=None,\n        folder_path: str | None = None,\n        max_words: int = 50,\n        show: bool = True,\n    ) -&gt; tuple[Figure, np.ndarray]:\n        if not topics:\n            try:\n                topics = self.corpus.visualization[\"word_cloud\"]\n            except Exception as e:\n                raise ValueError(f\"Failed to retrieve topics from corpus: {e}\") from e\n        n_topics = len(topics)\n        n_cols = min(3, n_topics)\n        n_rows = math.ceil(n_topics / n_cols)\n        cols = list(mcolors.TABLEAU_COLORS.values())\n\n        fig, axes = plt.subplots(\n            n_rows,\n            n_cols,\n            figsize=(6 * n_cols, 4 * n_rows),\n            sharex=True,\n            sharey=True,\n        )\n        axes_flat = axes.flatten().tolist() if isinstance(axes, np.ndarray) else [axes]\n\n        for idx, (topic_id, words) in enumerate(topics):\n            ax = axes_flat[idx]\n            topic_words = dict(words)\n            color = cols[idx % len(cols)]\n            cloud = WordCloud(\n                stopwords=STOPWORDS,\n                background_color=\"white\",\n                width=800,\n                height=400,\n                max_words=max_words,\n                colormap=\"tab10\",\n                color_func=lambda *args, color=color, **kwargs: color,\n                prefer_horizontal=0.9,\n            )\n            cloud.generate_from_frequencies(topic_words)\n            ax.imshow(cloud)\n            ax.set_title(f\"Topic {topic_id}\", fontdict=dict(size=14))\n            ax.axis(\"off\")\n\n        for extra_ax in axes_flat[len(topics) :]:\n            extra_ax.set_visible(False)\n\n        fig.tight_layout()\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, np.array(axes_flat).reshape(n_rows, n_cols)\n\n    def plot_top_terms(\n        self,\n        df: pd.DataFrame | None = None,\n        term_column: str = \"term\",\n        frequency_column: str = \"frequency\",\n        top_n: int = 20,\n        folder_path: str | None = None,\n        ascending: bool = False,\n        show: bool = True,\n    ) -&gt; tuple[Figure, Axes]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\") from e\n        if top_n &lt;= 0:\n            raise ValueError(\"top_n must be greater than zero.\")\n\n        df = self._ensure_columns(df, [term_column, frequency_column])\n        subset = df[[term_column, frequency_column]].dropna()\n        if subset.empty:\n            raise ValueError(\"No data available to plot top terms.\")\n\n        subset = subset.sort_values(frequency_column, ascending=ascending).head(top_n)\n        subset = subset.iloc[::-1]\n\n        fig, ax = plt.subplots(figsize=(10, max(4, top_n * 0.4)))\n        ax.barh(subset[term_column], subset[frequency_column], color=\"steelblue\")\n        ax.set_xlabel(\"Frequency\")\n        ax.set_ylabel(\"Term\")\n        ax.set_title(\"Top Terms by Frequency\")\n        for idx, value in enumerate(subset[frequency_column]):\n            ax.text(value, idx, f\" {value}\", va=\"center\")\n        fig.tight_layout()\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, ax\n\n    def plot_correlation_heatmap(\n        self,\n        df: pd.DataFrame | None = None,\n        columns: Sequence[str] | None = None,\n        folder_path: str | None = None,\n        cmap: str = \"coolwarm\",\n        show: bool = True,\n    ) -&gt; tuple[Figure, Axes]:\n        if df is None:\n            try:\n                df = pd.DataFrame(self.corpus.visualization[\"assign_topics\"])\n            except Exception as e:\n                raise ValueError(f\"Failed to create DataFrame from corpus: {e}\") from e\n        if columns:\n            df = self._ensure_columns(df, columns)\n            data = df[list(columns)]\n        else:\n            data = df\n        if data.empty:\n            raise ValueError(\"No data available to compute correlation heatmap.\")\n\n        numeric_data = data.select_dtypes(include=[np.number])\n        if numeric_data.shape[1] &lt; 2:\n            raise ValueError(\n                \"At least two numeric columns are required for correlation heatmap.\"\n            )\n\n        corr = numeric_data.corr()\n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.heatmap(corr, ax=ax, cmap=cmap, annot=True, fmt=\".2f\", square=True)\n        ax.set_title(\"Correlation Heatmap\")\n        fig.tight_layout()\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, ax\n\n    def plot_importance(\n        self,\n        topics: Sequence[tuple[int, Sequence[tuple[str, float]]]],\n        processed_docs: Sequence[Sequence[str]],\n        folder_path: str | None = None,\n        show: bool = True,\n    ) -&gt; tuple[Figure, np.ndarray]:\n        if not topics:\n            raise ValueError(\"No topics provided to plot importance.\")\n        if not processed_docs:\n            raise ValueError(\"No processed documents provided to plot importance.\")\n\n        counter = Counter(word for doc in processed_docs for word in doc)\n        rows = []\n        for topic_id, words in topics:\n            for word, weight in words:\n                rows.append(\n                    {\n                        \"word\": word,\n                        \"topic_id\": topic_id,\n                        \"importance\": weight,\n                        \"word_count\": counter.get(word, 0),\n                    }\n                )\n\n        df = pd.DataFrame(rows)\n        if df.empty:\n            raise ValueError(\"Unable to build importance DataFrame from inputs.\")\n\n        topic_ids = sorted(df[\"topic_id\"].unique())\n        n_topics = len(topic_ids)\n        n_cols = min(3, n_topics)\n        n_rows = math.ceil(n_topics / n_cols)\n        cols = list(mcolors.TABLEAU_COLORS.values())\n\n        fig, axes = plt.subplots(\n            n_rows,\n            n_cols,\n            figsize=(7 * n_cols, 5 * n_rows),\n            sharey=False,\n            dpi=160,\n        )\n        axes_flat = axes.flatten().tolist() if isinstance(axes, np.ndarray) else [axes]\n\n        for idx, topic_id in enumerate(topic_ids):\n            ax = axes_flat[idx]\n            subset = df[df[\"topic_id\"] == topic_id]\n            color = cols[idx % len(cols)]\n            ax.bar(\n                subset[\"word\"],\n                subset[\"word_count\"],\n                color=color,\n                width=0.5,\n                alpha=0.4,\n                label=\"Word Count\",\n            )\n            ax_twin = ax.twinx()\n            ax_twin.plot(\n                subset[\"word\"],\n                subset[\"importance\"],\n                color=color,\n                marker=\"o\",\n                label=\"Importance\",\n            )\n            ax.set_title(f\"Topic {topic_id}\", color=color, fontsize=14)\n            ax.set_xlabel(\"Word\")\n            ax.set_ylabel(\"Word Count\", color=color)\n            ax.tick_params(axis=\"y\", labelcolor=color)\n            ax_twin.set_ylabel(\"Importance\", color=color)\n            ax_twin.tick_params(axis=\"y\", labelcolor=color)\n            ax.set_xticklabels(subset[\"word\"], rotation=30, ha=\"right\")\n            ax.legend(loc=\"upper left\")\n            ax_twin.legend(loc=\"upper right\")\n\n        for extra_ax in axes_flat[len(topic_ids) :]:\n            extra_ax.set_visible(False)\n\n        fig.tight_layout()\n        fig.suptitle(\n            \"Word Count and Importance of Topic Keywords\",\n            fontsize=20,\n            y=1.02,\n        )\n\n        fig = self._finalize_plot(fig, folder_path, show)\n        return fig, np.array(axes_flat).reshape(n_rows, n_cols)\n\n    def sentence_chart(self, lda_model, text, start=0, end=13, folder_path=None):\n        if lda_model is None:\n            raise ValueError(\"LDA model is not provided.\")\n        corp = text[start:end]\n        mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n        fig, axes = plt.subplots(\n            end - start, 1, figsize=(20, (end - start) * 0.95), dpi=160\n        )\n        axes[0].axis(\"off\")\n        for i, ax in enumerate(axes):\n            try:\n                if i &gt; 0:\n                    corp_cur = corp[i - 1]\n                    topic_percs, wordid_topics, _ = lda_model[corp_cur]\n                    word_dominanttopic = [\n                        (lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics\n                    ]\n                    ax.text(\n                        0.01,\n                        0.5,\n                        \"Doc \" + str(i - 1) + \": \",\n                        verticalalignment=\"center\",\n                        fontsize=16,\n                        color=\"black\",\n                        transform=ax.transAxes,\n                        fontweight=700,\n                    )\n\n                    # Draw Rectange\n                    topic_percs_sorted = sorted(\n                        topic_percs, key=lambda x: (x[1]), reverse=True\n                    )\n                    ax.add_patch(\n                        Rectangle(\n                            (0.0, 0.05),\n                            0.99,\n                            0.90,\n                            fill=None,\n                            alpha=1,\n                            color=mycolors[topic_percs_sorted[0][0]],\n                            linewidth=2,\n                        )\n                    )\n\n                    word_pos = 0.06\n                    for j, (word, topics) in enumerate(word_dominanttopic):\n                        if j &lt; 14:\n                            ax.text(\n                                word_pos,\n                                0.5,\n                                word,\n                                horizontalalignment=\"left\",\n                                verticalalignment=\"center\",\n                                fontsize=16,\n                                color=mycolors[topics],\n                                transform=ax.transAxes,\n                                fontweight=700,\n                            )\n                            word_pos += 0.009 * len(\n                                word\n                            )  # to move the word for the next iter\n                            ax.axis(\"off\")\n                    ax.text(\n                        word_pos,\n                        0.5,\n                        \". . .\",\n                        horizontalalignment=\"left\",\n                        verticalalignment=\"center\",\n                        fontsize=16,\n                        color=\"black\",\n                        transform=ax.transAxes,\n                    )\n            except Exception as e:\n                logger.exception(f\"Error occurred while processing document {i - 1}: {e}\")\n                continue\n\n        plt.subplots_adjust(wspace=0, hspace=0)\n        plt.suptitle(\n            \"Sentence Topic Coloring for Documents: \"\n            + str(start)\n            + \" to \"\n            + str(end - 2),\n            fontsize=22,\n            y=0.95,\n            fontweight=700,\n        )\n        plt.tight_layout()\n        plt.show(block=False)\n        # save\n        if folder_path:\n            plt.savefig(folder_path)\n            plt.close()\n\n    def _cluster_chart(self, lda_model, text, n_topics=3, folder_path=None):\n        # Get topic weights\n        topic_weights = []\n        for i, row_list in enumerate(lda_model[text]):\n            topic_weights.append([w for i, w in row_list[0]])\n\n        # Array of topic weights\n        arr = pd.DataFrame(topic_weights).fillna(0).values\n\n        # Keep the well separated points (optional)\n        arr = arr[np.amax(arr, axis=1) &gt; 0.35]\n\n        # Dominant topic number in each doc\n        topic_num = np.argmax(arr, axis=1)\n\n        # tSNE Dimension Reduction\n        tsne_model = TSNE(\n            n_components=2, verbose=1, random_state=0, angle=0.99, init=\"pca\"\n        )\n        tsne_lda = tsne_model.fit_transform(arr)\n\n        # Plot\n        plt.figure(figsize=(16, 10), dpi=160)\n        for i in range(n_topics):\n            plt.scatter(\n                tsne_lda[topic_num == i, 0],\n                tsne_lda[topic_num == i, 1],\n                label=str(i),\n                alpha=0.5,\n            )\n        plt.title(\"t-SNE Clustering of Topics\", fontsize=22)\n        plt.xlabel(\"t-SNE Dimension 1\", fontsize=16)\n        plt.ylabel(\"t-SNE Dimension 2\", fontsize=16)\n        plt.legend(title=\"Topic Number\", loc=\"upper right\")\n        plt.show(block=False)\n        # save\n        if folder_path:\n            plt.savefig(folder_path)\n            plt.close()\n\n    def most_discussed_topics(\n        self, lda_model, dominant_topics, topic_percentages, folder_path=None\n    ):\n\n        # Distribution of Dominant Topics in Each Document\n        df = pd.DataFrame(dominant_topics, columns=[\"Document_Id\", \"Dominant_Topic\"])\n        dominant_topic_in_each_doc = df.groupby(\"Dominant_Topic\").size()\n        df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(\n            name=\"count\"\n        ).reset_index()\n\n        # Total Topic Distribution by actual weight\n        topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n        df_topic_weightage_by_doc = (\n            topic_weightage_by_doc.sum().to_frame(name=\"count\").reset_index()\n        )\n\n        # Top 3 Keywords for each Topic\n        topic_top3words = [\n            (i, topic)\n            for i, topics in lda_model.show_topics(formatted=False)\n            for j, (topic, wt) in enumerate(topics)\n            if j &lt; 3\n        ]\n\n        df_top3words_stacked = pd.DataFrame(\n            topic_top3words, columns=[\"topic_id\", \"words\"]\n        )\n        df_top3words = df_top3words_stacked.groupby(\"topic_id\").agg(\", \\n\".join)\n        df_top3words.reset_index(level=0, inplace=True)\n\n        # Plot\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n\n        # Topic Distribution by Dominant Topics\n        ax1.bar(\n            x=\"Dominant_Topic\",\n            height=\"count\",\n            data=df_dominant_topic_in_each_doc,\n            width=0.5,\n            color=\"firebrick\",\n        )\n        ax1.set_xticks(\n            range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__())\n        )\n        tick_formatter = FuncFormatter(\n            lambda x, pos: \"Topic \"\n            + str(x)\n            + \"\\n\"\n            + df_top3words.loc[df_top3words.topic_id == x, \"words\"].values[0]  # type: ignore\n        )\n        ax1.xaxis.set_major_formatter(tick_formatter)\n        ax1.set_title(\"Number of Documents by Dominant Topic\", fontdict=dict(size=10))\n        ax1.set_ylabel(\"Number of Documents\")\n        ax1.set_ylim(0, 1000)\n\n        # Topic Distribution by Topic Weights\n        ax2.bar(\n            x=\"index\",\n            height=\"count\",\n            data=df_topic_weightage_by_doc,\n            width=0.5,\n            color=\"steelblue\",\n        )\n        ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n        ax2.xaxis.set_major_formatter(tick_formatter)\n        ax2.set_title(\"Number of Documents by Topic Weightage\", fontdict=dict(size=10))\n\n        plt.show(block=False)\n\n        # save\n        if folder_path:\n            plt.savefig(folder_path)\n            plt.close()\n\n    def update_annot(self, ind):\n        if self.annot is None or self.sc is None or self.c is None:\n            raise RuntimeError(\"cluster_chart must be called before update_annot.\")\n        indices_array = np.atleast_1d(ind.get(\"ind\", []))\n        if indices_array.size == 0:\n            return\n        indices = indices_array.astype(int)\n        idx = int(indices[0])\n        offsets = np.asarray(self.sc.get_offsets())\n        pos = offsets[idx]\n        annot = self.annot\n        annot.xy = (float(pos[0]), float(pos[1]))\n        text = \"{}, {}\".format(\n            \" \".join(list(map(str, indices))),\n            \" \".join([self.names[n] for n in indices]),\n        )\n        annot.set_text(text)\n        cmap = plt.get_cmap(\"RdYlGn\")\n        norm = mcolors.Normalize(1, 4)\n        bbox = annot.get_bbox_patch()\n        if bbox is not None:\n            try:\n                color_value = float(self.c[idx])\n            except (TypeError, ValueError):\n                color_value = 1.0\n            bbox.set_facecolor(cmap(norm(color_value)))\n            bbox.set_alpha(0.4)\n\n    def hover(self, event):\n        if self.annot is None or self.sc is None or self.fig is None or self.ax is None:\n            return\n        vis = self.annot.get_visible()\n        if event.inaxes == self.ax:\n            cont, ind = self.sc.contains(event)\n            if cont:\n                self.update_annot(ind)\n                self.annot.set_visible(True)\n                self.fig.canvas.draw_idle()\n            elif vis:\n                self.annot.set_visible(False)\n                self.fig.canvas.draw_idle()\n\n    # https://stackoverflow.com/questions/7908636/how-to-add-hovering-annotations-to-a-plot\n    def cluster_chart(self, data, folder_path=None):\n        # Scatter plot for Text Cluster Prediction\n        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n        self.names = list(map(str, data[\"title\"]))\n        self.sc = plt.scatter(\n            data[\"x\"],\n            data[\"y\"],\n            c=data[\"colour\"],\n            s=36,\n            edgecolors=\"black\",\n            linewidths=0.75,\n        )\n        self.c = np.asarray(data[\"colour\"])\n        self.annot = self.ax.annotate(\n            \"\",\n            xy=(0, 0),\n            xytext=(20, 20),\n            textcoords=\"offset points\",\n            bbox=dict(boxstyle=\"round\", fc=\"w\"),\n            arrowprops=dict(arrowstyle=\"-&gt;\"),\n        )\n        self.annot.set_visible(False)\n        plt.title(\"Text Cluster Prediction\")\n        plt.axis(\"off\")  # Optional: Remove axes for a cleaner look\n        plt.colorbar(self.sc, label=\"Colour\")  # Add colorbar if needed\n        self.fig.canvas.mpl_connect(\"motion_notify_event\", self.hover)\n        plt.show(block=False)\n        # save\n        if folder_path:\n            # annotate with data['title']\n            for i, txt in enumerate(data[\"title\"]):\n                plt.annotate(\n                    txt,\n                    (data[\"x\"][i], data[\"y\"][i]),\n                    fontsize=8,\n                    ha=\"right\",\n                    va=\"bottom\",\n                )\n            plt.savefig(folder_path)\n            plt.close()\n\n    def get_lda_viz(\n        self,\n        lda_model,\n        corpus_bow,\n        dictionary,\n        folder_path: str | None = None,\n        mds: str = \"tsne\",\n        lambda_val: float = 0.6,\n        show: bool = True,\n    ) -&gt; str | None:\n        \"\"\"\n        Generate an interactive LDA visualization using pyLDAvis.\n\n        Args:\n            lda_model: The trained LDA model\n            corpus_bow: Bag of words corpus\n            dictionary: Gensim dictionary\n            folder_path: Path to save the HTML visualization\n            mds: Dimension reduction method ('tsne', 'mmds', or 'pcoa')\n            lambda_val: Lambda parameter for relevance metric (default: 0.6).\n                       Mettler et al. (2025) performed several experiments to identify\n                       the optimal value of \u03bb, which turned out to be 0.6.\n            show: Whether to display the visualization\n\n        Returns:\n            HTML string of the visualization if successful, None otherwise\n\n        Raises:\n            ImportError: If pyLDAvis is not installed\n            ValueError: If required inputs are missing\n        \"\"\"\n        if not PYLDAVIS_AVAILABLE:\n            raise ImportError(\n                \"pyLDAvis is not installed. Install it with: pip install pyLDAvis\"\n            )\n\n        if lda_model is None:\n            raise ValueError(\"LDA model is required\")\n        if corpus_bow is None:\n            raise ValueError(\"Corpus bag of words is required\")\n        if dictionary is None:\n            raise ValueError(\"Dictionary is required\")\n\n        try:\n            # Prepare the visualization data\n            vis_data = gensimvis.prepare(\n                lda_model,\n                corpus_bow,\n                dictionary,\n                mds=mds,\n                R=30,\n                lambda_step=0.01,\n                plot_opts={\"xlab\": \"PC1\", \"ylab\": \"PC2\"},\n            )\n\n            # Save to HTML file if path provided\n            if folder_path:\n                output_path = Path(folder_path)\n                if output_path.parent:\n                    output_path.parent.mkdir(parents=True, exist_ok=True)\n                pyLDAvis.save_html(vis_data, str(output_path))\n                logger.info(f\"LDA visualization saved to {output_path}\")\n\n            # Return HTML string for embedding or further use\n            html_string = pyLDAvis.prepared_data_to_html(vis_data)\n            return html_string\n\n        except Exception as e:\n            logger.exception(f\"Error generating LDA visualization: {e}\")\n            raise\n\n    def draw_tdabm(\n        self,\n        corpus: Corpus | None = None,\n        folder_path: str | None = None,\n        show: bool = True,\n    ) -&gt; Figure:\n        \"\"\"\n        Draw TDABM (Topological Data Analysis Ball Mapper) visualization.\n\n        Creates a 2D graph showing landmark points as circles:\n        - Circle size is proportional to the count of points in the ball\n        - Circle color represents mean y value (red for low, purple for high)\n        - Lines connect landmark points with non-empty intersections\n\n        Based on the algorithm by Rudkin and Dlotko (2024).\n\n        Args:\n            corpus: Corpus with 'tdabm' metadata. If None, uses self.corpus\n            folder_path: Path to save the figure. If None, uses self.folder_path\n            show: Whether to display the plot\n\n        Returns:\n            Matplotlib Figure object\n        \"\"\"\n        if corpus is None:\n            corpus = self.corpus\n\n        if corpus is None:\n            raise ValueError(\"No corpus provided\")\n\n        if \"tdabm\" not in corpus.metadata:\n            raise ValueError(\n                \"Corpus metadata does not contain 'tdabm' data. Run TDABM analysis first.\"\n            )\n\n        tdabm_data = corpus.metadata[\"tdabm\"]\n        landmarks = tdabm_data[\"landmarks\"]\n\n        if not landmarks:\n            raise ValueError(\"No landmarks found in TDABM data\")\n\n        # Create figure\n        fig, ax = plt.subplots(figsize=(12, 10))\n\n        # Collect all landmark locations\n        locations = [landmark[\"location\"] for landmark in landmarks]\n        counts = [landmark[\"count\"] for landmark in landmarks]\n        mean_ys = [landmark[\"mean_y\"] for landmark in landmarks]\n\n        # Perform PCA to reduce to 2 components (PC1, PC2)\n        from sklearn.decomposition import PCA\n\n        locations_array = np.array(locations)\n        if locations_array.shape[1] &lt; 2:\n            # If only 1D, pad with zeros\n            locations_array = np.pad(locations_array, ((0, 0), (0, 1)), mode=\"constant\")\n        pca = PCA(n_components=2)\n        positions = pca.fit_transform(locations_array)\n\n        # Normalize mean_y for color mapping (red=0, purple=max)\n        min_y = min(mean_ys)\n        max_y = max(mean_ys)\n\n        if max_y - min_y &gt; 0:\n            normalized_ys = [(y - min_y) / (max_y - min_y) for y in mean_ys]\n        else:\n            normalized_ys = [0.5] * len(mean_ys)\n\n        # Create color map: red (0) to green (1)\n        colors = []\n        for norm_y in normalized_ys:\n            # Interpolate from red (1,0,0) to green (0,1,0)\n            r = 1.0 - norm_y\n            g = norm_y\n            b = 0.0\n            colors.append((r, g, b))\n\n        # Draw connections first (so they appear behind circles)\n        landmark_dict = {lm[\"id\"]: idx for idx, lm in enumerate(landmarks)}\n\n        for i, landmark in enumerate(landmarks):\n            for connected_id in landmark[\"connections\"]:\n                if connected_id in landmark_dict:\n                    j = landmark_dict[connected_id]\n                    # Only draw each connection once (avoid duplicates)\n                    if i &lt; j:\n                        ax.plot(\n                            [positions[i, 0], positions[j, 0]],\n                            [positions[i, 1], positions[j, 1]],\n                            \"k-\",\n                            alpha=0.3,\n                            linewidth=1,\n                            zorder=1,\n                        )\n\n        # Normalize counts for circle sizes (scale for visibility)\n        max_count = max(counts)\n        min_count = min(counts)\n\n        if max_count &gt; min_count:\n            # Scale sizes between 100 and 2000\n            sizes = [\n                100 + 1900 * (c - min_count) / (max_count - min_count) for c in counts\n            ]\n        else:\n            sizes = [500] * len(counts)\n\n        # Draw circles for landmarks\n        ax.scatter(\n            positions[:, 0],\n            positions[:, 1],\n            s=sizes,\n            c=colors,\n            alpha=0.6,\n            edgecolors=\"black\",\n            linewidths=1.5,\n            zorder=2,\n        )\n\n        # Add count and mean_y as label inside each circle\n        for i, (pos, count, mean_y) in enumerate(zip(positions, counts, mean_ys)):\n            ax.annotate(\n                f\"{count}\\n{mean_y:.2f}\",\n                xy=pos,\n                xytext=(0, 0),\n                textcoords=\"offset points\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n                fontweight=\"bold\",\n                zorder=3,\n            )\n\n        # Set labels and title\n        y_var = tdabm_data.get(\"y_variable\", \"y\")\n\n        # Axis labels reflect PCA components\n        ax.set_xlabel(\"PC1\", fontsize=12)\n        ax.set_ylabel(\"PC2\", fontsize=12)\n\n        ax.set_title(\n            f\"TDABM Visualization\\n\"\n            f'Y variable: {y_var}, Radius: {tdabm_data.get(\"radius\", 0.3)}\\n'\n            f\"Landmarks: {len(landmarks)}\",\n            fontsize=14,\n            fontweight=\"bold\",\n        )\n\n        # Add colorbar for mean_y (red to green)\n        sm = plt.cm.ScalarMappable(\n            cmap=mcolors.LinearSegmentedColormap.from_list(\n                \"red_green\", [\"red\", \"green\"]\n            ),\n            norm=mcolors.Normalize(vmin=min_y, vmax=max_y),\n        )\n        sm.set_array([])\n        cbar = plt.colorbar(sm, ax=ax)\n        cbar.set_label(f\"Mean {y_var}\", fontsize=12)\n\n        # Add legend for circle sizes\n        # Create dummy scatter plots for legend\n        legend_counts = [min_count, (min_count + max_count) / 2, max_count]\n        legend_sizes = []\n        for c in legend_counts:\n            if max_count &gt; min_count:\n                size = 100 + 1900 * (c - min_count) / (max_count - min_count)\n            else:\n                size = 500\n            legend_sizes.append(size)\n\n        legend_elements = []\n        for size, count in zip(legend_sizes, legend_counts):\n            legend_elements.append(\n                plt.scatter(\n                    [],\n                    [],\n                    s=size,\n                    c=\"gray\",\n                    alpha=0.6,\n                    edgecolors=\"black\",\n                    linewidths=1.5,\n                    label=f\"{int(count)} points\",\n                )\n            )\n\n        ax.legend(\n            handles=legend_elements,\n            title=\"Ball Size\",\n            loc=\"upper right\",\n            framealpha=0.9,\n        )\n\n        ax.grid(True, alpha=0.3)\n        ax.set_aspect(\"equal\", adjustable=\"box\")\n\n        plt.tight_layout()\n\n        return self._finalize_plot(fig, folder_path, show)\n\n    def draw_graph(\n        self,\n        corpus: Corpus | None = None,\n        folder_path: str | None = None,\n        show: bool = True,\n        layout: str = \"spring\",\n    ) -&gt; Figure:\n        \"\"\"\n        Draw graph visualization from corpus metadata.\n\n        Creates a visualization of the graph structure showing documents,\n        keywords, clusters, and metadata nodes along with their relationships.\n\n        Args:\n            corpus: Corpus with 'graph' metadata. If None, uses self.corpus\n            folder_path: Path to save the figure. If None, uses self.folder_path\n            show: Whether to display the plot\n            layout: Graph layout algorithm ('spring', 'circular', 'kamada_kawai', 'spectral')\n\n        Returns:\n            Matplotlib Figure object\n\n        Raises:\n            ValueError: If corpus or graph metadata is missing\n        \"\"\"\n        if corpus is None:\n            corpus = self.corpus\n\n        if corpus is None:\n            raise ValueError(\"No corpus provided\")\n\n        if \"graph\" not in corpus.metadata:\n            raise ValueError(\n                \"Corpus metadata does not contain 'graph' data. Run graph generation first.\"\n            )\n\n        graph_data = corpus.metadata[\"graph\"]\n        nodes = graph_data[\"nodes\"]\n        edges = graph_data[\"edges\"]\n\n        if not nodes:\n            raise ValueError(\"No nodes found in graph data\")\n\n        # Create NetworkX graph\n        G = nx.Graph()\n\n        # Add nodes with their labels (store as maps keyed by node id)\n        node_labels: dict[str, str] = {}\n        node_color_map_by_id: dict[str, str] = {}\n        node_size_map_by_id: dict[str, float] = {}\n\n        # Color mapping for different node types\n        color_map = {\n            \"document\": \"#FF6B6B\",  # Red\n            \"keyword\": \"#4ECDC4\",  # Teal\n            \"cluster\": \"#95E1D3\",  # Light green\n            \"metadata\": \"#FFD93D\",  # Yellow\n        }\n\n        for node in nodes:\n            node_id = str(node.get(\"id\"))\n            label = node.get(\"label\", \"metadata\")\n            properties = node.get(\"properties\", {})\n\n            G.add_node(node_id, label=label, **properties)\n\n            # Set node label (use name property if available)\n            if properties.get(\"name\"):\n                node_labels[node_id] = str(properties[\"name\"])\n            else:\n                # For keywords, remove the \"keyword:\" prefix\n                if node_id.startswith(\"keyword:\"):\n                    node_labels[node_id] = node_id.replace(\"keyword:\", \"\")\n                elif node_id.startswith(\"cluster:\"):\n                    node_labels[node_id] = f\"C{node_id.replace('cluster:', '')}\"\n                elif node_id.startswith(\"metadata:\"):\n                    node_labels[node_id] = \"M\"\n                else:\n                    node_labels[node_id] = node_id\n\n            # Set node color based on type\n            node_color_map_by_id[node_id] = color_map.get(label, \"#CCCCCC\")\n\n            # Set node size based on type (documents larger)\n            if label == \"document\":\n                node_size_map_by_id[node_id] = 800.0\n            elif label == \"keyword\":\n                node_size_map_by_id[node_id] = 500.0\n            elif label == \"cluster\":\n                node_size_map_by_id[node_id] = 600.0\n            else:\n                node_size_map_by_id[node_id] = 400.0\n\n        # Add edges\n        for edge in edges:\n            source = str(edge.get(\"source\"))\n            target = str(edge.get(\"target\"))\n            # If edge introduces unknown nodes, add with default properties\n            if source not in G:\n                G.add_node(source, label=\"metadata\")\n                node_labels[source] = (\n                    source if not source.startswith(\"metadata:\") else \"M\"\n                )\n                node_color_map_by_id[source] = color_map.get(\"metadata\", \"#CCCCCC\")\n                node_size_map_by_id[source] = 400.0\n            if target not in G:\n                G.add_node(target, label=\"metadata\")\n                node_labels[target] = (\n                    target if not target.startswith(\"metadata:\") else \"M\"\n                )\n                node_color_map_by_id[target] = color_map.get(\"metadata\", \"#CCCCCC\")\n                node_size_map_by_id[target] = 400.0\n            G.add_edge(source, target)\n\n        # Create figure\n        fig, ax = plt.subplots(figsize=(16, 12))\n\n        # Choose layout algorithm\n        if layout == \"spring\":\n            pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n        elif layout == \"circular\":\n            pos = nx.circular_layout(G)\n        elif layout == \"kamada_kawai\":\n            pos = nx.kamada_kawai_layout(G)\n        elif layout == \"spectral\":\n            pos = nx.spectral_layout(G)\n        else:\n            pos = nx.spring_layout(G, seed=42)\n\n        # Draw edges first (so they appear behind nodes)\n        nx.draw_networkx_edges(\n            G,\n            pos,\n            ax=ax,\n            edge_color=\"#CCCCCC\",\n            width=1.5,\n            alpha=0.6,\n        )\n\n        # Build aligned arrays for node attributes\n        nodelist = list(G.nodes())\n        node_colors = [node_color_map_by_id.get(n, \"#CCCCCC\") for n in nodelist]\n        node_sizes = np.asarray(\n            [float(node_size_map_by_id.get(n, 400.0)) for n in nodelist]\n        )\n        # Draw nodes with explicit nodelist\n        nx.draw_networkx_nodes(\n            G,\n            pos,\n            nodelist=nodelist,\n            ax=ax,\n            node_color=node_colors,\n            node_size=node_sizes,\n            alpha=0.9,\n            edgecolors=\"black\",\n            linewidths=1.5,\n        )\n\n        # Draw labels\n        # Labels aligned to nodelist\n        labels_ordered = {n: node_labels.get(n, str(n)) for n in nodelist}\n        nx.draw_networkx_labels(\n            G,\n            pos,\n            labels=labels_ordered,\n            ax=ax,\n            font_size=8,\n            font_weight=\"bold\",\n            font_color=\"black\",\n        )\n\n        # Add title and legend\n        # Compute stats if not provided\n        num_nodes = int(graph_data.get(\"num_nodes\", len(G.nodes())))\n        num_edges = int(graph_data.get(\"num_edges\", len(G.edges())))\n        num_documents = int(\n            graph_data.get(\n                \"num_documents\",\n                sum(1 for _, d in G.nodes(data=True) if d.get(\"label\") == \"document\"),\n            )\n        )\n        ax.set_title(\n            f\"Graph Visualization\\n\"\n            f\"Nodes: {num_nodes}, \"\n            f\"Edges: {num_edges}, \"\n            f\"Documents: {num_documents}\",\n            fontsize=14,\n            fontweight=\"bold\",\n            pad=20,\n        )\n\n        # Create legend\n        from matplotlib.patches import Patch\n\n        legend_elements = []\n        for node_type, color in color_map.items():\n            legend_elements.append(\n                Patch(facecolor=color, edgecolor=\"black\", label=node_type.capitalize())\n            )\n\n        ax.legend(\n            handles=legend_elements,\n            loc=\"upper left\",\n            framealpha=0.9,\n            title=\"Node Types\",\n        )\n\n        ax.axis(\"off\")\n        plt.tight_layout()\n\n        return self._finalize_plot(fig, folder_path, show)\n</code></pre>"},{"location":"modules/#visualize.QRVisualize.draw_graph","title":"<code>draw_graph(corpus=None, folder_path=None, show=True, layout='spring')</code>","text":"<p>Draw graph visualization from corpus metadata.</p> <p>Creates a visualization of the graph structure showing documents, keywords, clusters, and metadata nodes along with their relationships.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>Corpus | None</code> <p>Corpus with 'graph' metadata. If None, uses self.corpus</p> <code>None</code> <code>folder_path</code> <code>str | None</code> <p>Path to save the figure. If None, uses self.folder_path</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot</p> <code>True</code> <code>layout</code> <code>str</code> <p>Graph layout algorithm ('spring', 'circular', 'kamada_kawai', 'spectral')</p> <code>'spring'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib Figure object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If corpus or graph metadata is missing</p> Source code in <code>src/crisp_t/visualize.py</code> <pre><code>def draw_graph(\n    self,\n    corpus: Corpus | None = None,\n    folder_path: str | None = None,\n    show: bool = True,\n    layout: str = \"spring\",\n) -&gt; Figure:\n    \"\"\"\n    Draw graph visualization from corpus metadata.\n\n    Creates a visualization of the graph structure showing documents,\n    keywords, clusters, and metadata nodes along with their relationships.\n\n    Args:\n        corpus: Corpus with 'graph' metadata. If None, uses self.corpus\n        folder_path: Path to save the figure. If None, uses self.folder_path\n        show: Whether to display the plot\n        layout: Graph layout algorithm ('spring', 'circular', 'kamada_kawai', 'spectral')\n\n    Returns:\n        Matplotlib Figure object\n\n    Raises:\n        ValueError: If corpus or graph metadata is missing\n    \"\"\"\n    if corpus is None:\n        corpus = self.corpus\n\n    if corpus is None:\n        raise ValueError(\"No corpus provided\")\n\n    if \"graph\" not in corpus.metadata:\n        raise ValueError(\n            \"Corpus metadata does not contain 'graph' data. Run graph generation first.\"\n        )\n\n    graph_data = corpus.metadata[\"graph\"]\n    nodes = graph_data[\"nodes\"]\n    edges = graph_data[\"edges\"]\n\n    if not nodes:\n        raise ValueError(\"No nodes found in graph data\")\n\n    # Create NetworkX graph\n    G = nx.Graph()\n\n    # Add nodes with their labels (store as maps keyed by node id)\n    node_labels: dict[str, str] = {}\n    node_color_map_by_id: dict[str, str] = {}\n    node_size_map_by_id: dict[str, float] = {}\n\n    # Color mapping for different node types\n    color_map = {\n        \"document\": \"#FF6B6B\",  # Red\n        \"keyword\": \"#4ECDC4\",  # Teal\n        \"cluster\": \"#95E1D3\",  # Light green\n        \"metadata\": \"#FFD93D\",  # Yellow\n    }\n\n    for node in nodes:\n        node_id = str(node.get(\"id\"))\n        label = node.get(\"label\", \"metadata\")\n        properties = node.get(\"properties\", {})\n\n        G.add_node(node_id, label=label, **properties)\n\n        # Set node label (use name property if available)\n        if properties.get(\"name\"):\n            node_labels[node_id] = str(properties[\"name\"])\n        else:\n            # For keywords, remove the \"keyword:\" prefix\n            if node_id.startswith(\"keyword:\"):\n                node_labels[node_id] = node_id.replace(\"keyword:\", \"\")\n            elif node_id.startswith(\"cluster:\"):\n                node_labels[node_id] = f\"C{node_id.replace('cluster:', '')}\"\n            elif node_id.startswith(\"metadata:\"):\n                node_labels[node_id] = \"M\"\n            else:\n                node_labels[node_id] = node_id\n\n        # Set node color based on type\n        node_color_map_by_id[node_id] = color_map.get(label, \"#CCCCCC\")\n\n        # Set node size based on type (documents larger)\n        if label == \"document\":\n            node_size_map_by_id[node_id] = 800.0\n        elif label == \"keyword\":\n            node_size_map_by_id[node_id] = 500.0\n        elif label == \"cluster\":\n            node_size_map_by_id[node_id] = 600.0\n        else:\n            node_size_map_by_id[node_id] = 400.0\n\n    # Add edges\n    for edge in edges:\n        source = str(edge.get(\"source\"))\n        target = str(edge.get(\"target\"))\n        # If edge introduces unknown nodes, add with default properties\n        if source not in G:\n            G.add_node(source, label=\"metadata\")\n            node_labels[source] = (\n                source if not source.startswith(\"metadata:\") else \"M\"\n            )\n            node_color_map_by_id[source] = color_map.get(\"metadata\", \"#CCCCCC\")\n            node_size_map_by_id[source] = 400.0\n        if target not in G:\n            G.add_node(target, label=\"metadata\")\n            node_labels[target] = (\n                target if not target.startswith(\"metadata:\") else \"M\"\n            )\n            node_color_map_by_id[target] = color_map.get(\"metadata\", \"#CCCCCC\")\n            node_size_map_by_id[target] = 400.0\n        G.add_edge(source, target)\n\n    # Create figure\n    fig, ax = plt.subplots(figsize=(16, 12))\n\n    # Choose layout algorithm\n    if layout == \"spring\":\n        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n    elif layout == \"circular\":\n        pos = nx.circular_layout(G)\n    elif layout == \"kamada_kawai\":\n        pos = nx.kamada_kawai_layout(G)\n    elif layout == \"spectral\":\n        pos = nx.spectral_layout(G)\n    else:\n        pos = nx.spring_layout(G, seed=42)\n\n    # Draw edges first (so they appear behind nodes)\n    nx.draw_networkx_edges(\n        G,\n        pos,\n        ax=ax,\n        edge_color=\"#CCCCCC\",\n        width=1.5,\n        alpha=0.6,\n    )\n\n    # Build aligned arrays for node attributes\n    nodelist = list(G.nodes())\n    node_colors = [node_color_map_by_id.get(n, \"#CCCCCC\") for n in nodelist]\n    node_sizes = np.asarray(\n        [float(node_size_map_by_id.get(n, 400.0)) for n in nodelist]\n    )\n    # Draw nodes with explicit nodelist\n    nx.draw_networkx_nodes(\n        G,\n        pos,\n        nodelist=nodelist,\n        ax=ax,\n        node_color=node_colors,\n        node_size=node_sizes,\n        alpha=0.9,\n        edgecolors=\"black\",\n        linewidths=1.5,\n    )\n\n    # Draw labels\n    # Labels aligned to nodelist\n    labels_ordered = {n: node_labels.get(n, str(n)) for n in nodelist}\n    nx.draw_networkx_labels(\n        G,\n        pos,\n        labels=labels_ordered,\n        ax=ax,\n        font_size=8,\n        font_weight=\"bold\",\n        font_color=\"black\",\n    )\n\n    # Add title and legend\n    # Compute stats if not provided\n    num_nodes = int(graph_data.get(\"num_nodes\", len(G.nodes())))\n    num_edges = int(graph_data.get(\"num_edges\", len(G.edges())))\n    num_documents = int(\n        graph_data.get(\n            \"num_documents\",\n            sum(1 for _, d in G.nodes(data=True) if d.get(\"label\") == \"document\"),\n        )\n    )\n    ax.set_title(\n        f\"Graph Visualization\\n\"\n        f\"Nodes: {num_nodes}, \"\n        f\"Edges: {num_edges}, \"\n        f\"Documents: {num_documents}\",\n        fontsize=14,\n        fontweight=\"bold\",\n        pad=20,\n    )\n\n    # Create legend\n    from matplotlib.patches import Patch\n\n    legend_elements = []\n    for node_type, color in color_map.items():\n        legend_elements.append(\n            Patch(facecolor=color, edgecolor=\"black\", label=node_type.capitalize())\n        )\n\n    ax.legend(\n        handles=legend_elements,\n        loc=\"upper left\",\n        framealpha=0.9,\n        title=\"Node Types\",\n    )\n\n    ax.axis(\"off\")\n    plt.tight_layout()\n\n    return self._finalize_plot(fig, folder_path, show)\n</code></pre>"},{"location":"modules/#visualize.QRVisualize.draw_tdabm","title":"<code>draw_tdabm(corpus=None, folder_path=None, show=True)</code>","text":"<p>Draw TDABM (Topological Data Analysis Ball Mapper) visualization.</p> <p>Creates a 2D graph showing landmark points as circles: - Circle size is proportional to the count of points in the ball - Circle color represents mean y value (red for low, purple for high) - Lines connect landmark points with non-empty intersections</p> <p>Based on the algorithm by Rudkin and Dlotko (2024).</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>Corpus | None</code> <p>Corpus with 'tdabm' metadata. If None, uses self.corpus</p> <code>None</code> <code>folder_path</code> <code>str | None</code> <p>Path to save the figure. If None, uses self.folder_path</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib Figure object</p> Source code in <code>src/crisp_t/visualize.py</code> <pre><code>def draw_tdabm(\n    self,\n    corpus: Corpus | None = None,\n    folder_path: str | None = None,\n    show: bool = True,\n) -&gt; Figure:\n    \"\"\"\n    Draw TDABM (Topological Data Analysis Ball Mapper) visualization.\n\n    Creates a 2D graph showing landmark points as circles:\n    - Circle size is proportional to the count of points in the ball\n    - Circle color represents mean y value (red for low, purple for high)\n    - Lines connect landmark points with non-empty intersections\n\n    Based on the algorithm by Rudkin and Dlotko (2024).\n\n    Args:\n        corpus: Corpus with 'tdabm' metadata. If None, uses self.corpus\n        folder_path: Path to save the figure. If None, uses self.folder_path\n        show: Whether to display the plot\n\n    Returns:\n        Matplotlib Figure object\n    \"\"\"\n    if corpus is None:\n        corpus = self.corpus\n\n    if corpus is None:\n        raise ValueError(\"No corpus provided\")\n\n    if \"tdabm\" not in corpus.metadata:\n        raise ValueError(\n            \"Corpus metadata does not contain 'tdabm' data. Run TDABM analysis first.\"\n        )\n\n    tdabm_data = corpus.metadata[\"tdabm\"]\n    landmarks = tdabm_data[\"landmarks\"]\n\n    if not landmarks:\n        raise ValueError(\"No landmarks found in TDABM data\")\n\n    # Create figure\n    fig, ax = plt.subplots(figsize=(12, 10))\n\n    # Collect all landmark locations\n    locations = [landmark[\"location\"] for landmark in landmarks]\n    counts = [landmark[\"count\"] for landmark in landmarks]\n    mean_ys = [landmark[\"mean_y\"] for landmark in landmarks]\n\n    # Perform PCA to reduce to 2 components (PC1, PC2)\n    from sklearn.decomposition import PCA\n\n    locations_array = np.array(locations)\n    if locations_array.shape[1] &lt; 2:\n        # If only 1D, pad with zeros\n        locations_array = np.pad(locations_array, ((0, 0), (0, 1)), mode=\"constant\")\n    pca = PCA(n_components=2)\n    positions = pca.fit_transform(locations_array)\n\n    # Normalize mean_y for color mapping (red=0, purple=max)\n    min_y = min(mean_ys)\n    max_y = max(mean_ys)\n\n    if max_y - min_y &gt; 0:\n        normalized_ys = [(y - min_y) / (max_y - min_y) for y in mean_ys]\n    else:\n        normalized_ys = [0.5] * len(mean_ys)\n\n    # Create color map: red (0) to green (1)\n    colors = []\n    for norm_y in normalized_ys:\n        # Interpolate from red (1,0,0) to green (0,1,0)\n        r = 1.0 - norm_y\n        g = norm_y\n        b = 0.0\n        colors.append((r, g, b))\n\n    # Draw connections first (so they appear behind circles)\n    landmark_dict = {lm[\"id\"]: idx for idx, lm in enumerate(landmarks)}\n\n    for i, landmark in enumerate(landmarks):\n        for connected_id in landmark[\"connections\"]:\n            if connected_id in landmark_dict:\n                j = landmark_dict[connected_id]\n                # Only draw each connection once (avoid duplicates)\n                if i &lt; j:\n                    ax.plot(\n                        [positions[i, 0], positions[j, 0]],\n                        [positions[i, 1], positions[j, 1]],\n                        \"k-\",\n                        alpha=0.3,\n                        linewidth=1,\n                        zorder=1,\n                    )\n\n    # Normalize counts for circle sizes (scale for visibility)\n    max_count = max(counts)\n    min_count = min(counts)\n\n    if max_count &gt; min_count:\n        # Scale sizes between 100 and 2000\n        sizes = [\n            100 + 1900 * (c - min_count) / (max_count - min_count) for c in counts\n        ]\n    else:\n        sizes = [500] * len(counts)\n\n    # Draw circles for landmarks\n    ax.scatter(\n        positions[:, 0],\n        positions[:, 1],\n        s=sizes,\n        c=colors,\n        alpha=0.6,\n        edgecolors=\"black\",\n        linewidths=1.5,\n        zorder=2,\n    )\n\n    # Add count and mean_y as label inside each circle\n    for i, (pos, count, mean_y) in enumerate(zip(positions, counts, mean_ys)):\n        ax.annotate(\n            f\"{count}\\n{mean_y:.2f}\",\n            xy=pos,\n            xytext=(0, 0),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"center\",\n            fontsize=8,\n            fontweight=\"bold\",\n            zorder=3,\n        )\n\n    # Set labels and title\n    y_var = tdabm_data.get(\"y_variable\", \"y\")\n\n    # Axis labels reflect PCA components\n    ax.set_xlabel(\"PC1\", fontsize=12)\n    ax.set_ylabel(\"PC2\", fontsize=12)\n\n    ax.set_title(\n        f\"TDABM Visualization\\n\"\n        f'Y variable: {y_var}, Radius: {tdabm_data.get(\"radius\", 0.3)}\\n'\n        f\"Landmarks: {len(landmarks)}\",\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\n    # Add colorbar for mean_y (red to green)\n    sm = plt.cm.ScalarMappable(\n        cmap=mcolors.LinearSegmentedColormap.from_list(\n            \"red_green\", [\"red\", \"green\"]\n        ),\n        norm=mcolors.Normalize(vmin=min_y, vmax=max_y),\n    )\n    sm.set_array([])\n    cbar = plt.colorbar(sm, ax=ax)\n    cbar.set_label(f\"Mean {y_var}\", fontsize=12)\n\n    # Add legend for circle sizes\n    # Create dummy scatter plots for legend\n    legend_counts = [min_count, (min_count + max_count) / 2, max_count]\n    legend_sizes = []\n    for c in legend_counts:\n        if max_count &gt; min_count:\n            size = 100 + 1900 * (c - min_count) / (max_count - min_count)\n        else:\n            size = 500\n        legend_sizes.append(size)\n\n    legend_elements = []\n    for size, count in zip(legend_sizes, legend_counts):\n        legend_elements.append(\n            plt.scatter(\n                [],\n                [],\n                s=size,\n                c=\"gray\",\n                alpha=0.6,\n                edgecolors=\"black\",\n                linewidths=1.5,\n                label=f\"{int(count)} points\",\n            )\n        )\n\n    ax.legend(\n        handles=legend_elements,\n        title=\"Ball Size\",\n        loc=\"upper right\",\n        framealpha=0.9,\n    )\n\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n\n    plt.tight_layout()\n\n    return self._finalize_plot(fig, folder_path, show)\n</code></pre>"},{"location":"modules/#visualize.QRVisualize.get_lda_viz","title":"<code>get_lda_viz(lda_model, corpus_bow, dictionary, folder_path=None, mds='tsne', lambda_val=0.6, show=True)</code>","text":"<p>Generate an interactive LDA visualization using pyLDAvis.</p> <p>Parameters:</p> Name Type Description Default <code>lda_model</code> <p>The trained LDA model</p> required <code>corpus_bow</code> <p>Bag of words corpus</p> required <code>dictionary</code> <p>Gensim dictionary</p> required <code>folder_path</code> <code>str | None</code> <p>Path to save the HTML visualization</p> <code>None</code> <code>mds</code> <code>str</code> <p>Dimension reduction method ('tsne', 'mmds', or 'pcoa')</p> <code>'tsne'</code> <code>lambda_val</code> <code>float</code> <p>Lambda parameter for relevance metric (default: 0.6).        Mettler et al. (2025) performed several experiments to identify        the optimal value of \u03bb, which turned out to be 0.6.</p> <code>0.6</code> <code>show</code> <code>bool</code> <p>Whether to display the visualization</p> <code>True</code> <p>Returns:</p> Type Description <code>str | None</code> <p>HTML string of the visualization if successful, None otherwise</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pyLDAvis is not installed</p> <code>ValueError</code> <p>If required inputs are missing</p> Source code in <code>src/crisp_t/visualize.py</code> <pre><code>def get_lda_viz(\n    self,\n    lda_model,\n    corpus_bow,\n    dictionary,\n    folder_path: str | None = None,\n    mds: str = \"tsne\",\n    lambda_val: float = 0.6,\n    show: bool = True,\n) -&gt; str | None:\n    \"\"\"\n    Generate an interactive LDA visualization using pyLDAvis.\n\n    Args:\n        lda_model: The trained LDA model\n        corpus_bow: Bag of words corpus\n        dictionary: Gensim dictionary\n        folder_path: Path to save the HTML visualization\n        mds: Dimension reduction method ('tsne', 'mmds', or 'pcoa')\n        lambda_val: Lambda parameter for relevance metric (default: 0.6).\n                   Mettler et al. (2025) performed several experiments to identify\n                   the optimal value of \u03bb, which turned out to be 0.6.\n        show: Whether to display the visualization\n\n    Returns:\n        HTML string of the visualization if successful, None otherwise\n\n    Raises:\n        ImportError: If pyLDAvis is not installed\n        ValueError: If required inputs are missing\n    \"\"\"\n    if not PYLDAVIS_AVAILABLE:\n        raise ImportError(\n            \"pyLDAvis is not installed. Install it with: pip install pyLDAvis\"\n        )\n\n    if lda_model is None:\n        raise ValueError(\"LDA model is required\")\n    if corpus_bow is None:\n        raise ValueError(\"Corpus bag of words is required\")\n    if dictionary is None:\n        raise ValueError(\"Dictionary is required\")\n\n    try:\n        # Prepare the visualization data\n        vis_data = gensimvis.prepare(\n            lda_model,\n            corpus_bow,\n            dictionary,\n            mds=mds,\n            R=30,\n            lambda_step=0.01,\n            plot_opts={\"xlab\": \"PC1\", \"ylab\": \"PC2\"},\n        )\n\n        # Save to HTML file if path provided\n        if folder_path:\n            output_path = Path(folder_path)\n            if output_path.parent:\n                output_path.parent.mkdir(parents=True, exist_ok=True)\n            pyLDAvis.save_html(vis_data, str(output_path))\n            logger.info(f\"LDA visualization saved to {output_path}\")\n\n        # Return HTML string for embedding or further use\n        html_string = pyLDAvis.prepared_data_to_html(vis_data)\n        return html_string\n\n    except Exception as e:\n        logger.exception(f\"Error generating LDA visualization: {e}\")\n        raise\n</code></pre>"}]}